{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Assignment-03 First Step of Machine Learning: Model and Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "同学们，今天我们的学习了基本的机器学习概念，相比你已经对机器学习的这些方法有一个基本的认识了。值得说明的是，机器学习不仅仅是一系列方法，更重要的是一种思维体系，即：依据以往的、现有的数据，构建某种方法来解决未见过的问题。而且决策树，贝叶斯只是实现这个目标的一个方法，包括之后的神经网络。很有可能有一天，神经网络也会被淘汰，但是重要的是我们要理解机器学习的目标，就是尽可能的自动化解决未知的问题。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](https://timgsa.baidu.com/timg?image&quality=80&size=b9999_10000&sec=1571556399207&di=4a97dc15ad08dd49d3748d1edf6109b3&imgtype=0&src=http%3A%2F%2Fc.hiphotos.baidu.com%2Fzhidao%2Fwh%3D450%2C600%2Fsign%3Dae742c6aedcd7b89e93932873a146e91%2F5d6034a85edf8db1b16050c40223dd54574e74c7.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part-1 Programming Review 编程回顾"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. Re-code the Linear-Regression Model using scikit-learning(10 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<评阅点>： \n",
    "> + 是否完成线性回归模型 (4')\n",
    "+ 能够进行预测新数据(3')\n",
    "+ 能够进行可视化操作(3')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://pypi.tuna.tsinghua.edu.cn/simple\n",
      "Requirement already satisfied: sklearn in /Users/qinliu/AI/anaconda3/lib/python3.7/site-packages (0.0)\n",
      "Collecting scikit-learn (from sklearn)\n",
      "\u001b[?25l  Downloading https://pypi.tuna.tsinghua.edu.cn/packages/82/d9/69769d4f79f3b719cc1255f9bd2b6928c72f43e6f74084e3c67db86c4d2b/scikit_learn-0.22.1-cp37-cp37m-macosx_10_6_intel.whl (11.0MB)\n",
      "\u001b[K     |████████████████████████████████| 11.0MB 118kB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: joblib>=0.11 in /Users/qinliu/AI/anaconda3/lib/python3.7/site-packages (from scikit-learn->sklearn) (0.13.2)\n",
      "Requirement already satisfied: numpy>=1.11.0 in /Users/qinliu/AI/anaconda3/lib/python3.7/site-packages (from scikit-learn->sklearn) (1.17.2)\n",
      "Requirement already satisfied: scipy>=0.17.0 in /Users/qinliu/AI/anaconda3/lib/python3.7/site-packages (from scikit-learn->sklearn) (1.3.1)\n",
      "Installing collected packages: scikit-learn\n",
      "Successfully installed scikit-learn-0.22.1\n"
     ]
    }
   ],
   "source": [
    "!pip install sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_data = np.random.random((20, 2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "random_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = random_data[:, 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = random_data[:, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.52619691, 0.45411523, 0.49032669, 0.06828408, 0.72578183,\n",
       "       0.45794579, 0.58920022, 0.40902253, 0.98643563, 0.74573986,\n",
       "       0.30265318, 0.15176903, 0.27721706, 0.62998253, 0.08267824,\n",
       "       0.28925074, 0.86111224, 0.08290958, 0.31936838, 0.30323034])"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.09260585, 0.87987019, 0.3461554 , 0.19884436, 0.66154601,\n",
       "       0.50901261, 0.03622195, 0.90977464, 0.4120157 , 0.38990702,\n",
       "       0.07209075, 0.13706614, 0.54245347, 0.27548141, 0.89501426,\n",
       "       0.79212703, 0.0823391 , 0.64782732, 0.35417003, 0.58841281])"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "reg = LinearRegression().fit(X.reshape(-1, 1), y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LinearRegression(copy_X=True, fit_intercept=True, n_jobs=None, normalize=False)"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.09260585, 0.87987019, 0.3461554 , 0.19884436, 0.66154601,\n",
       "       0.50901261, 0.03622195, 0.90977464, 0.4120157 , 0.38990702,\n",
       "       0.07209075, 0.13706614, 0.54245347, 0.27548141, 0.89501426,\n",
       "       0.79212703, 0.0823391 , 0.64782732, 0.35417003, 0.58841281])"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.09260585],\n",
       "       [0.87987019],\n",
       "       [0.3461554 ],\n",
       "       [0.19884436],\n",
       "       [0.66154601],\n",
       "       [0.50901261],\n",
       "       [0.03622195],\n",
       "       [0.90977464],\n",
       "       [0.4120157 ],\n",
       "       [0.38990702],\n",
       "       [0.07209075],\n",
       "       [0.13706614],\n",
       "       [0.54245347],\n",
       "       [0.27548141],\n",
       "       [0.89501426],\n",
       "       [0.79212703],\n",
       "       [0.0823391 ],\n",
       "       [0.64782732],\n",
       "       [0.35417003],\n",
       "       [0.58841281]])"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.reshape(-1, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.05505989256379673"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reg.score(X.reshape(-1, 1), y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.52619691, 0.45411523, 0.49032669, 0.06828408, 0.72578183,\n",
       "       0.45794579, 0.58920022, 0.40902253, 0.98643563, 0.74573986,\n",
       "       0.30265318, 0.15176903, 0.27721706, 0.62998253, 0.08267824,\n",
       "       0.28925074, 0.86111224, 0.08290958, 0.31936838, 0.30323034])"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "sklearn.linear_model.base.LinearRegression"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(reg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.21019959])"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reg.coef_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5303898821766584"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reg.intercept_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f(x):\n",
    "    return reg.coef_ * x + reg.intercept_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7fc8f80fc910>]"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.scatter(X, y)\n",
    "plt.plot(X, f(X), color = 'red')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model(X, y):\n",
    "    # 直接存储X,y即可\n",
    "    return [(Xi, yi) for Xi, yi in zip(X, y)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.spatial.distance import cosine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "def distance(x1, x2):\n",
    "    return cosine(x1, x2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(x, k=5):\n",
    "    # 在predict的时候，需要做大量的计算\n",
    "    most_similars = sorted(model(X, y), key=lambda xi: distance(xi[0], x))[:k]\n",
    "    \n",
    "    y_hats = [_y for x, _y in most_similars]\n",
    "    \n",
    "    print(\"most_similars\", most_similars)\n",
    "    \n",
    "    print(\"y_hats\", y_hats)\n",
    "    \n",
    "    return np.mean(y_hats)\n",
    "# -> regression: numerical -> most_similars(y)\n",
    "# -> classification: categorical -> most_similar (y)\n",
    "\n",
    "# 已经获得了最相似的数据集\n",
    "# 然后呢，Counter() -> most_common() -> 就可以获得出现最多的这个y了"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([0.59465282, 0.99103924, 0.94788221, 0.47868857, 0.6190507 ,\n",
       "        0.80425047, 0.87651885, 0.97205672, 0.05989759, 0.99019775,\n",
       "        0.0185682 , 0.75025824, 0.42390673, 0.84580356, 0.16108818,\n",
       "        0.57485969, 0.33543376, 0.37234642, 0.84253403, 0.55325973]),\n",
       " array([17.96834777, 16.27992584, 14.70162161, 11.4144269 , 11.29527934,\n",
       "        10.77695633, 14.74535256, 15.02556009,  2.80262769, 13.26864988,\n",
       "         0.24881384, 10.05346036,  6.68035018, 19.33376766,  9.15858165,\n",
       "        11.70311988,  6.49481235,  9.98944203, 13.28995605,  8.41368038]))"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "myself_knn = model(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0.5946528185601869, 17.968347768706504),\n",
       " (0.9910392419681112, 16.27992584237269),\n",
       " (0.9478822096717707, 14.70162160960173),\n",
       " (0.47868857449599567, 11.414426898246342),\n",
       " (0.6190506969427724, 11.29527933903315),\n",
       " (0.8042504727317771, 10.776956334605813),\n",
       " (0.8765188474298964, 14.745352555560611),\n",
       " (0.9720567233313673, 15.025560092640323),\n",
       " (0.05989758897981412, 2.8026276923295095),\n",
       " (0.990197752073304, 13.268649877782273),\n",
       " (0.018568197321467528, 0.24881384410766483),\n",
       " (0.7502582361277619, 10.05346036411201),\n",
       " (0.42390673011944013, 6.680350183600499),\n",
       " (0.8458035566622677, 19.333767659274386),\n",
       " (0.16108818315359952, 9.158581654258233),\n",
       " (0.5748596921831854, 11.703119875254686),\n",
       " (0.33543375780728546, 6.4948123546176255),\n",
       " (0.3723464199820531, 9.989442027759512),\n",
       " (0.8425340335104387, 13.28995604903988),\n",
       " (0.5532597300497378, 8.413680382666488)]"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "myself_knn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "most_similars [(0.5946528185601869, 17.968347768706504), (0.8425340335104387, 13.28995604903988), (0.9910392419681112, 16.27992584237269), (0.6190506969427724, 11.29527933903315), (0.8042504727317771, 10.776956334605813)]\n",
      "y_hats [17.968347768706504, 13.28995604903988, 16.27992584237269, 11.29527933903315, 10.776956334605813]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "13.922093066751609"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict(0.9)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. Complete the unfinished KNN Model using pure python to solve the previous Line-Regression problem. (8 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<评阅点>:\n",
    "> + 是否完成了KNN模型 (4')\n",
    "+ 是否能够预测新的数据 (4')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model(X, y):\n",
    "    # 直接存储X,y即可\n",
    "    return [(Xi, yi) for Xi, yi in zip(X, y)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.spatial.distance import cosine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "def distance(x1, x2):\n",
    "    return cosine(x1, x2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(x, k=5):\n",
    "    # 在predict的时候，需要做大量的计算\n",
    "    most_similars = sorted(model(X, y), key=lambda xi: distance(xi[0], x))[:k]\n",
    "    \n",
    "    y_hats = [_y for x, _y in most_similars]\n",
    "    \n",
    "    print(\"most_similars\", most_similars)\n",
    "    \n",
    "    print(\"y_hats\", y_hats)\n",
    "    \n",
    "    return np.mean(y_hats)\n",
    "# -> regression: numerical -> most_similars(y)\n",
    "# -> classification: categorical -> most_similar (y)\n",
    "\n",
    "# 已经获得了最相似的数据集\n",
    "# 然后呢，Counter() -> most_common() -> 就可以获得出现最多的这个y了"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([0.09260585, 0.87987019, 0.3461554 , 0.19884436, 0.66154601,\n",
       "        0.50901261, 0.03622195, 0.90977464, 0.4120157 , 0.38990702,\n",
       "        0.07209075, 0.13706614, 0.54245347, 0.27548141, 0.89501426,\n",
       "        0.79212703, 0.0823391 , 0.64782732, 0.35417003, 0.58841281]),\n",
       " array([0.52619691, 0.45411523, 0.49032669, 0.06828408, 0.72578183,\n",
       "        0.45794579, 0.58920022, 0.40902253, 0.98643563, 0.74573986,\n",
       "        0.30265318, 0.15176903, 0.27721706, 0.62998253, 0.08267824,\n",
       "        0.28925074, 0.86111224, 0.08290958, 0.31936838, 0.30323034]))"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "myself_knn = model(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0.09260585224660478, 0.5261969146396398),\n",
       " (0.8798701908638658, 0.45411523399135745),\n",
       " (0.34615539757700964, 0.4903266918258711),\n",
       " (0.19884435683979906, 0.06828408415191178),\n",
       " (0.6615460065799738, 0.7257818264839189),\n",
       " (0.5090126136618282, 0.4579457896203202),\n",
       " (0.03622194861154904, 0.5892002159487495),\n",
       " (0.9097746357492839, 0.4090225250821288),\n",
       " (0.412015704811307, 0.9864356321118465),\n",
       " (0.3899070217184064, 0.7457398570685915),\n",
       " (0.0720907527849749, 0.3026531764399957),\n",
       " (0.13706614289823893, 0.1517690347942312),\n",
       " (0.5424534690484876, 0.2772170577101679),\n",
       " (0.27548140915671615, 0.6299825324442834),\n",
       " (0.8950142558038954, 0.082678243218402),\n",
       " (0.7921270272774245, 0.28925074232068726),\n",
       " (0.08233909924025062, 0.861112241742074),\n",
       " (0.6478273153222663, 0.08290958172409313),\n",
       " (0.35417003243540013, 0.3193683767000354),\n",
       " (0.5884128108206848, 0.3032303424239944)]"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "myself_knn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "most_similars [(0.5090126136618282, 0.4579457896203202), (0.03622194861154904, 0.5892002159487495), (0.7921270272774245, 0.28925074232068726), (0.09260585224660478, 0.5261969146396398), (0.8798701908638658, 0.45411523399135745)]\n",
      "y_hats [0.4579457896203202, 0.5892002159487495, 0.28925074232068726, 0.5261969146396398, 0.45411523399135745]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.46334177930415077"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict(0.9)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. Re-code the Decision Tree, which could sort the features by salience. (12 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<评阅点>\n",
    "> + 是否实现了信息熵 (1' )\n",
    "+ 是否实现了最优先特征点的选择(5')\n",
    "+ 是否实现了持续的特征选则(6')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "from icecream import ic\n",
    "\n",
    "from collections import Counter\n",
    "\n",
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [],
   "source": [
    "mock_data = {\n",
    "    'gender':['F', 'F', 'F', 'F', 'M', 'M', 'M'],\n",
    "    'income': ['+10', '-10', '+10', '+10', '+10', '+10', '-10'],\n",
    "    'family_number': [1, 1, 2, 1, 1, 1, 2],\n",
    "   # 'pet': [1, 1, 1, 0, 0, 0, 1],\n",
    "    'bought': [1, 1, 1, 0, 0, 0, 1],\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [],
   "source": [
    "mock_data = {\n",
    "    'gender':['F', 'F', 'F', 'F', 'F', 'M', 'M'],\n",
    "    'income': ['+10', '-10', '+10', '+10', '+10', '+10', '-10'],\n",
    "    'family_number': [1, 1, 2, 1, 1, 1, 2],\n",
    "   # 'pet': [1, 1, 1, 0, 0, 0, 1],\n",
    "    'bought': [1, 1, 1, 0, 0, 1, 0],\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = pd.DataFrame.from_dict(mock_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>gender</th>\n",
       "      <th>income</th>\n",
       "      <th>family_number</th>\n",
       "      <th>bought</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>F</td>\n",
       "      <td>+10</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>F</td>\n",
       "      <td>-10</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>F</td>\n",
       "      <td>+10</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>F</td>\n",
       "      <td>+10</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>M</td>\n",
       "      <td>+10</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>M</td>\n",
       "      <td>+10</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>M</td>\n",
       "      <td>-10</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  gender income  family_number  bought\n",
       "0      F    +10              1       1\n",
       "1      F    -10              1       1\n",
       "2      F    +10              2       1\n",
       "3      F    +10              1       0\n",
       "4      M    +10              1       0\n",
       "5      M    +10              1       0\n",
       "6      M    -10              2       1"
      ]
     },
     "execution_count": 189,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ic| f: 'income'\n",
      "ic| values: {'+10', '-10'}\n",
      "ic| sub_spliter_1: [1, 1, 0, 0, 1]\n",
      "ic| entropy_1: 0.6730116670092565\n",
      "ic| sub_spliter_2: [1, 0]\n",
      "ic| entropy_2: 0.6931471805599453\n",
      "ic| entropy_v: 1.366158847569202\n",
      "ic| sub_spliter_1: [1, 0]\n",
      "ic| entropy_1: 0.6931471805599453\n",
      "ic| sub_spliter_2: [1, 1, 0, 0, 1]\n",
      "ic| entropy_2: 0.6730116670092565\n",
      "ic| entropy_v: 1.366158847569202\n",
      "ic| f: 'gender'\n",
      "ic| values: {'M', 'F'}\n",
      "ic| sub_spliter_1: [1, 0]\n",
      "ic| entropy_1: 0.6931471805599453\n",
      "ic| sub_spliter_2: [1, 1, 1, 0, 0]\n",
      "ic| entropy_2: 0.6730116670092565\n",
      "ic| entropy_v: 1.366158847569202\n",
      "ic| sub_spliter_1: [1, 1, 1, 0, 0]\n",
      "ic| "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'income', 'gender', 'family_number'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "entropy_1: 0.6730116670092565\n",
      "ic| sub_spliter_2: [1, 0]\n",
      "ic| entropy_2: 0.6931471805599453\n",
      "ic| entropy_v: 1.366158847569202\n",
      "ic| f: 'family_number'\n",
      "ic| values: {1, 2}\n",
      "ic| sub_spliter_1: [1, 1, 0, 0, 1]\n",
      "ic| entropy_1: 0.6730116670092565\n",
      "ic| sub_spliter_2: [1, 0]\n",
      "ic| entropy_2: 0.6931471805599453\n",
      "ic| entropy_v: 1.366158847569202\n",
      "ic| sub_spliter_1: [1, 0]\n",
      "ic| entropy_1: 0.6931471805599453\n",
      "ic| sub_spliter_2: [1, 1, 0, 0, 1]\n",
      "ic| entropy_2: 0.6730116670092565\n",
      "ic| entropy_v: 1.366158847569202\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "spliter is: ('family_number', 1, 1.366158847569202)\n",
      "the min entropy is: 1.366158847569202\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "('family_number', 1, 1.366158847569202)"
      ]
     },
     "execution_count": 179,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "find_the_optimal_spilter1(dataset, 'bought')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>gender</th>\n",
       "      <th>income</th>\n",
       "      <th>family_number</th>\n",
       "      <th>bought</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>F</td>\n",
       "      <td>-10</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  gender income  family_number  bought\n",
       "1      F    -10              1       1"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[1:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: []\n",
       "Index: []"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>gender</th>\n",
       "      <th>income</th>\n",
       "      <th>family_number</th>\n",
       "      <th>bought</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>F</td>\n",
       "      <td>+10</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>F</td>\n",
       "      <td>-10</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>F</td>\n",
       "      <td>+10</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>M</td>\n",
       "      <td>+10</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>M</td>\n",
       "      <td>+10</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  gender income  family_number  bought\n",
       "0      F    +10              1       1\n",
       "1      F    -10              1       1\n",
       "3      F    +10              1       0\n",
       "4      M    +10              1       0\n",
       "5      M    +10              1       0"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "splited_data = dataset[dataset['family_number'] == 1]\n",
    "splited_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "entropy([0, 0, 0, 0, 0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# you code here\n",
    "import numpy as np\n",
    "from icecream import ic\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [],
   "source": [
    "#信息熵\n",
    "def entropy(elements):\n",
    "    counter = Counter(elements)\n",
    "    #计算每个set的概率\n",
    "    probs = [counter[c] / len(elements) for c in set(elements)]\n",
    "    return - sum(p * np.log(p) for p in probs)\n",
    "\n",
    "#找到信息熵最小的列\n",
    "def find_the_optimal_spilter(training_data: pd.DataFrame, target: str) -> str:\n",
    "    x_fields = set(training_data.columns.tolist()) - {target}\n",
    "    # pdb.set_trace()\n",
    "    spliter = None\n",
    "    min_entropy = float('inf')\n",
    "    \n",
    "    for f in x_fields:\n",
    "        ic(f)\n",
    "        values = set(training_data[f])\n",
    "        ic(values)\n",
    "        for v in values:\n",
    "            sub_spliter_1 = training_data[training_data[f] == v][target].tolist()\n",
    "            #ic(sub_spliter_1)\n",
    "\n",
    "            entropy_1 = entropy(sub_spliter_1)\n",
    "            #ic(entropy_1)\n",
    "            \n",
    "            sub_spliter_2 = training_data[training_data[f] != v][target].tolist()\n",
    "            #ic(sub_spliter_2)\n",
    "            \n",
    "            entropy_2 = entropy(sub_spliter_2)\n",
    "            #ic(entropy_2)\n",
    "            \n",
    "            entropy_v = entropy_1 + entropy_2\n",
    "            #ic(entropy_v)\n",
    "            # entropy_1 = 0控制有无纯的数据，如果没有，则无需分割(决策树终止条件)\n",
    "            # entropy_1 <= entropy_2来控制getDecisionTree分割entropy不为0的数据，即更不纯的数据\n",
    "            if entropy_v <= min_entropy and entropy_1 <= entropy_2:\n",
    "                min_entropy = entropy_v\n",
    "                spliter = (f, v , entropy_1, entropy_2)\n",
    "    \n",
    "    #print('spliter is: {}'.format(spliter))\n",
    "    #print('the min entropy is: {}'.format(min_entropy))\n",
    "    return spliter\n",
    "import pdb\n",
    "\n",
    "def getDecisionTree(training_data: pd.DataFrame, target: str, printprocess = True):\n",
    "    # pdb.set_trace()\n",
    "    decisionTree = {}\n",
    "    while True:\n",
    "        \n",
    "        #查找\n",
    "        (key, value, entropy1, entropy2) = find_the_optimal_spilter(training_data=training_data, target=target)\n",
    "        decisionTree[key] = value\n",
    "        # 信息熵为0直接返回，找到最优解\n",
    "        if (entropy1 + entropy2 == 0):\n",
    "            break\n",
    "        # 两组数据的entropy都不为0，则没有纯的数据，决策树停止\n",
    "        if (min(entropy1, entropy2) != 0):\n",
    "            break\n",
    "        # 获取新的dataset\n",
    "        training_data = training_data[training_data[key] != value]\n",
    "        # 移除已经计算的列\n",
    "        training_data = training_data.drop([key],axis=1)\n",
    "\n",
    "        # 如果已经没有可以移除的列，则跳出(此处判断需要排除target列))\n",
    "        if len(training_data.columns) == 1:\n",
    "            print('training_data.columns:', training_data.columns)\n",
    "            break\n",
    "        # 如果没有可用数据跳出循环\n",
    "        if training_data.shape[0] == 0:\n",
    "            print('training_data.shape:', training_data.shape)\n",
    "            break\n",
    "    # 是否打印决策过程         \n",
    "    if printprocess:\n",
    "        i = 0\n",
    "        for k,v in decisionTree.items():\n",
    "            i += 1\n",
    "            print('第{0}次使用列 \"{1}\" = {2} 进行分割。'.format(i,k,v))\n",
    "    return decisionTree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>gender</th>\n",
       "      <th>income</th>\n",
       "      <th>bought</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>F</td>\n",
       "      <td>+10</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>F</td>\n",
       "      <td>-10</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>F</td>\n",
       "      <td>+10</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>F</td>\n",
       "      <td>+10</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>M</td>\n",
       "      <td>+10</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  gender income  bought\n",
       "0      F    +10       1\n",
       "1      F    -10       1\n",
       "3      F    +10       0\n",
       "4      F    +10       0\n",
       "5      M    +10       1"
      ]
     },
     "execution_count": 182,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ex_family_dataset = dataset[dataset['family_number']==1].drop('family_number', axis=1)\n",
    "ex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "getDecisionTree(training_data=ex_family_dataset, target='bought')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ic| f: 'income'\n",
      "ic| values: {'+10', '-10'}\n",
      "ic| sub_spliter_1: [1, 1, 0, 0, 0]\n",
      "ic| entropy_1: 0.6730116670092565\n",
      "ic| sub_spliter_2: [1, 1]\n",
      "ic| entropy_2: -0.0\n",
      "ic| entropy_v: 0.6730116670092565\n",
      "ic| sub_spliter_1: [1, 1]\n",
      "ic| entropy_1: -0.0\n",
      "ic| sub_spliter_2: [1, 1, 0, 0, 0]\n",
      "ic| entropy_2: 0.6730116670092565\n",
      "ic| entropy_v: 0.6730116670092565\n",
      "ic| f: 'gender'\n",
      "ic| values: {'M', 'F'}\n",
      "ic| sub_spliter_1: [0, 0, 1]\n",
      "ic| entropy_1: 0.6365141682948128\n",
      "ic| sub_spliter_2: [1, 1, 1, 0]\n",
      "ic| entropy_2: 0.5623351446188083\n",
      "ic| entropy_v: 1.198849312913621\n",
      "ic| sub_spliter_1: [1, 1, 1, 0]\n",
      "ic| entropy_1: 0.5623351446188083\n",
      "ic| sub_spliter_2: [0, 0, 1]\n",
      "ic| entropy_2: 0.6365141682948128\n",
      "ic| entropy_v: 1.198849312913621\n",
      "ic| f: 'family_number'\n",
      "ic| values: {1, 2}\n",
      "ic| sub_spliter_1: [1, 1, 0, 0, 0]\n",
      "ic| entropy_1: 0.6730116670092565\n",
      "ic| sub_spliter_2: [1, 1]\n",
      "ic| entropy_2: -0.0\n",
      "ic| entropy_v: 0.6730116670092565\n",
      "ic| sub_spliter_1: [1, 1]\n",
      "ic| entropy_1: -0.0\n",
      "ic| sub_spliter_2: [1, 1, 0, 0, 0]\n",
      "ic| entropy_2: 0.6730116670092565\n",
      "ic| entropy_v: 0.6730116670092565\n",
      "ic| f: 'income'\n",
      "ic| values: {'+10', '-10'}\n",
      "ic| sub_spliter_1: [1, 0, 0, 0]\n",
      "ic| entropy_1: 0.5623351446188083\n",
      "ic| sub_spliter_2: [1]\n",
      "ic| entropy_2: -0.0\n",
      "ic| entropy_v: 0.5623351446188083\n",
      "ic| sub_spliter_1: [1]\n",
      "ic| entropy_1: -0.0\n",
      "ic| sub_spliter_2: [1, 0, 0, 0]\n",
      "ic| entropy_2: 0.5623351446188083\n",
      "ic| entropy_v: 0.5623351446188083\n",
      "ic| f: 'gender'\n",
      "ic| values: {'M', 'F'}\n",
      "ic| sub_spliter_1: [0, 0]\n",
      "ic| entropy_1: -0.0\n",
      "ic| sub_spliter_2: [1, 1, 0]\n",
      "ic| entropy_2: 0.6365141682948128\n",
      "ic| entropy_v: 0.6365141682948128\n",
      "ic| sub_spliter_1: [1, 1, 0]\n",
      "ic| entropy_1: 0.6365141682948128\n",
      "ic| sub_spliter_2: [0, 0]\n",
      "ic| entropy_2: -0.0\n",
      "ic| entropy_v: 0.6365141682948128\n",
      "ic| f: 'gender'\n",
      "ic| values: {'M', 'F'}\n",
      "ic| sub_spliter_1: [0, 0]\n",
      "ic| entropy_1: -0.0\n",
      "ic| sub_spliter_2: [1, 0]\n",
      "ic| entropy_2: 0.6931471805599453\n",
      "ic| entropy_v: 0.6931471805599453\n",
      "ic| sub_spliter_1: [1, 0]\n",
      "ic| entropy_1: 0.6931471805599453\n",
      "ic| sub_spliter_2: [0, 0]\n",
      "ic| entropy_2: -0.0\n",
      "ic| entropy_v: 0.6931471805599453\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training_data.columns: Index(['bought'], dtype='object')\n",
      "第1次使用列 \"family_number\" = 2 进行分割。\n",
      "第2次使用列 \"income\" = -10 进行分割。\n",
      "第3次使用列 \"gender\" = M 进行分割。\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'family_number': 2, 'income': '-10', 'gender': 'M'}"
      ]
     },
     "execution_count": 194,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "getDecisionTree(training_data=dataset, target='bought')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>gender</th>\n",
       "      <th>bought</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>F</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>F</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>M</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>M</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  gender  bought\n",
       "0      F       1\n",
       "1      F       0\n",
       "2      M       0\n",
       "3      M       1"
      ]
     },
     "execution_count": 157,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mock_data_gender = {\n",
    "    'gender':['F', 'F', 'M', 'M'],\n",
    "    'bought': [1, 0, 0, 1],\n",
    "}\n",
    "dataset_gender = pd.DataFrame.from_dict(mock_data_gender)\n",
    "dataset_gender"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ic| f: 'gender'\n",
      "ic| values: {'M', 'F'}\n",
      "ic| sub_spliter_1: [0, 1]\n",
      "ic| entropy_1: 0.6931471805599453\n",
      "ic| sub_spliter_2: [1, 0]\n",
      "ic| entropy_2: 0.6931471805599453\n",
      "ic| entropy_v: 1.3862943611198906\n",
      "ic| sub_spliter_1: [1, 0]\n",
      "ic| entropy_1: 0.6931471805599453\n",
      "ic| sub_spliter_2: [0, 1]\n",
      "ic| entropy_2: 0.6931471805599453\n",
      "ic| entropy_v: 1.3862943611198906\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'gender'}\n",
      "spliter is: ('gender', 'F', 1.3862943611198906)\n",
      "the min entropy is: 1.3862943611198906\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "('gender', 'F', 1.3862943611198906)"
      ]
     },
     "execution_count": 161,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "find_the_optimal_spilter1(dataset_gender, 'bought')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> <ipython-input-140-4db10fb9e79c>(51)getDecisionTree()\n",
      "-> decisionTree = {}\n",
      "(Pdb) c\n",
      "> <ipython-input-140-4db10fb9e79c>(17)find_the_optimal_spilter()\n",
      "-> spliter = None\n",
      "(Pdb) c\n",
      "第1次使用列 \"gender\" = M 进行分割。\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'gender': 'M'}"
      ]
     },
     "execution_count": 145,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "getDecisionTree(training_data=dataset_gender, target='bought')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4. Finish the K-Means using 2-D matplotlib (8 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<评阅点>\n",
    "> + 是否完成了KMeans模型，基于scikit-learning (3')\n",
    "+ 是否完成了可视化任务（5'）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "X1 = [random.randint(0, 100) for _ in range(100)]\n",
    "X2 = [random.randint(0, 100) for _ in range(100)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.collections.PathCollection at 0x7feaa9f879d0>"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.scatter(X1, X2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data = [[x1, x2] for x1, x2 in zip(X1, X2)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster = KMeans(n_clusters=6, max_iter=500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "KMeans(algorithm='auto', copy_x=True, init='k-means++', max_iter=500,\n",
       "       n_clusters=6, n_init=10, n_jobs=None, precompute_distances='auto',\n",
       "       random_state=None, tol=0.0001, verbose=0)"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cluster.fit(training_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[74.94444444, 13.44444444],\n",
       "       [69.4       , 56.2       ],\n",
       "       [21.28571429, 86.78571429],\n",
       "       [86.        , 87.54545455],\n",
       "       [20.72222222, 15.5       ],\n",
       "       [23.57894737, 46.89473684]])"
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cluster.cluster_centers_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([5, 1, 4, 1, 2, 2, 3, 1, 0, 3, 0, 5, 1, 0, 4, 1, 2, 5, 5, 4, 4, 5,\n",
       "       2, 5, 0, 1, 5, 5, 4, 5, 2, 5, 0, 3, 4, 3, 2, 0, 5, 4, 4, 3, 0, 1,\n",
       "       0, 5, 2, 0, 2, 5, 2, 4, 1, 5, 0, 3, 4, 0, 5, 0, 5, 4, 4, 2, 5, 1,\n",
       "       0, 1, 3, 3, 1, 2, 1, 5, 1, 2, 0, 3, 2, 5, 1, 0, 4, 0, 2, 1, 4, 1,\n",
       "       1, 4, 4, 3, 4, 1, 4, 0, 0, 1, 3, 1], dtype=int32)"
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cluster.labels_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "centers = defaultdict(list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "for label, location in zip(cluster.labels_, training_data):\n",
    "    centers[label].append(location)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "color = ['red', 'green', 'grey', 'black', 'yellow', 'orange']\n",
    "\n",
    "for i, c in enumerate(centers):\n",
    "    for location in centers[c]:\n",
    "        plt.scatter(*location, c=color[i])\n",
    "        \n",
    "for center in cluster.cluster_centers_:\n",
    "     plt.scatter(*center, s=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part-2 Question and Answer 问答"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. What's the *model*? why  all the models are wrong, but some are useful? (5 points) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ans:\n",
    "### Model:\n",
    "The model is the mathematical model used to describe the objective world, and the model is abstracted from the data. In data analysis, we usually have only data on hand, and then look at the data to find the law, the rule is the model.Just like when I was a kid in a number game,1,4,16,...()...256. What's in parentheses? Only by abstracting the strings into models can we know what is in parentheses. In fact, when we were very young to contact with machine learning, but at that time only the examination, without these in-depth thinking.\n",
    "\n",
    "### Reason:\n",
    "Every model involves many features, we need to choose different features in specific case"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<评阅点>\n",
    "> + 对模型的理解是否正确,对模型的抽象性是否正确(5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. What's the underfitting and overfitting? List the reasons that could make model overfitting or underfitting. (10 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ans:\n",
    "### Underfitting:\n",
    "Underfitting refers to a model that can neither model the training data nor generalize to new data.\n",
    "\n",
    "An underfit machine learning model is not a suitable model and will be obvious as it will have poor performance on the training data.\n",
    "\n",
    "Underfitting is often not discussed as it is easy to detect given a good performance metric. The remedy is to move on and try alternate machine learning algorithms. Nevertheless, it does provide a good contrast to the problem of overfitting.\n",
    "\n",
    "### Overfitting:\n",
    "Overfitting refers to a model that models the training data too well.\n",
    "For example, decision trees are a nonparametric machine learning algorithm that is very flexible and is subject to overfitting training data. This problem can be addressed by pruning a tree after it has learned in order to remove some of the detail it has picked up.\n",
    "\n",
    "### The reasons that make model overfitting:\n",
    "1. Model complexity is high\n",
    "2. little training data\n",
    "3. Small training error\n",
    "4. large test error\n",
    "### The reasons that make model underfitting:\n",
    "1. Model complexity is low \n",
    "2. can't fit all the data well \n",
    "3. High training error "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<评阅点>\n",
    "> + 对过拟合和欠拟合的理解是否正确 (3')\n",
    "+ 对欠拟合产生的原因是否理解正确(2')\n",
    "+ 对过拟合产生的原因是否理解正确(5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. What's the precision, recall, AUC, F1, F2score. What are they mainly target on? (12')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ans:\n",
    "### Precision：\n",
    "all say 'yes' and correct / all say yes\n",
    "precision = true positive / true positive + false positive\n",
    "#### Target:\n",
    "How many people with positive pneumonia actually get pneumonia \n",
    "### Recall:\n",
    "all say 'yes' and correct / all truely yes\n",
    "recall = true positive / true positive + false negative\n",
    "#### Target:\n",
    "How many people were tested for true pneumonia \n",
    "### AUC:\n",
    "#### Roc：\n",
    "A receiver operating characteristic curve, or ROC curve, is a graphical plot that illustrates the diagnostic ability of a binary classifier system as its discrimination threshold is varied.\n",
    "\n",
    "The X axis is False Positive Rate, while the y axis is True Positive Rate.The data in the Roc curve indicates the model effect as the classification threshold varies.When the classifier(predictive model) is the model random guessing, the Roc is a linear line(from (0,0) to (1,1))\n",
    "\n",
    "#### Auc:\n",
    "Area under the curve, the larger the Auc is, the better the model is.To be specific:\n",
    "\n",
    "AUC =1, is the perfect classifier, when adopting this prediction model, there is at least one threshold to get the perfect prediction. most predictive occasions, there is no perfect classifier. \n",
    "\n",
    "0.5< AUC <1, better than random guesses. This classifier (model) can have predictive value if it sets the threshold properly. \n",
    "\n",
    "AUC =0.5, as with random guesses (example: copper throw), the model has no predictive value. \n",
    "\n",
    "AUC <0.5, worse than random guesses; but better than random guesses as long as they are always counter-predicted. \n",
    "\n",
    "#### Target:\n",
    "The standard of messuring the model\n",
    "\n",
    "### F1 score:\n",
    " $F_1 score =  \\frac{2*precision*recall}{precision + recall} $\n",
    "#### Target:\n",
    "In that Precision and Recall always trade off, we use F1 to balance them(comprehensive consideration)\n",
    "\n",
    "### F2 score:\n",
    "$F_{\\beta}$ $score = $ $(1+{\\beta}^{2})$ $\\frac{precision * recall}{({\\beta})^{2}precision + recall}$¶\n",
    "#### Target:\n",
    "if beta = 1 precision and recall is equal, actually like F1 score\n",
    "if beta > 1 recall is more important\n",
    "if beta < 1 precision is more important\n",
    "To conclude that, F2 score can justify the weight of precision and recall in the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<评阅点>\n",
    "> + 对precision, recall, AUC, F1, F2 理解是否正确(6‘)\n",
    "+ 对precision, recall, AUC, F1, F2的使用侧重点是否理解正确 (6’)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4. Based on our course and yourself mind, what's the machine learning?  (8')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ans:\n",
    "Machine learning：\n",
    "As Arhur Samuel's definition, \"Field of study that gives computers the ability to learn without being explicitly programmed\".\n",
    "\n",
    "In my humble opinion, Machine Learning is a way of thinking that the model is constantly optimized through the observed data.\n",
    "\n",
    "To be exac, it is also a type of Artificial Intelligence that enables the programmers to write programs in a more simple way. It focuses more on developing programs that teach computers to change when exposed to new data and to grow. Its goal is to understand and follow the methods by using algorithms to do that task automatically without any human assistance.\n",
    "\n",
    "In terms of traditional programming, developers should write a huge number of 'if-else' logic codes.If the demmands are changed, they should rewrite, whose cost is enormous.\n",
    "\n",
    "Nevertheless, machine learning is a thinking way that is 'data driven', meaning we programmers do not need change logic codes again and again, we can just update the data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<评阅点> 开放式问题，是否能说出来机器学习这种思维方式和传统的分析式编程的区别（8'）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5. \"正确定义了机器学习模型的评价标准(evaluation)， 问题基本上就已经解决一半\". 这句话是否正确？你是怎么看待的？ (8‘)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I agree with the idea to some extent.The standard of the ML model is significant.However, the training data is more crucial."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<评阅点> 开放式问题，主要看能理解评价指标对机器学习模型的重要性."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part-03 Programming Practice 编程练习"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. In our course and previous practice, we complete some importance components of Decision Tree. In this problem, you need to build a **completed** Decision Tree Model. You show finish a `predicate()` function, which accepts three parameters **<gender, income, family_number>**, and outputs the predicated 'bought': 1 or 0.  (20 points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ic| f: 'income'\n",
      "ic| values: {'+10', '-10'}\n",
      "ic| f: 'gender'\n",
      "ic| values: {'M', 'F'}\n",
      "ic| f: 'family_number'\n",
      "ic| values: {1, 2}\n",
      "ic| f: 'income'\n",
      "ic| values: {'+10', '-10'}\n",
      "ic| f: 'gender'\n",
      "ic| values: {'M', 'F'}\n",
      "ic| f: 'gender'\n",
      "ic| values: {'M', 'F'}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training_data.columns: Index(['bought'], dtype='object')\n",
      "第1次使用列 \"family_number\" = 2 进行分割。\n",
      "第2次使用列 \"income\" = -10 进行分割。\n",
      "第3次使用列 \"gender\" = M 进行分割。\n",
      "{'family_number': 2, 'income': '-10', 'gender': 'M'}\n"
     ]
    }
   ],
   "source": [
    "tree = getDecisionTree(dataset, 'bought')\n",
    "print(decisionTree)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ic| f: 'income'\n",
      "ic| values: {'+10', '-10'}\n",
      "ic| f: 'gender'\n",
      "ic| values: {'M', 'F'}\n",
      "ic| f: 'family_number'\n",
      "ic| values: {1, 2}\n",
      "ic| f: 'income'\n",
      "ic| values: {'+10', '-10'}\n",
      "ic| f: 'gender'\n",
      "ic| values: {'M', 'F'}\n",
      "ic| f: 'gender'\n",
      "ic| values: {'M', 'F'}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training_data.columns: Index(['bought'], dtype='object')\n",
      "第1次使用列 \"family_number\" = 2 进行分割。\n",
      "第2次使用列 \"income\" = -10 进行分割。\n",
      "第3次使用列 \"gender\" = M 进行分割。\n"
     ]
    }
   ],
   "source": [
    "def predicate(gender, income, family_number, tree = getDecisionTree(dataset, 'bought')):\n",
    "    user = {}\n",
    "    user['gender'] = gender\n",
    "    user['income'] = income\n",
    "    user['family_number'] = family_number\n",
    "    for k, v in tree.items():\n",
    "        print('k:', k)\n",
    "        print('v:', v)\n",
    "        if (v == user[k]):\n",
    "            return 1\n",
    "        break\n",
    "    return 0\n",
    "        \n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "k: family_number\n",
      "v: 2\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 221,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predicate('F', '+10', 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<评阅点>\n",
    "> + 是否将之前的决策树模型的部分进行合并组装， predicate函数能够顺利运行(8')\n",
    "+ 是够能够输入未曾见过的X变量，例如gender, income, family_number 分别是： <M, -10, 1>, 模型能够预测出结果 (12')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. 将上一节课(第二节课)的线性回归问题中的Loss函数改成\"绝对值\"，并且改变其偏导的求值方式，观察其结果的变化。(19 point)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_boston\n",
    "import random\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "dataset = load_boston() #加载数据集\n",
    "\n",
    "x,y = dataset['data'],dataset['target']\n",
    "\n",
    "X_rm = x[:,5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def price(rm, k, b):\n",
    "    return k*rm + b\n",
    "def loss(y, y_hat):\n",
    "    return sum(np.abs(y_i - y_hat_i) for y_i, y_hat_i in zip(list(y), list(y_hat)))/len(list(y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define partial derivative\n",
    "def partial_derivative_k(x, y, y_hat):\n",
    "    n = len(y)\n",
    "    gradient = 0\n",
    "    for x_i, y_i, y_hat_i in zip(list(x), list(y), list(y_hat)):\n",
    "        if y_i > y_hat_i:\n",
    "            partial_k = -x_i\n",
    "        if y_i == y_hat_i:\n",
    "            partial_k = 0\n",
    "        if y_i < y_hat_i:\n",
    "            partial_k = x_i\n",
    "        gradient +=  partial_k\n",
    "    return 1 / n * gradient\n",
    "\n",
    "def partial_derivative_b(y, y_hat):\n",
    "    n = len(y)\n",
    "    gradient = 0\n",
    "    for y_i, y_hat_i in zip(list(y), list(y_hat)):\n",
    "        if y_i > y_hat_i:\n",
    "            partial_b = -1\n",
    "        if y_i == y_hat_i:\n",
    "            partial_b = 0\n",
    "        if y_i < y_hat_i:\n",
    "            partial_b = 1\n",
    "        gradient += partial_b\n",
    "    return 1 / n * gradient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 0, the loss is 254.0545283041111, parameters k is 47.48226321203717 and b is -21.82132954343541\n",
      "Iteration 1, the loss is 253.64956201028397, parameters k is 47.41941686816365 and b is -21.831329543435412\n",
      "Iteration 2, the loss is 253.24459571645752, parameters k is 47.356570524290134 and b is -21.841329543435414\n",
      "Iteration 3, the loss is 252.83962942263062, parameters k is 47.29372418041662 and b is -21.851329543435416\n",
      "Iteration 4, the loss is 252.43466312880355, parameters k is 47.2308778365431 and b is -21.861329543435417\n",
      "Iteration 5, the loss is 252.0296968349767, parameters k is 47.168031492669584 and b is -21.87132954343542\n",
      "Iteration 6, the loss is 251.62473054114994, parameters k is 47.10518514879607 and b is -21.88132954343542\n",
      "Iteration 7, the loss is 251.21976424732296, parameters k is 47.04233880492255 and b is -21.891329543435422\n",
      "Iteration 8, the loss is 250.8147979534963, parameters k is 46.979492461049034 and b is -21.901329543435423\n",
      "Iteration 9, the loss is 250.40983165966918, parameters k is 46.91664611717552 and b is -21.911329543435425\n",
      "Iteration 10, the loss is 250.0048653658426, parameters k is 46.853799773302 and b is -21.921329543435427\n",
      "Iteration 11, the loss is 249.59989907201583, parameters k is 46.790953429428484 and b is -21.931329543435428\n",
      "Iteration 12, the loss is 249.19493277818896, parameters k is 46.72810708555497 and b is -21.94132954343543\n",
      "Iteration 13, the loss is 248.789966484362, parameters k is 46.66526074168145 and b is -21.95132954343543\n",
      "Iteration 14, the loss is 248.38500019053507, parameters k is 46.602414397807934 and b is -21.961329543435433\n",
      "Iteration 15, the loss is 247.98003389670848, parameters k is 46.53956805393442 and b is -21.971329543435434\n",
      "Iteration 16, the loss is 247.57506760288155, parameters k is 46.4767217100609 and b is -21.981329543435436\n",
      "Iteration 17, the loss is 247.17010130905484, parameters k is 46.41387536618738 and b is -21.991329543435437\n",
      "Iteration 18, the loss is 246.7651350152278, parameters k is 46.35102902231387 and b is -22.00132954343544\n",
      "Iteration 19, the loss is 246.3601687214008, parameters k is 46.28818267844035 and b is -22.01132954343544\n",
      "Iteration 20, the loss is 245.95520242757428, parameters k is 46.22533633456683 and b is -22.021329543435442\n",
      "Iteration 21, the loss is 245.55023613374766, parameters k is 46.16248999069332 and b is -22.031329543435444\n",
      "Iteration 22, the loss is 245.14526983992056, parameters k is 46.0996436468198 and b is -22.041329543435445\n",
      "Iteration 23, the loss is 244.74030354609368, parameters k is 46.03679730294628 and b is -22.051329543435447\n",
      "Iteration 24, the loss is 244.33533725226692, parameters k is 45.973950959072766 and b is -22.06132954343545\n",
      "Iteration 25, the loss is 243.9303709584401, parameters k is 45.91110461519925 and b is -22.07132954343545\n",
      "Iteration 26, the loss is 243.52540466461332, parameters k is 45.84825827132573 and b is -22.08132954343545\n",
      "Iteration 27, the loss is 243.12043837078645, parameters k is 45.785411927452216 and b is -22.091329543435453\n",
      "Iteration 28, the loss is 242.71547207695923, parameters k is 45.7225655835787 and b is -22.101329543435455\n",
      "Iteration 29, the loss is 242.31050578313258, parameters k is 45.65971923970518 and b is -22.111329543435456\n",
      "Iteration 30, the loss is 241.90553948930585, parameters k is 45.596872895831666 and b is -22.121329543435458\n",
      "Iteration 31, the loss is 241.50057319547906, parameters k is 45.53402655195815 and b is -22.13132954343546\n",
      "Iteration 32, the loss is 241.09560690165185, parameters k is 45.47118020808463 and b is -22.14132954343546\n",
      "Iteration 33, the loss is 240.69064060782526, parameters k is 45.408333864211116 and b is -22.151329543435462\n",
      "Iteration 34, the loss is 240.28567431399821, parameters k is 45.3454875203376 and b is -22.161329543435464\n",
      "Iteration 35, the loss is 239.88070802017154, parameters k is 45.28264117646408 and b is -22.171329543435466\n",
      "Iteration 36, the loss is 239.47574172634492, parameters k is 45.219794832590566 and b is -22.181329543435467\n",
      "Iteration 37, the loss is 239.07077543251805, parameters k is 45.15694848871705 and b is -22.19132954343547\n",
      "Iteration 38, the loss is 238.66580913869126, parameters k is 45.09410214484353 and b is -22.20132954343547\n",
      "Iteration 39, the loss is 238.26084284486416, parameters k is 45.031255800970015 and b is -22.211329543435472\n",
      "Iteration 40, the loss is 237.8558765510374, parameters k is 44.9684094570965 and b is -22.221329543435473\n",
      "Iteration 41, the loss is 237.4509102572106, parameters k is 44.90556311322298 and b is -22.231329543435475\n",
      "Iteration 42, the loss is 237.0459439633838, parameters k is 44.842716769349465 and b is -22.241329543435477\n",
      "Iteration 43, the loss is 236.64097766955683, parameters k is 44.77987042547595 and b is -22.251329543435478\n",
      "Iteration 44, the loss is 236.23601137572965, parameters k is 44.71702408160243 and b is -22.26132954343548\n",
      "Iteration 45, the loss is 235.83104508190328, parameters k is 44.654177737728915 and b is -22.27132954343548\n",
      "Iteration 46, the loss is 235.42607878807632, parameters k is 44.5913313938554 and b is -22.281329543435483\n",
      "Iteration 47, the loss is 235.02111249424976, parameters k is 44.52848504998188 and b is -22.291329543435484\n",
      "Iteration 48, the loss is 234.61614620042275, parameters k is 44.465638706108365 and b is -22.301329543435486\n",
      "Iteration 49, the loss is 234.21117990659607, parameters k is 44.40279236223485 and b is -22.311329543435487\n",
      "Iteration 50, the loss is 233.806213612769, parameters k is 44.33994601836133 and b is -22.32132954343549\n",
      "Iteration 51, the loss is 233.40124731894235, parameters k is 44.277099674487815 and b is -22.33132954343549\n",
      "Iteration 52, the loss is 232.9962810251154, parameters k is 44.2142533306143 and b is -22.341329543435492\n",
      "Iteration 53, the loss is 232.5913147312886, parameters k is 44.15140698674078 and b is -22.351329543435494\n",
      "Iteration 54, the loss is 232.18634843746176, parameters k is 44.088560642867265 and b is -22.361329543435495\n",
      "Iteration 55, the loss is 231.7813821436348, parameters k is 44.02571429899375 and b is -22.371329543435497\n",
      "Iteration 56, the loss is 231.37641584980778, parameters k is 43.96286795512023 and b is -22.3813295434355\n",
      "Iteration 57, the loss is 230.97144955598125, parameters k is 43.900021611246714 and b is -22.3913295434355\n",
      "Iteration 58, the loss is 230.56648326215432, parameters k is 43.8371752673732 and b is -22.4013295434355\n",
      "Iteration 59, the loss is 230.16151696832736, parameters k is 43.77432892349968 and b is -22.411329543435503\n",
      "Iteration 60, the loss is 229.75655067450057, parameters k is 43.711482579626164 and b is -22.421329543435505\n",
      "Iteration 61, the loss is 229.35158438067378, parameters k is 43.64863623575265 and b is -22.431329543435506\n",
      "Iteration 62, the loss is 228.9466180868468, parameters k is 43.58578989187913 and b is -22.441329543435508\n",
      "Iteration 63, the loss is 228.54165179302035, parameters k is 43.522943548005614 and b is -22.45132954343551\n",
      "Iteration 64, the loss is 228.13668549919365, parameters k is 43.4600972041321 and b is -22.46132954343551\n",
      "Iteration 65, the loss is 227.73171920536646, parameters k is 43.39725086025858 and b is -22.471329543435512\n",
      "Iteration 66, the loss is 227.32675291153964, parameters k is 43.334404516385064 and b is -22.481329543435514\n",
      "Iteration 67, the loss is 226.92178661771288, parameters k is 43.27155817251155 and b is -22.491329543435516\n",
      "Iteration 68, the loss is 226.51682032388592, parameters k is 43.20871182863803 and b is -22.501329543435517\n",
      "Iteration 69, the loss is 226.11185403005913, parameters k is 43.145865484764514 and b is -22.51132954343552\n",
      "Iteration 70, the loss is 225.7068877362321, parameters k is 43.083019140891 and b is -22.52132954343552\n",
      "Iteration 71, the loss is 225.3019214424054, parameters k is 43.02017279701748 and b is -22.531329543435522\n",
      "Iteration 72, the loss is 224.8969551485787, parameters k is 42.95732645314396 and b is -22.541329543435523\n",
      "Iteration 73, the loss is 224.4919888547517, parameters k is 42.89448010927045 and b is -22.551329543435525\n",
      "Iteration 74, the loss is 224.0870225609247, parameters k is 42.83163376539693 and b is -22.561329543435527\n",
      "Iteration 75, the loss is 223.68205626709815, parameters k is 42.76878742152341 and b is -22.571329543435528\n",
      "Iteration 76, the loss is 223.27708997327125, parameters k is 42.705941077649896 and b is -22.58132954343553\n",
      "Iteration 77, the loss is 222.87212367944434, parameters k is 42.64309473377638 and b is -22.59132954343553\n",
      "Iteration 78, the loss is 222.46715738561755, parameters k is 42.58024838990286 and b is -22.601329543435533\n",
      "Iteration 79, the loss is 222.06219109179077, parameters k is 42.517402046029346 and b is -22.611329543435534\n",
      "Iteration 80, the loss is 221.6572247979639, parameters k is 42.45455570215583 and b is -22.621329543435536\n",
      "Iteration 81, the loss is 221.25225850413716, parameters k is 42.39170935828231 and b is -22.631329543435537\n",
      "Iteration 82, the loss is 220.84729221031006, parameters k is 42.328863014408796 and b is -22.64132954343554\n",
      "Iteration 83, the loss is 220.44232591648338, parameters k is 42.26601667053528 and b is -22.65132954343554\n",
      "Iteration 84, the loss is 220.03735962265657, parameters k is 42.20317032666176 and b is -22.661329543435542\n",
      "Iteration 85, the loss is 219.6323933288297, parameters k is 42.140323982788246 and b is -22.671329543435544\n",
      "Iteration 86, the loss is 219.22742703500302, parameters k is 42.07747763891473 and b is -22.681329543435545\n",
      "Iteration 87, the loss is 218.82246074117583, parameters k is 42.01463129504121 and b is -22.691329543435547\n",
      "Iteration 88, the loss is 218.41749444734913, parameters k is 41.951784951167696 and b is -22.70132954343555\n",
      "Iteration 89, the loss is 218.01252815352223, parameters k is 41.88893860729418 and b is -22.71132954343555\n",
      "Iteration 90, the loss is 217.60756185969547, parameters k is 41.82609226342066 and b is -22.72132954343555\n",
      "Iteration 91, the loss is 217.20259556586862, parameters k is 41.763245919547145 and b is -22.731329543435553\n",
      "Iteration 92, the loss is 216.7976292720417, parameters k is 41.70039957567363 and b is -22.741329543435555\n",
      "Iteration 93, the loss is 216.392662978215, parameters k is 41.63755323180011 and b is -22.751329543435556\n",
      "Iteration 94, the loss is 215.98769668438834, parameters k is 41.574706887926595 and b is -22.761329543435558\n",
      "Iteration 95, the loss is 215.5827303905611, parameters k is 41.51186054405308 and b is -22.77132954343556\n",
      "Iteration 96, the loss is 215.17776409673448, parameters k is 41.44901420017956 and b is -22.78132954343556\n",
      "Iteration 97, the loss is 214.77279780290763, parameters k is 41.386167856306045 and b is -22.791329543435562\n",
      "Iteration 98, the loss is 214.36783150908064, parameters k is 41.32332151243253 and b is -22.801329543435564\n",
      "Iteration 99, the loss is 213.96286521525394, parameters k is 41.26047516855901 and b is -22.811329543435566\n",
      "Iteration 100, the loss is 213.55789892142727, parameters k is 41.197628824685495 and b is -22.821329543435567\n",
      "Iteration 101, the loss is 213.15293262760025, parameters k is 41.13478248081198 and b is -22.83132954343557\n",
      "Iteration 102, the loss is 212.74796633377343, parameters k is 41.07193613693846 and b is -22.84132954343557\n",
      "Iteration 103, the loss is 212.34300003994665, parameters k is 41.009089793064945 and b is -22.851329543435572\n",
      "Iteration 104, the loss is 211.93803374611957, parameters k is 40.94624344919143 and b is -22.861329543435573\n",
      "Iteration 105, the loss is 211.53306745229276, parameters k is 40.88339710531791 and b is -22.871329543435575\n",
      "Iteration 106, the loss is 211.1281011584659, parameters k is 40.820550761444395 and b is -22.881329543435577\n",
      "Iteration 107, the loss is 210.723134864639, parameters k is 40.75770441757088 and b is -22.891329543435578\n",
      "Iteration 108, the loss is 210.31816857081222, parameters k is 40.69485807369736 and b is -22.90132954343558\n",
      "Iteration 109, the loss is 209.9132022769856, parameters k is 40.632011729823844 and b is -22.91132954343558\n",
      "Iteration 110, the loss is 209.5082359831585, parameters k is 40.56916538595033 and b is -22.921329543435583\n",
      "Iteration 111, the loss is 209.1032696893318, parameters k is 40.50631904207681 and b is -22.931329543435584\n",
      "Iteration 112, the loss is 208.69830339550478, parameters k is 40.443472698203294 and b is -22.941329543435586\n",
      "Iteration 113, the loss is 208.29333710167808, parameters k is 40.38062635432978 and b is -22.951329543435588\n",
      "Iteration 114, the loss is 207.88837080785103, parameters k is 40.31778001045626 and b is -22.96132954343559\n",
      "Iteration 115, the loss is 207.48340451402453, parameters k is 40.254933666582744 and b is -22.97132954343559\n",
      "Iteration 116, the loss is 207.07843822019777, parameters k is 40.19208732270923 and b is -22.981329543435592\n",
      "Iteration 117, the loss is 206.6734719263708, parameters k is 40.12924097883571 and b is -22.991329543435594\n",
      "Iteration 118, the loss is 206.26850563254405, parameters k is 40.066394634962194 and b is -23.001329543435595\n",
      "Iteration 119, the loss is 205.86353933871715, parameters k is 40.00354829108868 and b is -23.011329543435597\n",
      "Iteration 120, the loss is 205.45857304489041, parameters k is 39.94070194721516 and b is -23.0213295434356\n",
      "Iteration 121, the loss is 205.05360675106348, parameters k is 39.877855603341644 and b is -23.0313295434356\n",
      "Iteration 122, the loss is 204.64864045723655, parameters k is 39.81500925946813 and b is -23.0413295434356\n",
      "Iteration 123, the loss is 204.2436741634098, parameters k is 39.75216291559461 and b is -23.051329543435603\n",
      "Iteration 124, the loss is 203.83870786958298, parameters k is 39.68931657172109 and b is -23.061329543435605\n",
      "Iteration 125, the loss is 203.4337415757561, parameters k is 39.62647022784758 and b is -23.071329543435606\n",
      "Iteration 126, the loss is 203.0287752819292, parameters k is 39.56362388397406 and b is -23.081329543435608\n",
      "Iteration 127, the loss is 202.62380898810252, parameters k is 39.50077754010054 and b is -23.09132954343561\n",
      "Iteration 128, the loss is 202.2188426942755, parameters k is 39.43793119622703 and b is -23.10132954343561\n",
      "Iteration 129, the loss is 201.81387640044895, parameters k is 39.37508485235351 and b is -23.111329543435613\n",
      "Iteration 130, the loss is 201.40891010662173, parameters k is 39.31223850847999 and b is -23.121329543435614\n",
      "Iteration 131, the loss is 201.00394381279506, parameters k is 39.249392164606476 and b is -23.131329543435616\n",
      "Iteration 132, the loss is 200.59897751896824, parameters k is 39.18654582073296 and b is -23.141329543435617\n",
      "Iteration 133, the loss is 200.19401122514137, parameters k is 39.12369947685944 and b is -23.15132954343562\n",
      "Iteration 134, the loss is 199.78904493131438, parameters k is 39.060853132985926 and b is -23.16132954343562\n",
      "Iteration 135, the loss is 199.3840786374877, parameters k is 38.99800678911241 and b is -23.171329543435622\n",
      "Iteration 136, the loss is 198.97911234366077, parameters k is 38.93516044523889 and b is -23.181329543435623\n",
      "Iteration 137, the loss is 198.57414604983384, parameters k is 38.872314101365376 and b is -23.191329543435625\n",
      "Iteration 138, the loss is 198.169179756007, parameters k is 38.80946775749186 and b is -23.201329543435627\n",
      "Iteration 139, the loss is 197.76421346218038, parameters k is 38.74662141361834 and b is -23.211329543435628\n",
      "Iteration 140, the loss is 197.35924716835328, parameters k is 38.683775069744826 and b is -23.22132954343563\n",
      "Iteration 141, the loss is 196.95428087452646, parameters k is 38.62092872587131 and b is -23.23132954343563\n",
      "Iteration 142, the loss is 196.54931458069981, parameters k is 38.55808238199779 and b is -23.241329543435633\n",
      "Iteration 143, the loss is 196.14434828687286, parameters k is 38.495236038124276 and b is -23.251329543435634\n",
      "Iteration 144, the loss is 195.7393819930461, parameters k is 38.43238969425076 and b is -23.261329543435636\n",
      "Iteration 145, the loss is 195.33441569921933, parameters k is 38.36954335037724 and b is -23.271329543435638\n",
      "Iteration 146, the loss is 194.92944940539243, parameters k is 38.306697006503725 and b is -23.28132954343564\n",
      "Iteration 147, the loss is 194.52448311156542, parameters k is 38.24385066263021 and b is -23.29132954343564\n",
      "Iteration 148, the loss is 194.11951681773866, parameters k is 38.18100431875669 and b is -23.301329543435642\n",
      "Iteration 149, the loss is 193.714550523912, parameters k is 38.118157974883175 and b is -23.311329543435644\n",
      "Iteration 150, the loss is 193.3095842300849, parameters k is 38.05531163100966 and b is -23.321329543435645\n",
      "Iteration 151, the loss is 192.90461793625832, parameters k is 37.99246528713614 and b is -23.331329543435647\n",
      "Iteration 152, the loss is 192.49965164243144, parameters k is 37.929618943262625 and b is -23.34132954343565\n",
      "Iteration 153, the loss is 192.09468534860457, parameters k is 37.86677259938911 and b is -23.35132954343565\n",
      "Iteration 154, the loss is 191.68971905477778, parameters k is 37.80392625551559 and b is -23.36132954343565\n",
      "Iteration 155, the loss is 191.28475276095094, parameters k is 37.741079911642075 and b is -23.371329543435653\n",
      "Iteration 156, the loss is 190.87978646712412, parameters k is 37.67823356776856 and b is -23.381329543435655\n",
      "Iteration 157, the loss is 190.4748201732972, parameters k is 37.61538722389504 and b is -23.391329543435656\n",
      "Iteration 158, the loss is 190.06985387947023, parameters k is 37.552540880021525 and b is -23.401329543435658\n",
      "Iteration 159, the loss is 189.66488758564336, parameters k is 37.48969453614801 and b is -23.41132954343566\n",
      "Iteration 160, the loss is 189.2599212918167, parameters k is 37.42684819227449 and b is -23.42132954343566\n",
      "Iteration 161, the loss is 188.85495499799, parameters k is 37.364001848400974 and b is -23.431329543435663\n",
      "Iteration 162, the loss is 188.44998870416302, parameters k is 37.30115550452746 and b is -23.441329543435664\n",
      "Iteration 163, the loss is 188.04502241033626, parameters k is 37.23830916065394 and b is -23.451329543435666\n",
      "Iteration 164, the loss is 187.64005611650916, parameters k is 37.175462816780424 and b is -23.461329543435667\n",
      "Iteration 165, the loss is 187.23508982268254, parameters k is 37.11261647290691 and b is -23.47132954343567\n",
      "Iteration 166, the loss is 186.8301235288559, parameters k is 37.04977012903339 and b is -23.48132954343567\n",
      "Iteration 167, the loss is 186.42515723502873, parameters k is 36.986923785159874 and b is -23.491329543435672\n",
      "Iteration 168, the loss is 186.02019094120212, parameters k is 36.92407744128636 and b is -23.501329543435673\n",
      "Iteration 169, the loss is 185.61522464737496, parameters k is 36.86123109741284 and b is -23.511329543435675\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 170, the loss is 185.21025835354848, parameters k is 36.798384753539324 and b is -23.521329543435677\n",
      "Iteration 171, the loss is 184.80529205972147, parameters k is 36.73553840966581 and b is -23.531329543435678\n",
      "Iteration 172, the loss is 184.40032576589465, parameters k is 36.67269206579229 and b is -23.54132954343568\n",
      "Iteration 173, the loss is 183.99535947206772, parameters k is 36.609845721918774 and b is -23.55132954343568\n",
      "Iteration 174, the loss is 183.59039317824096, parameters k is 36.54699937804526 and b is -23.561329543435683\n",
      "Iteration 175, the loss is 183.18542688441403, parameters k is 36.48415303417174 and b is -23.571329543435684\n",
      "Iteration 176, the loss is 182.7804605905875, parameters k is 36.42130669029822 and b is -23.581329543435686\n",
      "Iteration 177, the loss is 182.3754942967604, parameters k is 36.35846034642471 and b is -23.591329543435688\n",
      "Iteration 178, the loss is 181.97052800293343, parameters k is 36.29561400255119 and b is -23.60132954343569\n",
      "Iteration 179, the loss is 181.56556170910653, parameters k is 36.23276765867767 and b is -23.61132954343569\n",
      "Iteration 180, the loss is 181.16059541527983, parameters k is 36.16992131480416 and b is -23.621329543435692\n",
      "Iteration 181, the loss is 180.75562912145307, parameters k is 36.10707497093064 and b is -23.631329543435694\n",
      "Iteration 182, the loss is 180.35066282762614, parameters k is 36.04422862705712 and b is -23.641329543435695\n",
      "Iteration 183, the loss is 179.9456965337993, parameters k is 35.981382283183606 and b is -23.651329543435697\n",
      "Iteration 184, the loss is 179.54073023997248, parameters k is 35.91853593931009 and b is -23.6613295434357\n",
      "Iteration 185, the loss is 179.13576394614586, parameters k is 35.85568959543657 and b is -23.6713295434357\n",
      "Iteration 186, the loss is 178.73079765231896, parameters k is 35.792843251563056 and b is -23.6813295434357\n",
      "Iteration 187, the loss is 178.32583135849208, parameters k is 35.72999690768954 and b is -23.691329543435703\n",
      "Iteration 188, the loss is 177.92086506466535, parameters k is 35.66715056381602 and b is -23.701329543435705\n",
      "Iteration 189, the loss is 177.5158987708382, parameters k is 35.604304219942506 and b is -23.711329543435706\n",
      "Iteration 190, the loss is 177.11093247701166, parameters k is 35.54145787606899 and b is -23.721329543435708\n",
      "Iteration 191, the loss is 176.70596618318464, parameters k is 35.47861153219547 and b is -23.73132954343571\n",
      "Iteration 192, the loss is 176.30099988935766, parameters k is 35.415765188321956 and b is -23.74132954343571\n",
      "Iteration 193, the loss is 175.89603359553098, parameters k is 35.35291884444844 and b is -23.751329543435713\n",
      "Iteration 194, the loss is 175.49106730170413, parameters k is 35.29007250057492 and b is -23.761329543435714\n",
      "Iteration 195, the loss is 175.0861010078773, parameters k is 35.227226156701406 and b is -23.771329543435716\n",
      "Iteration 196, the loss is 174.68113471405047, parameters k is 35.16437981282789 and b is -23.781329543435717\n",
      "Iteration 197, the loss is 174.27616842022354, parameters k is 35.10153346895437 and b is -23.79132954343572\n",
      "Iteration 198, the loss is 173.8712021263969, parameters k is 35.038687125080855 and b is -23.80132954343572\n",
      "Iteration 199, the loss is 173.4662358325699, parameters k is 34.97584078120734 and b is -23.811329543435722\n",
      "Iteration 200, the loss is 173.06126953874312, parameters k is 34.91299443733382 and b is -23.821329543435724\n",
      "Iteration 201, the loss is 172.65630324491633, parameters k is 34.850148093460305 and b is -23.831329543435725\n",
      "Iteration 202, the loss is 172.25133695108946, parameters k is 34.78730174958679 and b is -23.841329543435727\n",
      "Iteration 203, the loss is 171.84637065726255, parameters k is 34.72445540571327 and b is -23.851329543435728\n",
      "Iteration 204, the loss is 171.44140436343588, parameters k is 34.661609061839755 and b is -23.86132954343573\n",
      "Iteration 205, the loss is 171.03643806960898, parameters k is 34.59876271796624 and b is -23.87132954343573\n",
      "Iteration 206, the loss is 170.63147177578205, parameters k is 34.53591637409272 and b is -23.881329543435733\n",
      "Iteration 207, the loss is 170.22650548195514, parameters k is 34.473070030219205 and b is -23.891329543435734\n",
      "Iteration 208, the loss is 169.82153918812833, parameters k is 34.41022368634569 and b is -23.901329543435736\n",
      "Iteration 209, the loss is 169.4165728943016, parameters k is 34.34737734247217 and b is -23.911329543435738\n",
      "Iteration 210, the loss is 169.01160660047472, parameters k is 34.284530998598655 and b is -23.92132954343574\n",
      "Iteration 211, the loss is 168.60664030664796, parameters k is 34.22168465472514 and b is -23.93132954343574\n",
      "Iteration 212, the loss is 168.20167401282103, parameters k is 34.15883831085162 and b is -23.941329543435742\n",
      "Iteration 213, the loss is 167.79670771899427, parameters k is 34.095991966978104 and b is -23.951329543435744\n",
      "Iteration 214, the loss is 167.39174142516728, parameters k is 34.03314562310459 and b is -23.961329543435745\n",
      "Iteration 215, the loss is 166.98677513134055, parameters k is 33.97029927923107 and b is -23.971329543435747\n",
      "Iteration 216, the loss is 166.58180883751376, parameters k is 33.907452935357554 and b is -23.98132954343575\n",
      "Iteration 217, the loss is 166.17684254368683, parameters k is 33.84460659148404 and b is -23.99132954343575\n",
      "Iteration 218, the loss is 165.77187624986, parameters k is 33.78176024761052 and b is -24.00132954343575\n",
      "Iteration 219, the loss is 165.36690995603314, parameters k is 33.718913903737004 and b is -24.011329543435753\n",
      "Iteration 220, the loss is 164.96194366220638, parameters k is 33.65606755986349 and b is -24.021329543435755\n",
      "Iteration 221, the loss is 164.5569773683797, parameters k is 33.59322121598997 and b is -24.031329543435756\n",
      "Iteration 222, the loss is 164.15201107455277, parameters k is 33.530374872116454 and b is -24.041329543435758\n",
      "Iteration 223, the loss is 163.74704478072567, parameters k is 33.46752852824294 and b is -24.05132954343576\n",
      "Iteration 224, the loss is 163.34207848689886, parameters k is 33.40468218436942 and b is -24.06132954343576\n",
      "Iteration 225, the loss is 162.937112193072, parameters k is 33.341835840495904 and b is -24.071329543435763\n",
      "Iteration 226, the loss is 162.53214589924522, parameters k is 33.27898949662239 and b is -24.081329543435764\n",
      "Iteration 227, the loss is 162.1271796054185, parameters k is 33.21614315274887 and b is -24.091329543435766\n",
      "Iteration 228, the loss is 161.72221331159147, parameters k is 33.15329680887535 and b is -24.101329543435767\n",
      "Iteration 229, the loss is 161.3172470177648, parameters k is 33.09045046500184 and b is -24.11132954343577\n",
      "Iteration 230, the loss is 160.9122807239381, parameters k is 33.02760412112832 and b is -24.12132954343577\n",
      "Iteration 231, the loss is 160.50731443011108, parameters k is 32.9647577772548 and b is -24.131329543435772\n",
      "Iteration 232, the loss is 160.10234813628435, parameters k is 32.90191143338129 and b is -24.141329543435774\n",
      "Iteration 233, the loss is 159.69738184245756, parameters k is 32.83906508950777 and b is -24.151329543435775\n",
      "Iteration 234, the loss is 159.29241554863043, parameters k is 32.77621874563425 and b is -24.161329543435777\n",
      "Iteration 235, the loss is 158.88744925480344, parameters k is 32.713372401760736 and b is -24.171329543435778\n",
      "Iteration 236, the loss is 158.48248296097697, parameters k is 32.65052605788722 and b is -24.18132954343578\n",
      "Iteration 237, the loss is 158.0775166671501, parameters k is 32.5876797140137 and b is -24.19132954343578\n",
      "Iteration 238, the loss is 157.67255037332316, parameters k is 32.524833370140186 and b is -24.201329543435783\n",
      "Iteration 239, the loss is 157.2675840794963, parameters k is 32.46198702626667 and b is -24.211329543435784\n",
      "Iteration 240, the loss is 156.8626177856694, parameters k is 32.39914068239315 and b is -24.221329543435786\n",
      "Iteration 241, the loss is 156.45765149184268, parameters k is 32.336294338519636 and b is -24.231329543435788\n",
      "Iteration 242, the loss is 156.05268519801587, parameters k is 32.27344799464612 and b is -24.24132954343579\n",
      "Iteration 243, the loss is 155.6477189041891, parameters k is 32.2106016507726 and b is -24.25132954343579\n",
      "Iteration 244, the loss is 155.242752610362, parameters k is 32.147755306899086 and b is -24.261329543435792\n",
      "Iteration 245, the loss is 154.83778631653544, parameters k is 32.08490896302557 and b is -24.271329543435794\n",
      "Iteration 246, the loss is 154.43282002270868, parameters k is 32.02206261915205 and b is -24.281329543435795\n",
      "Iteration 247, the loss is 154.02785372888167, parameters k is 31.959216275278536 and b is -24.291329543435797\n",
      "Iteration 248, the loss is 153.62288743505488, parameters k is 31.89636993140502 and b is -24.3013295434358\n",
      "Iteration 249, the loss is 153.2179211412281, parameters k is 31.833523587531502 and b is -24.3113295434358\n",
      "Iteration 250, the loss is 152.81295484740093, parameters k is 31.770677243657985 and b is -24.3213295434358\n",
      "Iteration 251, the loss is 152.40798855357423, parameters k is 31.70783089978447 and b is -24.331329543435803\n",
      "Iteration 252, the loss is 152.00302225974747, parameters k is 31.644984555910952 and b is -24.341329543435805\n",
      "Iteration 253, the loss is 151.5980559659205, parameters k is 31.582138212037435 and b is -24.351329543435806\n",
      "Iteration 254, the loss is 151.1930896720938, parameters k is 31.51929186816392 and b is -24.361329543435808\n",
      "Iteration 255, the loss is 150.78812337826668, parameters k is 31.4564455242904 and b is -24.37132954343581\n",
      "Iteration 256, the loss is 150.38315708444006, parameters k is 31.393599180416885 and b is -24.38132954343581\n",
      "Iteration 257, the loss is 149.9781907906134, parameters k is 31.33075283654337 and b is -24.391329543435813\n",
      "Iteration 258, the loss is 149.5732244967864, parameters k is 31.26790649266985 and b is -24.401329543435814\n",
      "Iteration 259, the loss is 149.16825820295955, parameters k is 31.205060148796335 and b is -24.411329543435816\n",
      "Iteration 260, the loss is 148.76329190913268, parameters k is 31.142213804922818 and b is -24.421329543435817\n",
      "Iteration 261, the loss is 148.35832561530586, parameters k is 31.0793674610493 and b is -24.43132954343582\n",
      "Iteration 262, the loss is 147.95335932147898, parameters k is 31.016521117175785 and b is -24.44132954343582\n",
      "Iteration 263, the loss is 147.54839302765222, parameters k is 30.953674773302268 and b is -24.451329543435822\n",
      "Iteration 264, the loss is 147.14342673382532, parameters k is 30.89082842942875 and b is -24.461329543435824\n",
      "Iteration 265, the loss is 146.73846043999833, parameters k is 30.827982085555234 and b is -24.471329543435825\n",
      "Iteration 266, the loss is 146.3334941461718, parameters k is 30.765135741681718 and b is -24.481329543435827\n",
      "Iteration 267, the loss is 145.92852785234479, parameters k is 30.7022893978082 and b is -24.49132954343583\n",
      "Iteration 268, the loss is 145.52356155851797, parameters k is 30.639443053934684 and b is -24.50132954343583\n",
      "Iteration 269, the loss is 145.11859526469127, parameters k is 30.576596710061168 and b is -24.51132954343583\n",
      "Iteration 270, the loss is 144.71362897086425, parameters k is 30.51375036618765 and b is -24.521329543435833\n",
      "Iteration 271, the loss is 144.30866267703743, parameters k is 30.450904022314134 and b is -24.531329543435834\n",
      "Iteration 272, the loss is 143.90369638321084, parameters k is 30.388057678440617 and b is -24.541329543435836\n",
      "Iteration 273, the loss is 143.49873008938383, parameters k is 30.3252113345671 and b is -24.551329543435838\n",
      "Iteration 274, the loss is 143.09376379555707, parameters k is 30.262364990693584 and b is -24.56132954343584\n",
      "Iteration 275, the loss is 142.68879750173025, parameters k is 30.199518646820067 and b is -24.57132954343584\n",
      "Iteration 276, the loss is 142.2838312079034, parameters k is 30.13667230294655 and b is -24.581329543435842\n",
      "Iteration 277, the loss is 141.87886491407653, parameters k is 30.073825959073034 and b is -24.591329543435844\n",
      "Iteration 278, the loss is 141.47389862024963, parameters k is 30.010979615199517 and b is -24.601329543435845\n",
      "Iteration 279, the loss is 141.06893232642278, parameters k is 29.948133271326 and b is -24.611329543435847\n",
      "Iteration 280, the loss is 140.66396603259605, parameters k is 29.885286927452483 and b is -24.62132954343585\n",
      "Iteration 281, the loss is 140.25899973876918, parameters k is 29.822440583578967 and b is -24.63132954343585\n",
      "Iteration 282, the loss is 139.85403344494233, parameters k is 29.75959423970545 and b is -24.64132954343585\n",
      "Iteration 283, the loss is 139.4490671511154, parameters k is 29.696747895831933 and b is -24.651329543435853\n",
      "Iteration 284, the loss is 139.04410085728878, parameters k is 29.633901551958417 and b is -24.661329543435855\n",
      "Iteration 285, the loss is 138.6391345634618, parameters k is 29.5710552080849 and b is -24.671329543435856\n",
      "Iteration 286, the loss is 138.23416826963484, parameters k is 29.508208864211383 and b is -24.681329543435858\n",
      "Iteration 287, the loss is 137.82920197580816, parameters k is 29.445362520337866 and b is -24.69132954343586\n",
      "Iteration 288, the loss is 137.4242356819812, parameters k is 29.38251617646435 and b is -24.70132954343586\n",
      "Iteration 289, the loss is 137.01926938815433, parameters k is 29.319669832590833 and b is -24.711329543435863\n",
      "Iteration 290, the loss is 136.61430309432762, parameters k is 29.256823488717316 and b is -24.721329543435864\n",
      "Iteration 291, the loss is 136.20933680050064, parameters k is 29.1939771448438 and b is -24.731329543435866\n",
      "Iteration 292, the loss is 135.8043705066738, parameters k is 29.131130800970283 and b is -24.741329543435867\n",
      "Iteration 293, the loss is 135.39940421284692, parameters k is 29.068284457096766 and b is -24.75132954343587\n",
      "Iteration 294, the loss is 134.9944379190203, parameters k is 29.00543811322325 and b is -24.76132954343587\n",
      "Iteration 295, the loss is 134.58947162519328, parameters k is 28.942591769349733 and b is -24.771329543435872\n",
      "Iteration 296, the loss is 134.1845053313665, parameters k is 28.879745425476216 and b is -24.781329543435874\n",
      "Iteration 297, the loss is 133.7795390375397, parameters k is 28.8168990816027 and b is -24.791329543435875\n",
      "Iteration 298, the loss is 133.37457274371272, parameters k is 28.754052737729182 and b is -24.801329543435877\n",
      "Iteration 299, the loss is 132.96960644988593, parameters k is 28.691206393855666 and b is -24.81132954343588\n",
      "Iteration 300, the loss is 132.56464015605908, parameters k is 28.62836004998215 and b is -24.82132954343588\n",
      "Iteration 301, the loss is 132.15967386223227, parameters k is 28.565513706108632 and b is -24.83132954343588\n",
      "Iteration 302, the loss is 131.75470756840542, parameters k is 28.502667362235115 and b is -24.841329543435883\n",
      "Iteration 303, the loss is 131.3497412745787, parameters k is 28.4398210183616 and b is -24.851329543435885\n",
      "Iteration 304, the loss is 130.9447749807518, parameters k is 28.376974674488082 and b is -24.861329543435886\n",
      "Iteration 305, the loss is 130.53980868692486, parameters k is 28.314128330614565 and b is -24.871329543435888\n",
      "Iteration 306, the loss is 130.13484239309813, parameters k is 28.25128198674105 and b is -24.88132954343589\n",
      "Iteration 307, the loss is 129.7298760992714, parameters k is 28.188435642867532 and b is -24.89132954343589\n",
      "Iteration 308, the loss is 129.3249098054444, parameters k is 28.125589298994015 and b is -24.901329543435892\n",
      "Iteration 309, the loss is 128.91994351161753, parameters k is 28.0627429551205 and b is -24.911329543435894\n",
      "Iteration 310, the loss is 128.51497721779066, parameters k is 27.99989661124698 and b is -24.921329543435895\n",
      "Iteration 311, the loss is 128.11001092396384, parameters k is 27.937050267373465 and b is -24.931329543435897\n",
      "Iteration 312, the loss is 127.7050446301372, parameters k is 27.874203923499948 and b is -24.9413295434359\n",
      "Iteration 313, the loss is 127.30007833631029, parameters k is 27.81135757962643 and b is -24.9513295434359\n",
      "Iteration 314, the loss is 126.8951120424833, parameters k is 27.748511235752915 and b is -24.9613295434359\n",
      "Iteration 315, the loss is 126.49014574865645, parameters k is 27.685664891879398 and b is -24.971329543435903\n",
      "Iteration 316, the loss is 126.08517945482967, parameters k is 27.62281854800588 and b is -24.981329543435905\n",
      "Iteration 317, the loss is 125.68021316100288, parameters k is 27.559972204132364 and b is -24.991329543435906\n",
      "Iteration 318, the loss is 125.27524686717602, parameters k is 27.497125860258848 and b is -25.001329543435908\n",
      "Iteration 319, the loss is 124.87028057334925, parameters k is 27.43427951638533 and b is -25.01132954343591\n",
      "Iteration 320, the loss is 124.46531427952226, parameters k is 27.371433172511814 and b is -25.02132954343591\n",
      "Iteration 321, the loss is 124.06034798569549, parameters k is 27.308586828638298 and b is -25.031329543435913\n",
      "Iteration 322, the loss is 123.65538169186877, parameters k is 27.24574048476478 and b is -25.041329543435914\n",
      "Iteration 323, the loss is 123.25041539804172, parameters k is 27.182894140891264 and b is -25.051329543435916\n",
      "Iteration 324, the loss is 122.84544910421504, parameters k is 27.120047797017747 and b is -25.061329543435917\n",
      "Iteration 325, the loss is 122.44048281038813, parameters k is 27.05720145314423 and b is -25.07132954343592\n",
      "Iteration 326, the loss is 122.0355165165613, parameters k is 26.994355109270714 and b is -25.08132954343592\n",
      "Iteration 327, the loss is 121.63055022273448, parameters k is 26.931508765397197 and b is -25.091329543435922\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 328, the loss is 121.22558392890765, parameters k is 26.86866242152368 and b is -25.101329543435924\n",
      "Iteration 329, the loss is 120.82061763508084, parameters k is 26.805816077650164 and b is -25.111329543435925\n",
      "Iteration 330, the loss is 120.41565134125393, parameters k is 26.742969733776647 and b is -25.121329543435927\n",
      "Iteration 331, the loss is 120.01068504742713, parameters k is 26.68012338990313 and b is -25.13132954343593\n",
      "Iteration 332, the loss is 119.60571875360027, parameters k is 26.617277046029614 and b is -25.14132954343593\n",
      "Iteration 333, the loss is 119.20075245977353, parameters k is 26.554430702156097 and b is -25.15132954343593\n",
      "Iteration 334, the loss is 118.79578616594647, parameters k is 26.49158435828258 and b is -25.161329543435933\n",
      "Iteration 335, the loss is 118.39081987211965, parameters k is 26.428738014409063 and b is -25.171329543435935\n",
      "Iteration 336, the loss is 117.98585357829299, parameters k is 26.365891670535547 and b is -25.181329543435936\n",
      "Iteration 337, the loss is 117.58088728446624, parameters k is 26.30304532666203 and b is -25.191329543435938\n",
      "Iteration 338, the loss is 117.1759209906392, parameters k is 26.240198982788513 and b is -25.20132954343594\n",
      "Iteration 339, the loss is 116.77095469681235, parameters k is 26.177352638914996 and b is -25.21132954343594\n",
      "Iteration 340, the loss is 116.36598840298555, parameters k is 26.11450629504148 and b is -25.221329543435942\n",
      "Iteration 341, the loss is 115.96102210915876, parameters k is 26.051659951167963 and b is -25.231329543435944\n",
      "Iteration 342, the loss is 115.5560558153318, parameters k is 25.988813607294446 and b is -25.241329543435945\n",
      "Iteration 343, the loss is 115.15108952150511, parameters k is 25.92596726342093 and b is -25.251329543435947\n",
      "Iteration 344, the loss is 114.7461232276782, parameters k is 25.863120919547413 and b is -25.26132954343595\n",
      "Iteration 345, the loss is 114.34115693385145, parameters k is 25.800274575673896 and b is -25.27132954343595\n",
      "Iteration 346, the loss is 113.93619064002449, parameters k is 25.73742823180038 and b is -25.28132954343595\n",
      "Iteration 347, the loss is 113.53122434619756, parameters k is 25.674581887926863 and b is -25.291329543435953\n",
      "Iteration 348, the loss is 113.12625805237084, parameters k is 25.611735544053346 and b is -25.301329543435955\n",
      "Iteration 349, the loss is 112.72129175854403, parameters k is 25.54888920017983 and b is -25.311329543435956\n",
      "Iteration 350, the loss is 112.31632546471725, parameters k is 25.486042856306312 and b is -25.321329543435958\n",
      "Iteration 351, the loss is 111.9113591708903, parameters k is 25.423196512432796 and b is -25.33132954343596\n",
      "Iteration 352, the loss is 111.50639287706348, parameters k is 25.36035016855928 and b is -25.34132954343596\n",
      "Iteration 353, the loss is 111.10142658323666, parameters k is 25.297503824685762 and b is -25.351329543435963\n",
      "Iteration 354, the loss is 110.69646028940987, parameters k is 25.234657480812245 and b is -25.361329543435964\n",
      "Iteration 355, the loss is 110.29149399558295, parameters k is 25.17181113693873 and b is -25.371329543435966\n",
      "Iteration 356, the loss is 109.88652770175614, parameters k is 25.108964793065212 and b is -25.381329543435967\n",
      "Iteration 357, the loss is 109.48156140792926, parameters k is 25.046118449191695 and b is -25.39132954343597\n",
      "Iteration 358, the loss is 109.07659511410249, parameters k is 24.98327210531818 and b is -25.40132954343597\n",
      "Iteration 359, the loss is 108.67162882027564, parameters k is 24.920425761444662 and b is -25.411329543435972\n",
      "Iteration 360, the loss is 108.26666252644884, parameters k is 24.857579417571145 and b is -25.421329543435974\n",
      "Iteration 361, the loss is 107.86169623262191, parameters k is 24.79473307369763 and b is -25.431329543435975\n",
      "Iteration 362, the loss is 107.45672993879512, parameters k is 24.73188672982411 and b is -25.441329543435977\n",
      "Iteration 363, the loss is 107.05176364496825, parameters k is 24.669040385950595 and b is -25.45132954343598\n",
      "Iteration 364, the loss is 106.64679735114152, parameters k is 24.606194042077078 and b is -25.46132954343598\n",
      "Iteration 365, the loss is 106.24183105731461, parameters k is 24.54334769820356 and b is -25.47132954343598\n",
      "Iteration 366, the loss is 105.83686476348768, parameters k is 24.480501354330045 and b is -25.481329543435983\n",
      "Iteration 367, the loss is 105.43189846966088, parameters k is 24.417655010456528 and b is -25.491329543435985\n",
      "Iteration 368, the loss is 105.02693217583402, parameters k is 24.35480866658301 and b is -25.501329543435986\n",
      "Iteration 369, the loss is 104.62196588200725, parameters k is 24.291962322709495 and b is -25.511329543435988\n",
      "Iteration 370, the loss is 104.2169995881802, parameters k is 24.229115978835978 and b is -25.52132954343599\n",
      "Iteration 371, the loss is 103.81203329435358, parameters k is 24.16626963496246 and b is -25.53132954343599\n",
      "Iteration 372, the loss is 103.40706700052674, parameters k is 24.103423291088944 and b is -25.541329543435992\n",
      "Iteration 373, the loss is 103.00210070669979, parameters k is 24.040576947215428 and b is -25.551329543435994\n",
      "Iteration 374, the loss is 102.59713441287295, parameters k is 23.97773060334191 and b is -25.561329543435996\n",
      "Iteration 375, the loss is 102.19216811904614, parameters k is 23.914884259468394 and b is -25.571329543435997\n",
      "Iteration 376, the loss is 101.78720182521923, parameters k is 23.852037915594877 and b is -25.581329543436\n",
      "Iteration 377, the loss is 101.38223553139245, parameters k is 23.78919157172136 and b is -25.591329543436\n",
      "Iteration 378, the loss is 100.97726923756585, parameters k is 23.726345227847844 and b is -25.601329543436\n",
      "Iteration 379, the loss is 100.57230294373882, parameters k is 23.663498883974327 and b is -25.611329543436003\n",
      "Iteration 380, the loss is 100.16733664991193, parameters k is 23.60065254010081 and b is -25.621329543436005\n",
      "Iteration 381, the loss is 99.76237035608521, parameters k is 23.537806196227294 and b is -25.631329543436006\n",
      "Iteration 382, the loss is 99.35740406225834, parameters k is 23.474959852353777 and b is -25.641329543436008\n",
      "Iteration 383, the loss is 98.95243776843147, parameters k is 23.41211350848026 and b is -25.65132954343601\n",
      "Iteration 384, the loss is 98.54747147460465, parameters k is 23.349267164606744 and b is -25.66132954343601\n",
      "Iteration 385, the loss is 98.14250518077783, parameters k is 23.286420820733227 and b is -25.671329543436013\n",
      "Iteration 386, the loss is 97.73753888695096, parameters k is 23.22357447685971 and b is -25.681329543436014\n",
      "Iteration 387, the loss is 97.33257259312408, parameters k is 23.160728132986193 and b is -25.691329543436016\n",
      "Iteration 388, the loss is 96.92760629929725, parameters k is 23.097881789112677 and b is -25.701329543436017\n",
      "Iteration 389, the loss is 96.52264000547049, parameters k is 23.03503544523916 and b is -25.71132954343602\n",
      "Iteration 390, the loss is 96.11767371164362, parameters k is 22.972189101365643 and b is -25.72132954343602\n",
      "Iteration 391, the loss is 95.71270741781692, parameters k is 22.909342757492126 and b is -25.731329543436022\n",
      "Iteration 392, the loss is 95.30774112398986, parameters k is 22.84649641361861 and b is -25.741329543436024\n",
      "Iteration 393, the loss is 94.90277483016304, parameters k is 22.783650069745093 and b is -25.751329543436025\n",
      "Iteration 394, the loss is 94.49780853633625, parameters k is 22.720803725871576 and b is -25.761329543436027\n",
      "Iteration 395, the loss is 94.0928422425094, parameters k is 22.65795738199806 and b is -25.77132954343603\n",
      "Iteration 396, the loss is 93.68787594868246, parameters k is 22.595111038124543 and b is -25.78132954343603\n",
      "Iteration 397, the loss is 93.28290965485573, parameters k is 22.532264694251026 and b is -25.79132954343603\n",
      "Iteration 398, the loss is 92.87794336102891, parameters k is 22.46941835037751 and b is -25.801329543436033\n",
      "Iteration 399, the loss is 92.47297706720202, parameters k is 22.406572006503993 and b is -25.811329543436035\n",
      "Iteration 400, the loss is 92.06801077337524, parameters k is 22.343725662630476 and b is -25.821329543436036\n",
      "Iteration 401, the loss is 91.66304447954835, parameters k is 22.28087931875696 and b is -25.831329543436038\n",
      "Iteration 402, the loss is 91.2580781857216, parameters k is 22.218032974883442 and b is -25.84132954343604\n",
      "Iteration 403, the loss is 90.85311189189477, parameters k is 22.155186631009926 and b is -25.85132954343604\n",
      "Iteration 404, the loss is 90.44814559806782, parameters k is 22.09234028713641 and b is -25.861329543436042\n",
      "Iteration 405, the loss is 90.04317930424094, parameters k is 22.029493943262892 and b is -25.871329543436044\n",
      "Iteration 406, the loss is 89.63821301041419, parameters k is 21.966647599389376 and b is -25.881329543436046\n",
      "Iteration 407, the loss is 89.23324671658719, parameters k is 21.90380125551586 and b is -25.891329543436047\n",
      "Iteration 408, the loss is 88.82828042276049, parameters k is 21.840954911642342 and b is -25.90132954343605\n",
      "Iteration 409, the loss is 88.42331412893361, parameters k is 21.778108567768825 and b is -25.91132954343605\n",
      "Iteration 410, the loss is 88.01834783510687, parameters k is 21.71526222389531 and b is -25.921329543436052\n",
      "Iteration 411, the loss is 87.6133815412799, parameters k is 21.652415880021792 and b is -25.931329543436053\n",
      "Iteration 412, the loss is 87.20841524745313, parameters k is 21.589569536148275 and b is -25.941329543436055\n",
      "Iteration 413, the loss is 86.80344895362624, parameters k is 21.52672319227476 and b is -25.951329543436056\n",
      "Iteration 414, the loss is 86.39848265979944, parameters k is 21.46387684840124 and b is -25.961329543436058\n",
      "Iteration 415, the loss is 85.99351636597258, parameters k is 21.401030504527725 and b is -25.97132954343606\n",
      "Iteration 416, the loss is 85.5885500721457, parameters k is 21.338184160654208 and b is -25.98132954343606\n",
      "Iteration 417, the loss is 85.18358377831899, parameters k is 21.27533781678069 and b is -25.991329543436063\n",
      "Iteration 418, the loss is 84.77861748449199, parameters k is 21.212491472907175 and b is -26.001329543436064\n",
      "Iteration 419, the loss is 84.37365119066521, parameters k is 21.149645129033658 and b is -26.011329543436066\n",
      "Iteration 420, the loss is 83.9686848968383, parameters k is 21.08679878516014 and b is -26.021329543436067\n",
      "Iteration 421, the loss is 83.56371860301158, parameters k is 21.023952441286625 and b is -26.03132954343607\n",
      "Iteration 422, the loss is 83.15875230918468, parameters k is 20.961106097413108 and b is -26.04132954343607\n",
      "Iteration 423, the loss is 82.7537860153579, parameters k is 20.89825975353959 and b is -26.051329543436072\n",
      "Iteration 424, the loss is 82.34881972153103, parameters k is 20.835413409666074 and b is -26.061329543436074\n",
      "Iteration 425, the loss is 81.94385342770417, parameters k is 20.772567065792558 and b is -26.071329543436075\n",
      "Iteration 426, the loss is 81.53888713387728, parameters k is 20.70972072191904 and b is -26.081329543436077\n",
      "Iteration 427, the loss is 81.13392084005054, parameters k is 20.646874378045524 and b is -26.09132954343608\n",
      "Iteration 428, the loss is 80.72895454622365, parameters k is 20.584028034172007 and b is -26.10132954343608\n",
      "Iteration 429, the loss is 80.32398825239682, parameters k is 20.52118169029849 and b is -26.11132954343608\n",
      "Iteration 430, the loss is 79.91902195857003, parameters k is 20.458335346424974 and b is -26.121329543436083\n",
      "Iteration 431, the loss is 79.51405566474304, parameters k is 20.395489002551457 and b is -26.131329543436085\n",
      "Iteration 432, the loss is 79.10908937091638, parameters k is 20.33264265867794 and b is -26.141329543436086\n",
      "Iteration 433, the loss is 78.70412307708949, parameters k is 20.269796314804424 and b is -26.151329543436088\n",
      "Iteration 434, the loss is 78.29915678326267, parameters k is 20.206949970930907 and b is -26.16132954343609\n",
      "Iteration 435, the loss is 77.89419048943586, parameters k is 20.14410362705739 and b is -26.17132954343609\n",
      "Iteration 436, the loss is 77.48922419560901, parameters k is 20.081257283183874 and b is -26.181329543436092\n",
      "Iteration 437, the loss is 77.08425790178218, parameters k is 20.018410939310357 and b is -26.191329543436094\n",
      "Iteration 438, the loss is 76.6792916079553, parameters k is 19.95556459543684 and b is -26.201329543436096\n",
      "Iteration 439, the loss is 76.27432531412853, parameters k is 19.892718251563323 and b is -26.211329543436097\n",
      "Iteration 440, the loss is 75.86935902030162, parameters k is 19.829871907689807 and b is -26.2213295434361\n",
      "Iteration 441, the loss is 75.46439272647481, parameters k is 19.76702556381629 and b is -26.2313295434361\n",
      "Iteration 442, the loss is 75.05942643264801, parameters k is 19.704179219942773 and b is -26.241329543436102\n",
      "Iteration 443, the loss is 74.65446013882104, parameters k is 19.641332876069256 and b is -26.251329543436103\n",
      "Iteration 444, the loss is 74.2494938449942, parameters k is 19.57848653219574 and b is -26.261329543436105\n",
      "Iteration 445, the loss is 73.84452755116739, parameters k is 19.515640188322223 and b is -26.271329543436106\n",
      "Iteration 446, the loss is 73.43956125734054, parameters k is 19.452793844448706 and b is -26.281329543436108\n",
      "Iteration 447, the loss is 73.0345949635137, parameters k is 19.38994750057519 and b is -26.29132954343611\n",
      "Iteration 448, the loss is 72.6296286696869, parameters k is 19.327101156701673 and b is -26.30132954343611\n",
      "Iteration 449, the loss is 72.22466237586, parameters k is 19.264254812828156 and b is -26.311329543436113\n",
      "Iteration 450, the loss is 71.81969608203325, parameters k is 19.20140846895464 and b is -26.321329543436114\n",
      "Iteration 451, the loss is 71.41472978820634, parameters k is 19.138562125081123 and b is -26.331329543436116\n",
      "Iteration 452, the loss is 71.00976349437954, parameters k is 19.075715781207606 and b is -26.341329543436117\n",
      "Iteration 453, the loss is 70.60479720055267, parameters k is 19.01286943733409 and b is -26.35132954343612\n",
      "Iteration 454, the loss is 70.19983090672581, parameters k is 18.950023093460572 and b is -26.36132954343612\n",
      "Iteration 455, the loss is 69.79486461289902, parameters k is 18.887176749587056 and b is -26.371329543436122\n",
      "Iteration 456, the loss is 69.38989831907219, parameters k is 18.82433040571354 and b is -26.381329543436124\n",
      "Iteration 457, the loss is 68.98493202524531, parameters k is 18.761484061840022 and b is -26.391329543436125\n",
      "Iteration 458, the loss is 68.57996573141861, parameters k is 18.698637717966506 and b is -26.401329543436127\n",
      "Iteration 459, the loss is 68.17499943759167, parameters k is 18.63579137409299 and b is -26.41132954343613\n",
      "Iteration 460, the loss is 67.77003314376486, parameters k is 18.572945030219472 and b is -26.42132954343613\n",
      "Iteration 461, the loss is 67.3650668499379, parameters k is 18.510098686345955 and b is -26.43132954343613\n",
      "Iteration 462, the loss is 66.96010055611114, parameters k is 18.44725234247244 and b is -26.441329543436133\n",
      "Iteration 463, the loss is 66.5551342622843, parameters k is 18.384405998598922 and b is -26.451329543436135\n",
      "Iteration 464, the loss is 66.1501679684574, parameters k is 18.321559654725405 and b is -26.461329543436136\n",
      "Iteration 465, the loss is 65.74520167463058, parameters k is 18.25871331085189 and b is -26.471329543436138\n",
      "Iteration 466, the loss is 65.34023538080375, parameters k is 18.19586696697837 and b is -26.48132954343614\n",
      "Iteration 467, the loss is 64.93526908697694, parameters k is 18.133020623104855 and b is -26.49132954343614\n",
      "Iteration 468, the loss is 64.53030279315013, parameters k is 18.07017427923134 and b is -26.501329543436142\n",
      "Iteration 469, the loss is 64.12533649932331, parameters k is 18.00732793535782 and b is -26.511329543436144\n",
      "Iteration 470, the loss is 63.72037020549636, parameters k is 17.944481591484305 and b is -26.521329543436146\n",
      "Iteration 471, the loss is 63.31540391166956, parameters k is 17.881635247610788 and b is -26.531329543436147\n",
      "Iteration 472, the loss is 62.91043761784272, parameters k is 17.81878890373727 and b is -26.54132954343615\n",
      "Iteration 473, the loss is 62.50547132401592, parameters k is 17.755942559863755 and b is -26.55132954343615\n",
      "Iteration 474, the loss is 62.10050503018909, parameters k is 17.693096215990238 and b is -26.561329543436152\n",
      "Iteration 475, the loss is 61.69553873636221, parameters k is 17.63024987211672 and b is -26.571329543436153\n",
      "Iteration 476, the loss is 61.29057244253538, parameters k is 17.567403528243204 and b is -26.581329543436155\n",
      "Iteration 477, the loss is 60.88560614870854, parameters k is 17.504557184369688 and b is -26.591329543436157\n",
      "Iteration 478, the loss is 60.480639854881694, parameters k is 17.44171084049617 and b is -26.601329543436158\n",
      "Iteration 479, the loss is 60.07567356105494, parameters k is 17.378864496622654 and b is -26.61132954343616\n",
      "Iteration 480, the loss is 59.67070726722807, parameters k is 17.316018152749137 and b is -26.62132954343616\n",
      "Iteration 481, the loss is 59.26574097340118, parameters k is 17.25317180887562 and b is -26.631329543436163\n",
      "Iteration 482, the loss is 58.8607746795743, parameters k is 17.190325465002104 and b is -26.641329543436164\n",
      "Iteration 483, the loss is 58.45580838574754, parameters k is 17.127479121128587 and b is -26.651329543436166\n",
      "Iteration 484, the loss is 58.050842091920615, parameters k is 17.06463277725507 and b is -26.661329543436167\n",
      "Iteration 485, the loss is 57.64587579809381, parameters k is 17.001786433381554 and b is -26.67132954343617\n",
      "Iteration 486, the loss is 57.240909504266924, parameters k is 16.938940089508037 and b is -26.68132954343617\n",
      "Iteration 487, the loss is 56.835943210440156, parameters k is 16.87609374563452 and b is -26.691329543436172\n",
      "Iteration 488, the loss is 56.43097691661338, parameters k is 16.813247401761004 and b is -26.701329543436174\n",
      "Iteration 489, the loss is 56.02601062278652, parameters k is 16.750401057887487 and b is -26.711329543436175\n",
      "Iteration 490, the loss is 55.62104432895963, parameters k is 16.68755471401397 and b is -26.721329543436177\n",
      "Iteration 491, the loss is 55.21607803513281, parameters k is 16.624708370140453 and b is -26.73132954343618\n",
      "Iteration 492, the loss is 54.81111174130592, parameters k is 16.561862026266937 and b is -26.74132954343618\n",
      "Iteration 493, the loss is 54.406145447479076, parameters k is 16.49901568239342 and b is -26.75132954343618\n",
      "Iteration 494, the loss is 54.001179153652245, parameters k is 16.436169338519903 and b is -26.761329543436183\n",
      "Iteration 495, the loss is 53.596212859825485, parameters k is 16.373322994646387 and b is -26.771329543436185\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 496, the loss is 53.191246565998604, parameters k is 16.31047665077287 and b is -26.781329543436186\n",
      "Iteration 497, the loss is 52.786280272171744, parameters k is 16.247630306899353 and b is -26.791329543436188\n",
      "Iteration 498, the loss is 52.381313978344906, parameters k is 16.184783963025836 and b is -26.80132954343619\n",
      "Iteration 499, the loss is 51.976347684518025, parameters k is 16.12193761915232 and b is -26.81132954343619\n",
      "Iteration 500, the loss is 51.57138139069119, parameters k is 16.059091275278803 and b is -26.821329543436192\n",
      "Iteration 501, the loss is 51.16641509686444, parameters k is 15.996244931405284 and b is -26.831329543436194\n",
      "Iteration 502, the loss is 50.76144880303758, parameters k is 15.933398587531766 and b is -26.841329543436196\n",
      "Iteration 503, the loss is 50.356482509210686, parameters k is 15.870552243658247 and b is -26.851329543436197\n",
      "Iteration 504, the loss is 49.95151621538381, parameters k is 15.807705899784729 and b is -26.8613295434362\n",
      "Iteration 505, the loss is 49.54654992155698, parameters k is 15.74485955591121 and b is -26.8713295434362\n",
      "Iteration 506, the loss is 49.14158362773014, parameters k is 15.682013212037692 and b is -26.881329543436202\n",
      "Iteration 507, the loss is 48.73661733390326, parameters k is 15.619166868164173 and b is -26.891329543436203\n",
      "Iteration 508, the loss is 48.33165104007639, parameters k is 15.556320524290655 and b is -26.901329543436205\n",
      "Iteration 509, the loss is 47.926684746249535, parameters k is 15.493474180417136 and b is -26.911329543436207\n",
      "Iteration 510, the loss is 47.52263192750498, parameters k is 15.430627836543618 and b is -26.921329543436208\n",
      "Iteration 511, the loss is 47.12020981080167, parameters k is 15.367977935357848 and b is -26.93129001774451\n",
      "Iteration 512, the loss is 46.717787694098384, parameters k is 15.305328034172078 and b is -26.94125049205281\n",
      "Iteration 513, the loss is 46.316045554212444, parameters k is 15.242678132986308 and b is -26.95121096636111\n",
      "Iteration 514, the loss is 45.915463646224154, parameters k is 15.180168982788679 and b is -26.96113191497771\n",
      "Iteration 515, the loss is 45.514881738235864, parameters k is 15.11765983259105 and b is -26.97105286359431\n",
      "Iteration 516, the loss is 45.114299830247546, parameters k is 15.055150682393421 and b is -26.98097381221091\n",
      "Iteration 517, the loss is 44.71371792225929, parameters k is 14.992641532195792 and b is -26.99089476082751\n",
      "Iteration 518, the loss is 44.31313601427098, parameters k is 14.930132381998163 and b is -27.000815709444108\n",
      "Iteration 519, the loss is 43.91255410628272, parameters k is 14.867623231800534 and b is -27.010736658060708\n",
      "Iteration 520, the loss is 43.51197219829444, parameters k is 14.805114081602905 and b is -27.020657606677307\n",
      "Iteration 521, the loss is 43.11139029030608, parameters k is 14.742604931405277 and b is -27.030578555293907\n",
      "Iteration 522, the loss is 42.71080838231778, parameters k is 14.680095781207648 and b is -27.040499503910507\n",
      "Iteration 523, the loss is 42.310226474329504, parameters k is 14.617586631010019 and b is -27.050420452527106\n",
      "Iteration 524, the loss is 41.9096445663412, parameters k is 14.55507748081239 and b is -27.060341401143706\n",
      "Iteration 525, the loss is 41.50906265835289, parameters k is 14.49256833061476 and b is -27.070262349760306\n",
      "Iteration 526, the loss is 41.108480750364635, parameters k is 14.430059180417132 and b is -27.080183298376905\n",
      "Iteration 527, the loss is 40.70789884237634, parameters k is 14.367550030219503 and b is -27.090104246993505\n",
      "Iteration 528, the loss is 40.307316934388055, parameters k is 14.305040880021874 and b is -27.100025195610105\n",
      "Iteration 529, the loss is 39.90673502639974, parameters k is 14.242531729824245 and b is -27.109946144226704\n",
      "Iteration 530, the loss is 39.50615311841146, parameters k is 14.180022579626616 and b is -27.119867092843304\n",
      "Iteration 531, the loss is 39.10557121042318, parameters k is 14.117513429428987 and b is -27.129788041459904\n",
      "Iteration 532, the loss is 38.70498930243492, parameters k is 14.055004279231358 and b is -27.139708990076503\n",
      "Iteration 533, the loss is 38.304407394446585, parameters k is 13.99249512903373 and b is -27.149629938693103\n",
      "Iteration 534, the loss is 37.903825486458345, parameters k is 13.9299859788361 and b is -27.159550887309702\n",
      "Iteration 535, the loss is 37.50324357846994, parameters k is 13.867476828638472 and b is -27.169471835926302\n",
      "Iteration 536, the loss is 37.10266167048171, parameters k is 13.804967678440843 and b is -27.1793927845429\n",
      "Iteration 537, the loss is 36.702079762493426, parameters k is 13.742458528243214 and b is -27.1893137331595\n",
      "Iteration 538, the loss is 36.30149785450513, parameters k is 13.679949378045585 and b is -27.1992346817761\n",
      "Iteration 539, the loss is 35.900915946516825, parameters k is 13.617440227847956 and b is -27.2091556303927\n",
      "Iteration 540, the loss is 35.50033403852854, parameters k is 13.554931077650327 and b is -27.2190765790093\n",
      "Iteration 541, the loss is 35.09975213054022, parameters k is 13.492421927452698 and b is -27.2289975276259\n",
      "Iteration 542, the loss is 34.69917022255192, parameters k is 13.429912777255069 and b is -27.2389184762425\n",
      "Iteration 543, the loss is 34.298588314563666, parameters k is 13.36740362705744 and b is -27.2488394248591\n",
      "Iteration 544, the loss is 33.898006406575384, parameters k is 13.304894476859811 and b is -27.2587603734757\n",
      "Iteration 545, the loss is 33.4974244985871, parameters k is 13.242385326662182 and b is -27.2686813220923\n",
      "Iteration 546, the loss is 33.09684259059879, parameters k is 13.179876176464553 and b is -27.278602270708898\n",
      "Iteration 547, the loss is 32.697146026246905, parameters k is 13.117367026266924 and b is -27.288523219325498\n",
      "Iteration 548, the loss is 32.29954008913767, parameters k is 13.055090089508031 and b is -27.2984046422504\n",
      "Iteration 549, the loss is 31.902792052645527, parameters k is 12.992813152749138 and b is -27.308286065175302\n",
      "Iteration 550, the loss is 31.507163527001875, parameters k is 12.93068890373728 and b is -27.318127962408504\n",
      "Iteration 551, the loss is 31.111535001358263, parameters k is 12.868564654725423 and b is -27.327969859641705\n",
      "Iteration 552, the loss is 30.71590647571466, parameters k is 12.806440405713566 and b is -27.337811756874906\n",
      "Iteration 553, the loss is 30.320277950071066, parameters k is 12.744316156701709 and b is -27.347653654108107\n",
      "Iteration 554, the loss is 29.924649424427443, parameters k is 12.682191907689852 and b is -27.35749555134131\n",
      "Iteration 555, the loss is 29.52902089878382, parameters k is 12.620067658677995 and b is -27.36733744857451\n",
      "Iteration 556, the loss is 29.13339237314019, parameters k is 12.557943409666137 and b is -27.37717934580771\n",
      "Iteration 557, the loss is 28.737763847496577, parameters k is 12.49581916065428 and b is -27.387021243040913\n",
      "Iteration 558, the loss is 28.34256621343206, parameters k is 12.433694911642423 and b is -27.396863140274114\n",
      "Iteration 559, the loss is 27.95006197914368, parameters k is 12.37181635433017 and b is -27.406665511815614\n",
      "Iteration 560, the loss is 27.557557744855345, parameters k is 12.309937797017916 and b is -27.416467883357114\n",
      "Iteration 561, the loss is 27.16505351056702, parameters k is 12.248059239705663 and b is -27.426270254898615\n",
      "Iteration 562, the loss is 26.772549276278646, parameters k is 12.18618068239341 and b is -27.436072626440115\n",
      "Iteration 563, the loss is 26.380045041990336, parameters k is 12.124302125081156 and b is -27.445874997981615\n",
      "Iteration 564, the loss is 25.987540807701947, parameters k is 12.062423567768903 and b is -27.455677369523116\n",
      "Iteration 565, the loss is 25.59503657341363, parameters k is 12.00054501045665 and b is -27.465479741064616\n",
      "Iteration 566, the loss is 25.2025323391253, parameters k is 11.938666453144396 and b is -27.475282112606116\n",
      "Iteration 567, the loss is 24.810028104836938, parameters k is 11.876787895832143 and b is -27.485084484147617\n",
      "Iteration 568, the loss is 24.41752387054861, parameters k is 11.81490933851989 and b is -27.494886855689117\n",
      "Iteration 569, the loss is 24.02501963626027, parameters k is 11.753030781207636 and b is -27.504689227230617\n",
      "Iteration 570, the loss is 23.632515401971908, parameters k is 11.691152223895383 and b is -27.514491598772118\n",
      "Iteration 571, the loss is 23.240011167683587, parameters k is 11.62927366658313 and b is -27.524293970313618\n",
      "Iteration 572, the loss is 22.84841284180076, parameters k is 11.567395109270876 and b is -27.53409634185512\n",
      "Iteration 573, the loss is 22.459248009483375, parameters k is 11.505780702156251 and b is -27.54385918770492\n",
      "Iteration 574, the loss is 22.070083177165987, parameters k is 11.444166295041626 and b is -27.553622033554724\n",
      "Iteration 575, the loss is 21.680918344848656, parameters k is 11.382551887927 and b is -27.563384879404527\n",
      "Iteration 576, the loss is 21.291753512531272, parameters k is 11.320937480812375 and b is -27.57314772525433\n",
      "Iteration 577, the loss is 20.90258868021388, parameters k is 11.25932307369775 and b is -27.582910571104133\n",
      "Iteration 578, the loss is 20.513423847896497, parameters k is 11.197708666583125 and b is -27.592673416953936\n",
      "Iteration 579, the loss is 20.124259015579135, parameters k is 11.1360942594685 and b is -27.60243626280374\n",
      "Iteration 580, the loss is 19.735094183261758, parameters k is 11.074479852353875 and b is -27.61219910865354\n",
      "Iteration 581, the loss is 19.347335271856256, parameters k is 11.01286544523925 and b is -27.621961954503345\n",
      "Iteration 582, the loss is 18.961657055759744, parameters k is 10.95152835037759 and b is -27.631685274661447\n",
      "Iteration 583, the loss is 18.575978839663215, parameters k is 10.89019125551593 and b is -27.64140859481955\n",
      "Iteration 584, the loss is 18.190300623566674, parameters k is 10.82885416065427 and b is -27.65113191497765\n",
      "Iteration 585, the loss is 17.804622407470152, parameters k is 10.76751706579261 and b is -27.660855235135752\n",
      "Iteration 586, the loss is 17.418944191373637, parameters k is 10.70617997093095 and b is -27.670578555293854\n",
      "Iteration 587, the loss is 17.03326597527711, parameters k is 10.64484287606929 and b is -27.680301875451956\n",
      "Iteration 588, the loss is 16.647587759180585, parameters k is 10.58350578120763 and b is -27.69002519561006\n",
      "Iteration 589, the loss is 16.261909543084062, parameters k is 10.522168686345971 and b is -27.69974851576816\n",
      "Iteration 590, the loss is 15.876231326987552, parameters k is 10.460831591484311 and b is -27.709471835926262\n",
      "Iteration 591, the loss is 15.490650743178922, parameters k is 10.399494496622651 and b is -27.719195156084364\n",
      "Iteration 592, the loss is 15.10922637999771, parameters k is 10.338400247610794 and b is -27.72887895055077\n",
      "Iteration 593, the loss is 14.730282575810769, parameters k is 10.277602006504075 and b is -27.738523219325472\n",
      "Iteration 594, the loss is 14.351338771623807, parameters k is 10.216803765397357 and b is -27.748167488100176\n",
      "Iteration 595, the loss is 13.972394967436884, parameters k is 10.156005524290638 and b is -27.75781175687488\n",
      "Iteration 596, the loss is 13.59345116324993, parameters k is 10.09520728318392 and b is -27.767456025649583\n",
      "Iteration 597, the loss is 13.21472362656618, parameters k is 10.0344090420772 and b is -27.777100294424287\n",
      "Iteration 598, the loss is 12.838580618238709, parameters k is 9.973774358282734 and b is -27.78670503750729\n",
      "Iteration 599, the loss is 12.468053487593535, parameters k is 9.913335840496172 and b is -27.796270254898594\n",
      "Iteration 600, the loss is 12.108476232539081, parameters k is 9.854011314804472 and b is -27.8056773695231\n",
      "Iteration 601, the loss is 11.755084397682403, parameters k is 9.795180978836093 and b is -27.815005432764206\n",
      "Iteration 602, the loss is 11.40776203667376, parameters k is 9.736977204132536 and b is -27.824254444621914\n",
      "Iteration 603, the loss is 11.062585068224436, parameters k is 9.67877342942898 and b is -27.833503456479622\n",
      "Iteration 604, the loss is 10.729140999438567, parameters k is 9.621501393855857 and b is -27.84263389126223\n",
      "Iteration 605, the loss is 10.40299500073134, parameters k is 9.56504765867799 and b is -27.85164574896974\n",
      "Iteration 606, the loss is 10.07978363545766, parameters k is 9.508757480812378 and b is -27.86061808098555\n",
      "Iteration 607, the loss is 9.760722507125664, parameters k is 9.452976472907238 and b is -27.86951136161796\n",
      "Iteration 608, the loss is 9.442892583321074, parameters k is 9.397195465002099 and b is -27.87840464225037\n",
      "Iteration 609, the loss is 9.12966668838598, parameters k is 9.341711690298542 and b is -27.887258397191083\n",
      "Iteration 610, the loss is 8.826766096469122, parameters k is 9.287108152749134 and b is -27.895993575056696\n",
      "Iteration 611, the loss is 8.531384367025487, parameters k is 9.233386275278779 and b is -27.90461017584721\n",
      "Iteration 612, the loss is 8.244392163660361, parameters k is 9.180259615199727 and b is -27.913147725254326\n",
      "Iteration 613, the loss is 7.966183749350642, parameters k is 9.127975959073245 and b is -27.921566697586343\n",
      "Iteration 614, the loss is 7.697874659441345, parameters k is 9.076553567768897 and b is -27.92986709284326\n",
      "Iteration 615, the loss is 7.438080057006585, parameters k is 9.026120820733324 and b is -27.938009385333377\n",
      "Iteration 616, the loss is 7.181320739666886, parameters k is 8.975947046029766 and b is -27.946112152131796\n",
      "Iteration 617, the loss is 6.934713354605958, parameters k is 8.92627307369775 and b is -27.954135867546817\n",
      "Iteration 618, the loss is 6.708745813984648, parameters k is 8.879164793065339 and b is -27.96180385173654\n",
      "Iteration 619, the loss is 6.488004189907581, parameters k is 8.832561136938857 and b is -27.969392784542865\n",
      "Iteration 620, the loss is 6.282850474680789, parameters k is 8.787155385950715 and b is -27.976784088890692\n",
      "Iteration 621, the loss is 6.098412989575107, parameters k is 8.744447184369687 and b is -27.983740610629823\n",
      "Iteration 622, the loss is 5.933459503851766, parameters k is 8.703580366187868 and b is -27.990420452527058\n",
      "Iteration 623, the loss is 5.783106576125184, parameters k is 8.665073508480358 and b is -27.996744563198995\n",
      "Iteration 624, the loss is 5.64234386011172, parameters k is 8.627572461049528 and b is -28.002910571104135\n",
      "Iteration 625, the loss is 5.517465808483488, parameters k is 8.592388646820279 and b is -28.008681322092276\n",
      "Iteration 626, the loss is 5.402820584631179, parameters k is 8.558442144843994 and b is -28.01425444462192\n",
      "Iteration 627, the loss is 5.298510288658308, parameters k is 8.526343271326207 and b is -28.019550887309666\n",
      "Iteration 628, the loss is 5.204975835147722, parameters k is 8.495783132986286 and b is -28.024610175847215\n",
      "Iteration 629, the loss is 5.12020295024977, parameters k is 8.46666121599024 and b is -28.029432310234565\n",
      "Iteration 630, the loss is 5.044382889247058, parameters k is 8.43906483259103 and b is -28.03401729047172\n",
      "Iteration 631, the loss is 4.975900201137032, parameters k is 8.412874891879566 and b is -28.038365116558676\n",
      "Iteration 632, the loss is 4.913669393938423, parameters k is 8.388179575674032 and b is -28.042475788495434\n",
      "Iteration 633, the loss is 4.857065300103817, parameters k is 8.364239674488262 and b is -28.046467883357096\n",
      "Iteration 634, the loss is 4.806452908030871, parameters k is 8.341883291089053 and b is -28.05022282406856\n",
      "Iteration 635, the loss is 4.7611275102691355, parameters k is 8.320469832591028 and b is -28.053819662013222\n",
      "Iteration 636, the loss is 4.722506519362565, parameters k is 8.300774674488261 and b is -28.05713982011599\n",
      "Iteration 637, the loss is 4.688567144906421, parameters k is 8.282235959073242 and b is -28.060262349760258\n",
      "Iteration 638, the loss is 4.662310014785447, parameters k is 8.265944002551503 and b is -28.06302914817923\n",
      "Iteration 639, the loss is 4.64112962574509, parameters k is 8.25094287606929 and b is -28.065598318139703\n",
      "Iteration 640, the loss is 4.624241300464123, parameters k is 8.237925583579171 and b is -28.06785128256658\n",
      "Iteration 641, the loss is 4.609986540821013, parameters k is 8.225921769349922 and b is -28.06994614422666\n",
      "Iteration 642, the loss is 4.597718540683892, parameters k is 8.214918982788657 and b is -28.07188290311994\n",
      "Iteration 643, the loss is 4.587148378160273, parameters k is 8.204417500575218 and b is -28.07374061062982\n",
      "Iteration 644, the loss is 4.57834477035582, parameters k is 8.194943093460594 and b is -28.075440215372904\n",
      "Iteration 645, the loss is 4.570977773983896, parameters k is 8.18642263891514 and b is -28.076981717349188\n",
      "Iteration 646, the loss is 4.564661071984959, parameters k is 8.178394279231345 and b is -28.078444167942074\n",
      "Iteration 647, the loss is 4.559210082960226, parameters k is 8.171074180417115 and b is -28.07978804145986\n",
      "Iteration 648, the loss is 4.554602523389383, parameters k is 8.164232184369684 and b is -28.08105286359425\n",
      "Iteration 649, the loss is 4.5505552806614284, parameters k is 8.157929042077194 and b is -28.082238634345238\n",
      "Iteration 650, the loss is 4.546792940557407, parameters k is 8.151851512432925 and b is -28.083384879404527\n",
      "Iteration 651, the loss is 4.543867214249783, parameters k is 8.146262006504072 and b is -28.084452073080417\n",
      "Iteration 652, the loss is 4.54185596508322, parameters k is 8.141862421523834 and b is -28.08532163829781\n",
      "Iteration 653, the loss is 4.539844715916658, parameters k is 8.137462836543596 and b is -28.086191203515202\n",
      "Iteration 654, the loss is 4.537976071432493, parameters k is 8.133063251563359 and b is -28.087060768732595\n",
      "Iteration 655, the loss is 4.536582872452321, parameters k is 8.129341255515929 and b is -28.08781175687489\n",
      "Iteration 656, the loss is 4.53530663196172, parameters k is 8.125840366187866 and b is -28.08852321932548\n",
      "Iteration 657, the loss is 4.534030391471129, parameters k is 8.122339476859803 and b is -28.089234681776073\n",
      "Iteration 658, the loss is 4.532820217211747, parameters k is 8.11883858753174 and b is -28.089946144226666\n",
      "Iteration 659, the loss is 4.53171165005521, parameters k is 8.115577579626603 and b is -28.09061808098556\n",
      "Iteration 660, the loss is 4.5306256001647265, parameters k is 8.112316571721465 and b is -28.091290017744456\n",
      "Iteration 661, the loss is 4.529691489422139, parameters k is 8.109326393855852 and b is -28.09192242881165\n",
      "Iteration 662, the loss is 4.528885082755921, parameters k is 8.10633621599024 and b is -28.092554839878844\n",
      "Iteration 663, the loss is 4.528239471072835, parameters k is 8.103856314804469 and b is -28.09310819956264\n",
      "Iteration 664, the loss is 4.5276484197221585, parameters k is 8.101376413618699 and b is -28.093661559246435\n",
      "Iteration 665, the loss is 4.52726778615564, parameters k is 8.099385247610794 and b is -28.09413586754683\n",
      "Iteration 666, the loss is 4.526990997265968, parameters k is 8.097636057887474 and b is -28.094570650155525\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 667, the loss is 4.526762693166191, parameters k is 8.096177698203679 and b is -28.09496590707252\n",
      "Iteration 668, the loss is 4.52654028906346, parameters k is 8.094719338519884 and b is -28.095361163989516\n",
      "Iteration 669, the loss is 4.5263814970748, parameters k is 8.093510465002097 and b is -28.095716895214814\n",
      "Iteration 670, the loss is 4.526227891657209, parameters k is 8.092301591484311 and b is -28.09607262644011\n",
      "Iteration 671, the loss is 4.526127173085399, parameters k is 8.09134912112858 and b is -28.09638883197371\n",
      "Iteration 672, the loss is 4.5260264545136035, parameters k is 8.090396650772849 and b is -28.096705037507306\n",
      "Iteration 673, the loss is 4.525925735941796, parameters k is 8.089444180417118 and b is -28.097021243040903\n",
      "Iteration 674, the loss is 4.5258250173699945, parameters k is 8.088491710061387 and b is -28.0973374485745\n",
      "Iteration 675, the loss is 4.525724298798193, parameters k is 8.087539239705656 and b is -28.097653654108097\n",
      "Iteration 676, the loss is 4.52562358022639, parameters k is 8.086586769349925 and b is -28.097969859641694\n",
      "Iteration 677, the loss is 4.52552286165458, parameters k is 8.085634298994194 and b is -28.09828606517529\n",
      "Iteration 678, the loss is 4.525422143082779, parameters k is 8.084681828638463 and b is -28.098602270708888\n",
      "Iteration 679, the loss is 4.525334864990651, parameters k is 8.083729358282731 and b is -28.098918476242485\n",
      "Iteration 680, the loss is 4.525274486720368, parameters k is 8.083003251563364 and b is -28.09919515608438\n",
      "Iteration 681, the loss is 4.525230793562377, parameters k is 8.082277144843996 and b is -28.099471835926277\n",
      "Iteration 682, the loss is 4.5252233280024985, parameters k is 8.082088468954668 and b is -28.099669464384775\n",
      "Iteration 683, the loss is 4.525215862442619, parameters k is 8.08189979306534 and b is -28.099867092843272\n",
      "Iteration 684, the loss is 4.525208396882729, parameters k is 8.081711117176011 and b is -28.10006472130177\n",
      "Iteration 685, the loss is 4.52520093132285, parameters k is 8.081522441286683 and b is -28.100262349760268\n",
      "Iteration 686, the loss is 4.525193465762964, parameters k is 8.081333765397355 and b is -28.100459978218765\n",
      "Iteration 687, the loss is 4.52518600020308, parameters k is 8.081145089508027 and b is -28.100657606677263\n",
      "Iteration 688, the loss is 4.525178534643208, parameters k is 8.080956413618699 and b is -28.10085523513576\n",
      "Iteration 689, the loss is 4.525171069083318, parameters k is 8.08076773772937 and b is -28.10105286359426\n",
      "Iteration 690, the loss is 4.525163603523434, parameters k is 8.080579061840043 and b is -28.101250492052756\n",
      "Iteration 691, the loss is 4.5251561379635525, parameters k is 8.080390385950714 and b is -28.101448120511254\n",
      "Iteration 692, the loss is 4.52514867240367, parameters k is 8.080201710061386 and b is -28.10164574896975\n",
      "Iteration 693, the loss is 4.525141820029891, parameters k is 8.080013034172058 and b is -28.10184337742825\n",
      "Iteration 694, the loss is 4.525139571182056, parameters k is 8.080091947215536 and b is -28.102001480195046\n",
      "Iteration 695, the loss is 4.525137061992819, parameters k is 8.079903271326208 and b is -28.102199108653544\n",
      "Iteration 696, the loss is 4.525133939617488, parameters k is 8.079982184369685 and b is -28.10235721142034\n",
      "Iteration 697, the loss is 4.525130817242161, parameters k is 8.080061097413163 and b is -28.102515314187137\n",
      "Iteration 698, the loss is 4.5251276948668275, parameters k is 8.08014001045664 and b is -28.102673416953934\n",
      "Iteration 699, the loss is 4.525125562975195, parameters k is 8.080218923500118 and b is -28.10283151972073\n",
      "Iteration 700, the loss is 4.525122936829755, parameters k is 8.08003024761079 and b is -28.10302914817923\n",
      "Iteration 701, the loss is 4.525119814454422, parameters k is 8.080109160654267 and b is -28.103187250946025\n",
      "Iteration 702, the loss is 4.52511669207909, parameters k is 8.080188073697744 and b is -28.10334535371282\n",
      "Iteration 703, the loss is 4.525113569703766, parameters k is 8.080266986741222 and b is -28.10350345647962\n",
      "Iteration 704, the loss is 4.525111554768338, parameters k is 8.0803458997847 and b is -28.103661559246415\n",
      "Iteration 705, the loss is 4.5251088116666915, parameters k is 8.080157223895371 and b is -28.103859187704913\n",
      "Iteration 706, the loss is 4.525105689291366, parameters k is 8.080236136938849 and b is -28.10401729047171\n",
      "Iteration 707, the loss is 4.525102566916033, parameters k is 8.080315049982326 and b is -28.104175393238506\n",
      "Iteration 708, the loss is 4.5250994445406985, parameters k is 8.080393963025804 and b is -28.104333496005303\n",
      "Iteration 709, the loss is 4.525097546561485, parameters k is 8.080472876069281 and b is -28.1044915987721\n",
      "Iteration 710, the loss is 4.525094686503625, parameters k is 8.080284200179953 and b is -28.104689227230597\n",
      "Iteration 711, the loss is 4.5250915641282985, parameters k is 8.08036311322343 and b is -28.104847329997394\n",
      "Iteration 712, the loss is 4.525088441752969, parameters k is 8.080442026266908 and b is -28.10500543276419\n",
      "Iteration 713, the loss is 4.525085319377634, parameters k is 8.080520939310386 and b is -28.105163535530988\n",
      "Iteration 714, the loss is 4.5250835383546395, parameters k is 8.080599852353863 and b is -28.105321638297784\n",
      "Iteration 715, the loss is 4.525080561340565, parameters k is 8.080411176464535 and b is -28.105519266756282\n",
      "Iteration 716, the loss is 4.525077438965238, parameters k is 8.080490089508013 and b is -28.10567736952308\n",
      "Iteration 717, the loss is 4.5250743165899046, parameters k is 8.08056900255149 and b is -28.105835472289876\n",
      "Iteration 718, the loss is 4.525071194214574, parameters k is 8.080647915594968 and b is -28.105993575056672\n",
      "Iteration 719, the loss is 4.5250695301477775, parameters k is 8.080726828638445 and b is -28.10615167782347\n",
      "Iteration 720, the loss is 4.525066436177506, parameters k is 8.080538152749117 and b is -28.106349306281967\n",
      "Iteration 721, the loss is 4.525063313802168, parameters k is 8.080617065792595 and b is -28.106507409048763\n",
      "Iteration 722, the loss is 4.525060191426843, parameters k is 8.080695978836072 and b is -28.10666551181556\n",
      "Iteration 723, the loss is 4.525057157602671, parameters k is 8.08077489187955 and b is -28.106823614582357\n",
      "Iteration 724, the loss is 4.525055433389771, parameters k is 8.080586215990222 and b is -28.107021243040855\n",
      "Iteration 725, the loss is 4.525052311014442, parameters k is 8.080665129033699 and b is -28.10717934580765\n",
      "Iteration 726, the loss is 4.525049188639109, parameters k is 8.080744042077177 and b is -28.107337448574448\n",
      "Iteration 727, the loss is 4.52504606626378, parameters k is 8.080822955120654 and b is -28.107495551341245\n",
      "Iteration 728, the loss is 4.525043149395814, parameters k is 8.080901868164132 and b is -28.10765365410804\n",
      "Iteration 729, the loss is 4.525041308226707, parameters k is 8.080713192274803 and b is -28.10785128256654\n",
      "Iteration 730, the loss is 4.525038185851377, parameters k is 8.080792105318281 and b is -28.108009385333336\n",
      "Iteration 731, the loss is 4.525035063476049, parameters k is 8.080871018361758 and b is -28.108167488100133\n",
      "Iteration 732, the loss is 4.525031941100711, parameters k is 8.080949931405236 and b is -28.10832559086693\n",
      "Iteration 733, the loss is 4.525029141188963, parameters k is 8.081028844448713 and b is -28.108483693633726\n",
      "Iteration 734, the loss is 4.525027183063642, parameters k is 8.080840168559385 and b is -28.108681322092224\n",
      "Iteration 735, the loss is 4.525024060688315, parameters k is 8.080919081602863 and b is -28.10883942485902\n",
      "Iteration 736, the loss is 4.525020938312985, parameters k is 8.08099799464634 and b is -28.108997527625817\n",
      "Iteration 737, the loss is 4.525017815937656, parameters k is 8.081076907689818 and b is -28.109155630392614\n",
      "Iteration 738, the loss is 4.525015132982105, parameters k is 8.081155820733295 and b is -28.10931373315941\n",
      "Iteration 739, the loss is 4.525013057900584, parameters k is 8.080967144843967 and b is -28.10951136161791\n",
      "Iteration 740, the loss is 4.525009935525255, parameters k is 8.081046057887445 and b is -28.109669464384705\n",
      "Iteration 741, the loss is 4.525006813149924, parameters k is 8.081124970930922 and b is -28.109827567151502\n",
      "Iteration 742, the loss is 4.525003690774587, parameters k is 8.0812038839744 and b is -28.1099856699183\n",
      "Iteration 743, the loss is 4.525001124775256, parameters k is 8.081282797017877 and b is -28.110143772685095\n",
      "Iteration 744, the loss is 4.524998932737518, parameters k is 8.08109412112855 and b is -28.110341401143593\n",
      "Iteration 745, the loss is 4.524995810362191, parameters k is 8.081173034172027 and b is -28.11049950391039\n",
      "Iteration 746, the loss is 4.52499268798686, parameters k is 8.081251947215504 and b is -28.110657606677186\n",
      "Iteration 747, the loss is 4.52498956561153, parameters k is 8.081330860258982 and b is -28.110815709443983\n",
      "Iteration 748, the loss is 4.5249871165684015, parameters k is 8.08140977330246 and b is -28.11097381221078\n",
      "Iteration 749, the loss is 4.524984807574454, parameters k is 8.081221097413131 and b is -28.111171440669278\n",
      "Iteration 750, the loss is 4.524981685199127, parameters k is 8.081300010456609 and b is -28.111329543436074\n",
      "Iteration 751, the loss is 4.524978562823797, parameters k is 8.081378923500086 and b is -28.11148764620287\n",
      "Iteration 752, the loss is 4.524975440448467, parameters k is 8.081457836543564 and b is -28.111645748969668\n",
      "Iteration 753, the loss is 4.524973108361545, parameters k is 8.081536749587041 and b is -28.111803851736465\n",
      "Iteration 754, the loss is 4.524970682411392, parameters k is 8.081348073697713 and b is -28.112001480194962\n",
      "Iteration 755, the loss is 4.5249675600360595, parameters k is 8.08142698674119 and b is -28.11215958296176\n",
      "Iteration 756, the loss is 4.524964437660731, parameters k is 8.081505899784668 and b is -28.112317685728556\n",
      "Iteration 757, the loss is 4.524961315285403, parameters k is 8.081584812828146 and b is -28.112475788495352\n",
      "Iteration 758, the loss is 4.524959100154695, parameters k is 8.081663725871623 and b is -28.11263389126215\n",
      "Iteration 759, the loss is 4.524956557248331, parameters k is 8.081475049982295 and b is -28.112831519720647\n",
      "Iteration 760, the loss is 4.524953434873003, parameters k is 8.081553963025772 and b is -28.112989622487444\n",
      "Iteration 761, the loss is 4.524950312497671, parameters k is 8.08163287606925 and b is -28.11314772525424\n",
      "Iteration 762, the loss is 4.524947190122335, parameters k is 8.081711789112727 and b is -28.113305828021037\n",
      "Iteration 763, the loss is 4.524945091947838, parameters k is 8.081790702156205 and b is -28.113463930787834\n",
      "Iteration 764, the loss is 4.524942432085268, parameters k is 8.081602026266877 and b is -28.11366155924633\n",
      "Iteration 765, the loss is 4.5249393097099375, parameters k is 8.081680939310354 and b is -28.113819662013128\n",
      "Iteration 766, the loss is 4.52493618733461, parameters k is 8.081759852353832 and b is -28.113977764779925\n",
      "Iteration 767, the loss is 4.524933064959277, parameters k is 8.08183876539731 and b is -28.11413586754672\n",
      "Iteration 768, the loss is 4.52493108374099, parameters k is 8.081917678440787 and b is -28.11429397031352\n",
      "Iteration 769, the loss is 4.524928306922209, parameters k is 8.081729002551459 and b is -28.114491598772016\n",
      "Iteration 770, the loss is 4.524925184546879, parameters k is 8.081807915594936 and b is -28.114649701538813\n",
      "Iteration 771, the loss is 4.524922062171545, parameters k is 8.081886828638414 and b is -28.11480780430561\n",
      "Iteration 772, the loss is 4.524918939796216, parameters k is 8.081965741681891 and b is -28.114965907072406\n",
      "Iteration 773, the loss is 4.524917075534132, parameters k is 8.082044654725369 and b is -28.115124009839203\n",
      "Iteration 774, the loss is 4.524914181759145, parameters k is 8.08185597883604 and b is -28.1153216382977\n",
      "Iteration 775, the loss is 4.52491105938381, parameters k is 8.081934891879518 and b is -28.115479741064497\n",
      "Iteration 776, the loss is 4.524907937008481, parameters k is 8.082013804922996 and b is -28.115637843831294\n",
      "Iteration 777, the loss is 4.524904814633151, parameters k is 8.082092717966473 and b is -28.11579594659809\n",
      "Iteration 778, the loss is 4.524903067327278, parameters k is 8.08217163100995 and b is -28.115954049364888\n",
      "Iteration 779, the loss is 4.524900056596079, parameters k is 8.081982955120623 and b is -28.116151677823385\n",
      "Iteration 780, the loss is 4.524896934220746, parameters k is 8.0820618681641 and b is -28.116309780590182\n",
      "Iteration 781, the loss is 4.524893811845417, parameters k is 8.082140781207578 and b is -28.11646788335698\n",
      "Iteration 782, the loss is 4.5248906947821705, parameters k is 8.082219694251055 and b is -28.116625986123776\n",
      "Iteration 783, the loss is 4.524889053808347, parameters k is 8.082031018361727 and b is -28.116823614582273\n",
      "Iteration 784, the loss is 4.5248859314330145, parameters k is 8.082109931405205 and b is -28.11698171734907\n",
      "Iteration 785, the loss is 4.524882809057681, parameters k is 8.082188844448682 and b is -28.117139820115867\n",
      "Iteration 786, the loss is 4.524879686682359, parameters k is 8.08226775749216 and b is -28.117297922882663\n",
      "Iteration 787, the loss is 4.524876686575313, parameters k is 8.082346670535637 and b is -28.11745602564946\n",
      "Iteration 788, the loss is 4.5248749286452865, parameters k is 8.082157994646309 and b is -28.117653654107958\n",
      "Iteration 789, the loss is 4.524871806269951, parameters k is 8.082236907689786 and b is -28.117811756874755\n",
      "Iteration 790, the loss is 4.524868683894627, parameters k is 8.082315820733264 and b is -28.11796985964155\n",
      "Iteration 791, the loss is 4.524865561519297, parameters k is 8.082394733776741 and b is -28.118127962408348\n",
      "Iteration 792, the loss is 4.524862678368458, parameters k is 8.082473646820219 and b is -28.118286065175145\n",
      "Iteration 793, the loss is 4.524860803482222, parameters k is 8.08228497093089 and b is -28.118483693633642\n",
      "Iteration 794, the loss is 4.524857681106887, parameters k is 8.082363883974368 and b is -28.11864179640044\n",
      "Iteration 795, the loss is 4.524854558731568, parameters k is 8.082442797017846 and b is -28.118799899167236\n",
      "Iteration 796, the loss is 4.524851436356229, parameters k is 8.082521710061323 and b is -28.118958001934033\n",
      "Iteration 797, the loss is 4.524848670161605, parameters k is 8.0826006231048 and b is -28.11911610470083\n",
      "Iteration 798, the loss is 4.52484667831916, parameters k is 8.082411947215473 and b is -28.119313733159327\n",
      "Iteration 799, the loss is 4.52484355594383, parameters k is 8.08249086025895 and b is -28.119471835926124\n",
      "Iteration 800, the loss is 4.524840433568499, parameters k is 8.082569773302428 and b is -28.11962993869292\n",
      "Iteration 801, the loss is 4.524837311193167, parameters k is 8.082648686345905 and b is -28.119788041459717\n",
      "Iteration 802, the loss is 4.524834661954753, parameters k is 8.082727599389383 and b is -28.119946144226514\n",
      "Iteration 803, the loss is 4.524832553156093, parameters k is 8.082538923500055 and b is -28.12014377268501\n",
      "Iteration 804, the loss is 4.524829430780759, parameters k is 8.082617836543532 and b is -28.12030187545181\n",
      "Iteration 805, the loss is 4.524826308405433, parameters k is 8.08269674958701 and b is -28.120459978218605\n",
      "Iteration 806, the loss is 4.524823186030104, parameters k is 8.082775662630487 and b is -28.120618080985402\n",
      "Iteration 807, the loss is 4.524820653747903, parameters k is 8.082854575673965 and b is -28.1207761837522\n",
      "Iteration 808, the loss is 4.524818427993035, parameters k is 8.082665899784637 and b is -28.120973812210696\n",
      "Iteration 809, the loss is 4.524815305617698, parameters k is 8.082744812828114 and b is -28.121131914977493\n",
      "Iteration 810, the loss is 4.524812183242374, parameters k is 8.082823725871592 and b is -28.12129001774429\n",
      "Iteration 811, the loss is 4.524809060867044, parameters k is 8.082902638915069 and b is -28.121448120511086\n",
      "Iteration 812, the loss is 4.524806645541046, parameters k is 8.082981551958547 and b is -28.121606223277883\n",
      "Iteration 813, the loss is 4.524804302829972, parameters k is 8.082792876069218 and b is -28.12180385173638\n",
      "Iteration 814, the loss is 4.524801180454637, parameters k is 8.082871789112696 and b is -28.121961954503178\n",
      "Iteration 815, the loss is 4.524798058079311, parameters k is 8.082950702156174 and b is -28.122120057269974\n",
      "Iteration 816, the loss is 4.524794935703985, parameters k is 8.083029615199651 and b is -28.12227816003677\n",
      "Iteration 817, the loss is 4.524792637334191, parameters k is 8.083108528243129 and b is -28.122436262803568\n",
      "Iteration 818, the loss is 4.524790177666909, parameters k is 8.0829198523538 and b is -28.122633891262065\n",
      "Iteration 819, the loss is 4.524787055291576, parameters k is 8.082998765397278 and b is -28.122791994028862\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 820, the loss is 4.524783932916246, parameters k is 8.083077678440755 and b is -28.12295009679566\n",
      "Iteration 821, the loss is 4.524780810540915, parameters k is 8.083156591484233 and b is -28.123108199562456\n",
      "Iteration 822, the loss is 4.524778629127337, parameters k is 8.08323550452771 and b is -28.123266302329252\n",
      "Iteration 823, the loss is 4.524776052503843, parameters k is 8.083046828638382 and b is -28.12346393078775\n",
      "Iteration 824, the loss is 4.5247729301285124, parameters k is 8.08312574168186 and b is -28.123622033554547\n",
      "Iteration 825, the loss is 4.524769807753186, parameters k is 8.083204654725337 and b is -28.123780136321344\n",
      "Iteration 826, the loss is 4.524766685377849, parameters k is 8.083283567768815 and b is -28.12393823908814\n",
      "Iteration 827, the loss is 4.524764620920483, parameters k is 8.083362480812292 and b is -28.124096341854937\n",
      "Iteration 828, the loss is 4.524761927340779, parameters k is 8.083173804922964 and b is -28.124293970313435\n",
      "Iteration 829, the loss is 4.524758804965451, parameters k is 8.083252717966442 and b is -28.12445207308023\n",
      "Iteration 830, the loss is 4.52475568259012, parameters k is 8.08333163100992 and b is -28.124610175847028\n",
      "Iteration 831, the loss is 4.524752560214791, parameters k is 8.083410544053397 and b is -28.124768278613825\n",
      "Iteration 832, the loss is 4.524750612713631, parameters k is 8.083489457096874 and b is -28.12492638138062\n",
      "Iteration 833, the loss is 4.52474780217772, parameters k is 8.083300781207546 and b is -28.12512400983912\n",
      "Iteration 834, the loss is 4.52474467980239, parameters k is 8.083379694251024 and b is -28.125282112605916\n",
      "Iteration 835, the loss is 4.52474155742706, parameters k is 8.083458607294501 and b is -28.125440215372713\n",
      "Iteration 836, the loss is 4.52473843505173, parameters k is 8.083537520337979 and b is -28.12559831813951\n",
      "Iteration 837, the loss is 4.524736604506777, parameters k is 8.083616433381456 and b is -28.125756420906306\n",
      "Iteration 838, the loss is 4.524733677014655, parameters k is 8.083427757492128 and b is -28.125954049364804\n",
      "Iteration 839, the loss is 4.5247305546393255, parameters k is 8.083506670535606 and b is -28.1261121521316\n",
      "Iteration 840, the loss is 4.524727432263991, parameters k is 8.083585583579083 and b is -28.126270254898397\n",
      "Iteration 841, the loss is 4.524724309888667, parameters k is 8.08366449662256 and b is -28.126428357665194\n",
      "Iteration 842, the loss is 4.524722596299929, parameters k is 8.083743409666038 and b is -28.12658646043199\n",
      "Iteration 843, the loss is 4.524719551851596, parameters k is 8.08355473377671 and b is -28.12678408889049\n",
      "Iteration 844, the loss is 4.524716429476262, parameters k is 8.083633646820187 and b is -28.126942191657285\n",
      "Iteration 845, the loss is 4.524713307100932, parameters k is 8.083712559863665 and b is -28.127100294424082\n",
      "Iteration 846, the loss is 4.524710223754812, parameters k is 8.083791472907143 and b is -28.12725839719088\n",
      "Iteration 847, the loss is 4.524709056513791, parameters k is 8.083602797017814 and b is -28.127456025649376\n",
      "Iteration 848, the loss is 4.524706500504099, parameters k is 8.083925346424929 and b is -28.127574602724476\n",
      "Iteration 849, the loss is 4.5247024933302145, parameters k is 8.0837366705356 and b is -28.127772231182973\n",
      "Iteration 850, the loss is 4.524699370954884, parameters k is 8.083815583579078 and b is -28.12793033394977\n",
      "Iteration 851, the loss is 4.524696248579552, parameters k is 8.083894496622555 and b is -28.128088436716567\n",
      "Iteration 852, the loss is 4.524694127958984, parameters k is 8.083973409666033 and b is -28.128246539483364\n",
      "Iteration 853, the loss is 4.5246914905424775, parameters k is 8.083784733776705 and b is -28.12844416794186\n",
      "Iteration 854, the loss is 4.524688368167147, parameters k is 8.083863646820182 and b is -28.128602270708658\n",
      "Iteration 855, the loss is 4.524685245791817, parameters k is 8.08394255986366 and b is -28.128760373475455\n",
      "Iteration 856, the loss is 4.524682123416494, parameters k is 8.084021472907137 and b is -28.12891847624225\n",
      "Iteration 857, the loss is 4.524680119752135, parameters k is 8.084100385950615 and b is -28.12907657900905\n",
      "Iteration 858, the loss is 4.524677533073683, parameters k is 8.083911710061287 and b is -28.129274207467546\n",
      "Iteration 859, the loss is 4.524676396501415, parameters k is 8.084234259468401 and b is -28.129392784542645\n",
      "Iteration 860, the loss is 4.524671309645769, parameters k is 8.084045583579073 and b is -28.129590413001143\n",
      "Iteration 861, the loss is 4.524668187270438, parameters k is 8.08412449662255 and b is -28.12974851576794\n",
      "Iteration 862, the loss is 4.524665659618045, parameters k is 8.084203409666028 and b is -28.129906618534736\n",
      "Iteration 863, the loss is 4.524664367683932, parameters k is 8.0840147337767 and b is -28.130104246993234\n",
      "Iteration 864, the loss is 4.524661936367337, parameters k is 8.084337283183814 and b is -28.130222824068333\n",
      "Iteration 865, the loss is 4.524657373499719, parameters k is 8.084148607294486 and b is -28.13042045252683\n",
      "Iteration 866, the loss is 4.524654251124386, parameters k is 8.084227520337963 and b is -28.130578555293628\n",
      "Iteration 867, the loss is 4.5246511994839675, parameters k is 8.08430643338144 and b is -28.130736658060425\n",
      "Iteration 868, the loss is 4.5246512022941845, parameters k is 8.084117757492113 and b is -28.130934286518922\n",
      "Iteration 869, the loss is 4.524647476233249, parameters k is 8.084440306899227 and b is -28.13105286359402\n",
      "Iteration 870, the loss is 4.524643437353671, parameters k is 8.084251631009899 and b is -28.13125049205252\n",
      "Iteration 871, the loss is 4.524640314978338, parameters k is 8.084330544053376 and b is -28.131408594819316\n",
      "Iteration 872, the loss is 4.524637192603011, parameters k is 8.084409457096854 and b is -28.131566697586113\n",
      "Iteration 873, the loss is 4.524635103688142, parameters k is 8.084488370140331 and b is -28.13172480035291\n",
      "Iteration 874, the loss is 4.524633616832528, parameters k is 8.084299694251003 and b is -28.131922428811407\n",
      "Iteration 875, the loss is 4.524631380437429, parameters k is 8.084622243658117 and b is -28.132041005886506\n",
      "Iteration 876, the loss is 4.524626378832288, parameters k is 8.084433567768789 and b is -28.132238634345004\n",
      "Iteration 877, the loss is 4.524623256456964, parameters k is 8.084512480812267 and b is -28.1323967371118\n",
      "Iteration 878, the loss is 4.524620643554064, parameters k is 8.084591393855744 and b is -28.132554839878598\n",
      "Iteration 879, the loss is 4.524620451442786, parameters k is 8.084402717966416 and b is -28.132752468337095\n",
      "Iteration 880, the loss is 4.52461692030335, parameters k is 8.08472526737353 and b is -28.132871045412195\n",
      "Iteration 881, the loss is 4.524612442686246, parameters k is 8.084536591484202 and b is -28.133068673870692\n",
      "Iteration 882, the loss is 4.524609320310908, parameters k is 8.08461550452768 and b is -28.13322677663749\n",
      "Iteration 883, the loss is 4.52460619793558, parameters k is 8.084694417571157 and b is -28.133384879404286\n",
      "Iteration 884, the loss is 4.524604547758235, parameters k is 8.084773330614635 and b is -28.133542982171083\n",
      "Iteration 885, the loss is 4.524602865981131, parameters k is 8.084584654725306 and b is -28.13374061062958\n",
      "Iteration 886, the loss is 4.5246008245075195, parameters k is 8.08490720413242 and b is -28.13385918770468\n",
      "Iteration 887, the loss is 4.52459538416487, parameters k is 8.084718528243092 and b is -28.134056816163177\n",
      "Iteration 888, the loss is 4.524592261789538, parameters k is 8.08479744128657 and b is -28.134214918929974\n",
      "Iteration 889, the loss is 4.524590087624153, parameters k is 8.084876354330047 and b is -28.13437302169677\n",
      "Iteration 890, the loss is 4.524589700591376, parameters k is 8.08468767844072 and b is -28.13457065015527\n",
      "Iteration 891, the loss is 4.524586364373439, parameters k is 8.085010227847834 and b is -28.134689227230368\n",
      "Iteration 892, the loss is 4.524581633036267, parameters k is 8.084821551958505 and b is -28.134886855688865\n",
      "Iteration 893, the loss is 4.524582641122725, parameters k is 8.08514410136562 and b is -28.135005432763965\n",
      "Iteration 894, the loss is 4.524575392285168, parameters k is 8.084955425476291 and b is -28.135203061222462\n",
      "Iteration 895, the loss is 4.5245735399011, parameters k is 8.085034338519769 and b is -28.13536116398926\n",
      "Iteration 896, the loss is 4.5245728877184295, parameters k is 8.08484566263044 and b is -28.135558792447757\n",
      "Iteration 897, the loss is 4.524569816650381, parameters k is 8.085168212037555 and b is -28.135677369522856\n",
      "Iteration 898, the loss is 4.524564820163312, parameters k is 8.084979536148227 and b is -28.135874997981354\n",
      "Iteration 899, the loss is 4.524566093399671, parameters k is 8.085302085555341 and b is -28.135993575056453\n",
      "Iteration 900, the loss is 4.524558627839785, parameters k is 8.085113409666013 and b is -28.13619120351495\n",
      "Iteration 901, the loss is 4.524560494917375, parameters k is 8.084924733776685 and b is -28.13638883197345\n",
      "Iteration 902, the loss is 4.5245549045890785, parameters k is 8.085247283183799 and b is -28.136507409048548\n",
      "Iteration 903, the loss is 4.524552427362259, parameters k is 8.08505860729447 and b is -28.136705037507046\n",
      "Iteration 904, the loss is 4.524551181338356, parameters k is 8.085381156701585 and b is -28.136823614582145\n",
      "Iteration 905, the loss is 4.524544775651762, parameters k is 8.085192480812257 and b is -28.137021243040643\n",
      "Iteration 906, the loss is 4.524542080116732, parameters k is 8.085271393855734 and b is -28.13717934580744\n",
      "Iteration 907, the loss is 4.524543682044413, parameters k is 8.085082717966406 and b is -28.137376974265937\n",
      "Iteration 908, the loss is 4.524538356866018, parameters k is 8.08540526737352 and b is -28.137495551341036\n",
      "Iteration 909, the loss is 4.524535614489306, parameters k is 8.085216591484192 and b is -28.137693179799534\n",
      "Iteration 910, the loss is 4.524534633615302, parameters k is 8.085539140891306 and b is -28.137811756874633\n",
      "Iteration 911, the loss is 4.524527906147393, parameters k is 8.085350465001978 and b is -28.13800938533313\n",
      "Iteration 912, the loss is 4.524525532393673, parameters k is 8.085429378045456 and b is -28.138167488099928\n",
      "Iteration 913, the loss is 4.524526869171463, parameters k is 8.085240702156128 and b is -28.138365116558425\n",
      "Iteration 914, the loss is 4.524521809142961, parameters k is 8.085563251563242 and b is -28.138483693633525\n",
      "Iteration 915, the loss is 4.524518801616353, parameters k is 8.085374575673914 and b is -28.138681322092022\n",
      "Iteration 916, the loss is 4.524518085892247, parameters k is 8.085697125081028 and b is -28.13879989916712\n",
      "Iteration 917, the loss is 4.524511036643026, parameters k is 8.0855084491917 and b is -28.13899752762562\n",
      "Iteration 918, the loss is 4.524508984670628, parameters k is 8.085587362235177 and b is -28.139155630392416\n",
      "Iteration 919, the loss is 4.524510056298502, parameters k is 8.08539868634585 and b is -28.139353258850914\n",
      "Iteration 920, the loss is 4.524505261419906, parameters k is 8.085721235752963 and b is -28.139471835926013\n",
      "Iteration 921, the loss is 4.524501988743399, parameters k is 8.085532559863635 and b is -28.13966946438451\n",
      "Iteration 922, the loss is 4.524501538169189, parameters k is 8.08585510927075 and b is -28.13978804145961\n",
      "Iteration 923, the loss is 4.524494167138664, parameters k is 8.085666433381421 and b is -28.139985669918108\n",
      "Iteration 924, the loss is 4.524492436947568, parameters k is 8.085745346424899 and b is -28.140143772684905\n",
      "Iteration 925, the loss is 4.524493243425551, parameters k is 8.08555667053557 and b is -28.140341401143402\n",
      "Iteration 926, the loss is 4.524488713696852, parameters k is 8.085879219942685 and b is -28.1404599782185\n",
      "Iteration 927, the loss is 4.52448517587044, parameters k is 8.085690544053357 and b is -28.140657606677\n",
      "Iteration 928, the loss is 4.524484990446136, parameters k is 8.08601309346047 and b is -28.1407761837521\n",
      "Iteration 929, the loss is 4.524477524886251, parameters k is 8.085824417571143 and b is -28.140973812210596\n",
      "Iteration 930, the loss is 4.524480850624497, parameters k is 8.085635741681815 and b is -28.141171440669094\n",
      "Iteration 931, the loss is 4.524473801635545, parameters k is 8.085958291088929 and b is -28.141290017744193\n",
      "Iteration 932, the loss is 4.524472783069393, parameters k is 8.0857696151996 and b is -28.14148764620269\n",
      "Iteration 933, the loss is 4.524470078384829, parameters k is 8.086092164606715 and b is -28.14160622327779\n",
      "Iteration 934, the loss is 4.524464715514277, parameters k is 8.085903488717387 and b is -28.141803851736288\n",
      "Iteration 935, the loss is 4.524466355134112, parameters k is 8.0862260381245 and b is -28.141922428811387\n",
      "Iteration 936, the loss is 4.524458889574228, parameters k is 8.086037362235173 and b is -28.142120057269885\n",
      "Iteration 937, the loss is 4.524460390268342, parameters k is 8.085848686345845 and b is -28.142317685728383\n",
      "Iteration 938, the loss is 4.524455166323516, parameters k is 8.086171235752959 and b is -28.142436262803482\n",
      "Iteration 939, the loss is 4.524452322713231, parameters k is 8.08598255986363 and b is -28.14263389126198\n",
      "Iteration 940, the loss is 4.5244514430728024, parameters k is 8.086305109270745 and b is -28.14275246833708\n",
      "Iteration 941, the loss is 4.524444485028464, parameters k is 8.086116433381417 and b is -28.142950096795577\n",
      "Iteration 942, the loss is 4.524441889923949, parameters k is 8.086171393855725 and b is -28.143108199562374\n",
      "Iteration 943, the loss is 4.524444349984079, parameters k is 8.085982717966397 and b is -28.14330582802087\n",
      "Iteration 944, the loss is 4.524438166673234, parameters k is 8.086305267373511 and b is -28.14342440509597\n",
      "Iteration 945, the loss is 4.524436282428974, parameters k is 8.086116591484183 and b is -28.14362203355447\n",
      "Iteration 946, the loss is 4.524434443422515, parameters k is 8.086439140891297 and b is -28.143740610629568\n",
      "Iteration 947, the loss is 4.5244282148738675, parameters k is 8.086250465001969 and b is -28.143938239088065\n",
      "Iteration 948, the loss is 4.524430720171803, parameters k is 8.086573014409083 and b is -28.144056816163165\n",
      "Iteration 949, the loss is 4.524423254611921, parameters k is 8.086384338519755 and b is -28.144254444621662\n",
      "Iteration 950, the loss is 4.524423889627924, parameters k is 8.086195662630427 and b is -28.14445207308016\n",
      "Iteration 951, the loss is 4.524419531361207, parameters k is 8.086518212037541 and b is -28.14457065015526\n",
      "Iteration 952, the loss is 4.524415822072809, parameters k is 8.086329536148213 and b is -28.144768278613757\n",
      "Iteration 953, the loss is 4.524415808110487, parameters k is 8.086652085555327 and b is -28.144886855688856\n",
      "Iteration 954, the loss is 4.524408832775769, parameters k is 8.086463409665999 and b is -28.145084484147354\n",
      "Iteration 955, the loss is 4.524406254961637, parameters k is 8.086518370140308 and b is -28.14524258691415\n",
      "Iteration 956, the loss is 4.524407849343658, parameters k is 8.08632969425098 and b is -28.14544021537265\n",
      "Iteration 957, the loss is 4.5244025317109156, parameters k is 8.086652243658094 and b is -28.145558792447748\n",
      "Iteration 958, the loss is 4.524399781788548, parameters k is 8.086463567768766 and b is -28.145756420906245\n",
      "Iteration 959, the loss is 4.524398808460208, parameters k is 8.08678611717588 and b is -28.145874997981345\n",
      "Iteration 960, the loss is 4.524392473328585, parameters k is 8.086597441286552 and b is -28.146072626439842\n",
      "Iteration 961, the loss is 4.524389671614721, parameters k is 8.08665240176086 and b is -28.14623072920664\n",
      "Iteration 962, the loss is 4.52438716772238, parameters k is 8.086707362235169 and b is -28.146388831973436\n",
      "Iteration 963, the loss is 4.524388161576198, parameters k is 8.08651868634584 and b is -28.146586460431934\n",
      "Iteration 964, the loss is 4.5243834444716695, parameters k is 8.086841235752955 and b is -28.146705037507033\n",
      "Iteration 965, the loss is 4.5243800940210885, parameters k is 8.086652559863627 and b is -28.14690266596553\n",
      "Iteration 966, the loss is 4.524379721220954, parameters k is 8.08697510927074 and b is -28.14702124304063\n",
      "Iteration 967, the loss is 4.52437331216753, parameters k is 8.086786433381413 and b is -28.147218871499128\n",
      "Iteration 968, the loss is 4.524370510453675, parameters k is 8.086841393855721 and b is -28.147376974265924\n",
      "Iteration 969, the loss is 4.524368080483127, parameters k is 8.08689635433003 and b is -28.14753507703272\n",
      "Iteration 970, the loss is 4.524368473808743, parameters k is 8.086707678440701 and b is -28.14773270549122\n",
      "Iteration 971, the loss is 4.524364357232411, parameters k is 8.087030227847816 and b is -28.147851282566318\n",
      "Iteration 972, the loss is 4.52436040625363, parameters k is 8.086841551958488 and b is -28.148048911024816\n",
      "Iteration 973, the loss is 4.524360633981695, parameters k is 8.087164101365602 and b is -28.148167488099915\n",
      "Iteration 974, the loss is 4.524354151006489, parameters k is 8.086975425476274 and b is -28.148365116558413\n",
      "Iteration 975, the loss is 4.524351349292629, parameters k is 8.087030385950582 and b is -28.14852321932521\n",
      "Iteration 976, the loss is 4.524348993243868, parameters k is 8.08708534642489 and b is -28.148681322092006\n",
      "Iteration 977, the loss is 4.524348786041274, parameters k is 8.086896670535562 and b is -28.148878950550504\n",
      "Iteration 978, the loss is 4.524345269993154, parameters k is 8.087219219942677 and b is -28.148997527625603\n",
      "Iteration 979, the loss is 4.52434072491762, parameters k is 8.087030544053349 and b is -28.1491951560841\n",
      "Iteration 980, the loss is 4.52433792320376, parameters k is 8.087085504527657 and b is -28.149353258850898\n",
      "Iteration 981, the loss is 4.524335121489895, parameters k is 8.087140465001966 and b is -28.149511361617694\n",
      "Iteration 982, the loss is 4.524332319776037, parameters k is 8.087195425476274 and b is -28.14966946438449\n",
      "Iteration 983, the loss is 4.5243295180621805, parameters k is 8.087250385950583 and b is -28.149827567151288\n",
      "Iteration 984, the loss is 4.524327366488411, parameters k is 8.087305346424891 and b is -28.149985669918085\n",
      "Iteration 985, the loss is 4.524326223379306, parameters k is 8.087116670535563 and b is -28.150183298376582\n",
      "Iteration 986, the loss is 4.524323643237699, parameters k is 8.087439219942677 and b is -28.15030187545168\n",
      "Iteration 987, the loss is 4.524318893687164, parameters k is 8.087250544053349 and b is -28.15049950391018\n",
      "Iteration 988, the loss is 4.52431609197331, parameters k is 8.087305504527658 and b is -28.150657606676976\n",
      "Iteration 989, the loss is 4.524313290259444, parameters k is 8.087360465001966 and b is -28.150815709443773\n",
      "Iteration 990, the loss is 4.524310488545583, parameters k is 8.087415425476275 and b is -28.15097381221057\n",
      "Iteration 991, the loss is 4.5243078273219295, parameters k is 8.087470385950583 and b is -28.151131914977366\n",
      "Iteration 992, the loss is 4.524307308200546, parameters k is 8.087281710061255 and b is -28.151329543435864\n",
      "Iteration 993, the loss is 4.524304104071213, parameters k is 8.08760425946837 and b is -28.151448120510963\n",
      "Iteration 994, the loss is 4.524299864170574, parameters k is 8.087415583579041 and b is -28.15164574896946\n",
      "Iteration 995, the loss is 4.524297062456719, parameters k is 8.08747054405335 and b is -28.151803851736258\n",
      "Iteration 996, the loss is 4.524294260742852, parameters k is 8.087525504527658 and b is -28.151961954503054\n",
      "Iteration 997, the loss is 4.524291459028992, parameters k is 8.087580465001967 and b is -28.15212005726985\n",
      "Iteration 998, the loss is 4.524288657315133, parameters k is 8.087635425476275 and b is -28.152278160036648\n",
      "Iteration 999, the loss is 4.524286200566472, parameters k is 8.087690385950584 and b is -28.152436262803445\n",
      "Iteration 1000, the loss is 4.5242847455385755, parameters k is 8.087501710061256 and b is -28.152633891261942\n",
      "Iteration 1001, the loss is 4.52428247731576, parameters k is 8.08782425946837 and b is -28.15275246833704\n",
      "Iteration 1002, the loss is 4.524278032940122, parameters k is 8.087635583579042 and b is -28.15295009679554\n",
      "Iteration 1003, the loss is 4.524275231226258, parameters k is 8.08769054405335 and b is -28.153108199562336\n",
      "Iteration 1004, the loss is 4.524272429512406, parameters k is 8.087745504527659 and b is -28.153266302329133\n",
      "Iteration 1005, the loss is 4.524269627798541, parameters k is 8.087800465001967 and b is -28.15342440509593\n",
      "Iteration 1006, the loss is 4.524266826084681, parameters k is 8.087855425476276 and b is -28.153582507862726\n",
      "Iteration 1007, the loss is 4.524264573811015, parameters k is 8.087910385950584 and b is -28.153740610629523\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1008, the loss is 4.524262182876604, parameters k is 8.087721710061256 and b is -28.15393823908802\n",
      "Iteration 1009, the loss is 4.524260850560303, parameters k is 8.08804425946837 and b is -28.15405681616312\n",
      "Iteration 1010, the loss is 4.524256201709667, parameters k is 8.087855583579042 and b is -28.154254444621618\n",
      "Iteration 1011, the loss is 4.5242533999958106, parameters k is 8.08791054405335 and b is -28.154412547388414\n",
      "Iteration 1012, the loss is 4.524250598281952, parameters k is 8.08796550452766 and b is -28.15457065015521\n",
      "Iteration 1013, the loss is 4.524247796568091, parameters k is 8.088020465001968 and b is -28.154728752922008\n",
      "Iteration 1014, the loss is 4.524245034644531, parameters k is 8.088075425476276 and b is -28.154886855688805\n",
      "Iteration 1015, the loss is 4.524243267697843, parameters k is 8.087886749586948 and b is -28.155084484147302\n",
      "Iteration 1016, the loss is 4.5242413113938165, parameters k is 8.088209298994062 and b is -28.1552030612224\n",
      "Iteration 1017, the loss is 4.5242371721930805, parameters k is 8.088020623104734 and b is -28.1554006896809\n",
      "Iteration 1018, the loss is 4.524234370479217, parameters k is 8.088075583579043 and b is -28.155558792447696\n",
      "Iteration 1019, the loss is 4.524231568765357, parameters k is 8.088130544053351 and b is -28.155716895214493\n",
      "Iteration 1020, the loss is 4.524228767051499, parameters k is 8.08818550452766 and b is -28.15587499798129\n",
      "Iteration 1021, the loss is 4.524225965337635, parameters k is 8.088240465001968 and b is -28.156033100748086\n",
      "Iteration 1022, the loss is 4.524223407889079, parameters k is 8.088295425476277 and b is -28.156191203514883\n",
      "Iteration 1023, the loss is 4.524221076034808, parameters k is 8.088106749586949 and b is -28.15638883197338\n",
      "Iteration 1024, the loss is 4.524218274320947, parameters k is 8.088161710061257 and b is -28.156546934740177\n",
      "Iteration 1025, the loss is 4.524215472607083, parameters k is 8.088216670535566 and b is -28.156705037506974\n",
      "Iteration 1026, the loss is 4.524212670893221, parameters k is 8.088271631009874 and b is -28.15686314027377\n",
      "Iteration 1027, the loss is 4.524209869179365, parameters k is 8.088326591484183 and b is -28.157021243040568\n",
      "Iteration 1028, the loss is 4.524207067465502, parameters k is 8.088381551958491 and b is -28.157179345807364\n",
      "Iteration 1029, the loss is 4.524204265751643, parameters k is 8.0884365124328 and b is -28.15733744857416\n",
      "Iteration 1030, the loss is 4.5242014640377795, parameters k is 8.088491472907108 and b is -28.157495551340958\n",
      "Iteration 1031, the loss is 4.524199241617417, parameters k is 8.088546433381417 and b is -28.157653654107754\n",
      "Iteration 1032, the loss is 4.524196574734946, parameters k is 8.088357757492089 and b is -28.157851282566252\n",
      "Iteration 1033, the loss is 4.524193773021094, parameters k is 8.088412717966397 and b is -28.15800938533305\n",
      "Iteration 1034, the loss is 4.524190971307226, parameters k is 8.088467678440706 and b is -28.158167488099846\n",
      "Iteration 1035, the loss is 4.524188169593367, parameters k is 8.088522638915014 and b is -28.158325590866642\n",
      "Iteration 1036, the loss is 4.524185367879512, parameters k is 8.088577599389323 and b is -28.15848369363344\n",
      "Iteration 1037, the loss is 4.524182566165648, parameters k is 8.088632559863631 and b is -28.158641796400236\n",
      "Iteration 1038, the loss is 4.524179764451788, parameters k is 8.08868752033794 and b is -28.158799899167033\n",
      "Iteration 1039, the loss is 4.524177162934731, parameters k is 8.088742480812249 and b is -28.15895800193383\n",
      "Iteration 1040, the loss is 4.524174875148951, parameters k is 8.08855380492292 and b is -28.159155630392327\n",
      "Iteration 1041, the loss is 4.5241720734350945, parameters k is 8.088608765397229 and b is -28.159313733159124\n",
      "Iteration 1042, the loss is 4.524169271721236, parameters k is 8.088663725871537 and b is -28.15947183592592\n",
      "Iteration 1043, the loss is 4.524166470007372, parameters k is 8.088718686345846 and b is -28.159629938692717\n",
      "Iteration 1044, the loss is 4.524163668293509, parameters k is 8.088773646820155 and b is -28.159788041459514\n",
      "Iteration 1045, the loss is 4.524160866579653, parameters k is 8.088828607294463 and b is -28.15994614422631\n",
      "Iteration 1046, the loss is 4.524158064865785, parameters k is 8.088883567768772 and b is -28.160104246993107\n",
      "Iteration 1047, the loss is 4.524155263151932, parameters k is 8.08893852824308 and b is -28.160262349759904\n",
      "Iteration 1048, the loss is 4.52415299666308, parameters k is 8.088993488717389 and b is -28.1604204525267\n",
      "Iteration 1049, the loss is 4.5241503738491, parameters k is 8.08880481282806 and b is -28.1606180809852\n",
      "Iteration 1050, the loss is 4.524147572135237, parameters k is 8.088859773302369 and b is -28.160776183751995\n",
      "Iteration 1051, the loss is 4.5241447704213815, parameters k is 8.088914733776678 and b is -28.160934286518792\n",
      "Iteration 1052, the loss is 4.524141968707518, parameters k is 8.088969694250986 and b is -28.16109238928559\n",
      "Iteration 1053, the loss is 4.524139166993651, parameters k is 8.089024654725295 and b is -28.161250492052385\n",
      "Iteration 1054, the loss is 4.524136365279795, parameters k is 8.089079615199603 and b is -28.161408594819182\n",
      "Iteration 1055, the loss is 4.524133563565935, parameters k is 8.089134575673912 and b is -28.16156669758598\n",
      "Iteration 1056, the loss is 4.524130917980392, parameters k is 8.08918953614822 and b is -28.161724800352776\n",
      "Iteration 1057, the loss is 4.524128674263105, parameters k is 8.089000860258892 and b is -28.161922428811273\n",
      "Iteration 1058, the loss is 4.524125872549243, parameters k is 8.0890558207332 and b is -28.16208053157807\n",
      "Iteration 1059, the loss is 4.524123070835378, parameters k is 8.08911078120751 and b is -28.162238634344867\n",
      "Iteration 1060, the loss is 4.524120269121524, parameters k is 8.089165741681818 and b is -28.162396737111663\n",
      "Iteration 1061, the loss is 4.524117467407663, parameters k is 8.089220702156126 and b is -28.16255483987846\n",
      "Iteration 1062, the loss is 4.524114665693805, parameters k is 8.089275662630435 and b is -28.162712942645257\n",
      "Iteration 1063, the loss is 4.52411186397994, parameters k is 8.089330623104743 and b is -28.162871045412054\n",
      "Iteration 1064, the loss is 4.524109062266077, parameters k is 8.089385583579052 and b is -28.16302914817885\n",
      "Iteration 1065, the loss is 4.524106751708731, parameters k is 8.08944054405336 and b is -28.163187250945647\n",
      "Iteration 1066, the loss is 4.524104172963247, parameters k is 8.089251868164032 and b is -28.163384879404145\n",
      "Iteration 1067, the loss is 4.524101371249386, parameters k is 8.08930682863834 and b is -28.16354298217094\n",
      "Iteration 1068, the loss is 4.524098569535522, parameters k is 8.08936178911265 and b is -28.16370108493774\n",
      "Iteration 1069, the loss is 4.524095767821662, parameters k is 8.089416749586958 and b is -28.163859187704535\n",
      "Iteration 1070, the loss is 4.524092966107805, parameters k is 8.089471710061266 and b is -28.16401729047133\n",
      "Iteration 1071, the loss is 4.524090164393944, parameters k is 8.089526670535575 and b is -28.16417539323813\n",
      "Iteration 1072, the loss is 4.524087362680081, parameters k is 8.089581631009883 and b is -28.164333496004925\n",
      "Iteration 1073, the loss is 4.524084673026057, parameters k is 8.089636591484192 and b is -28.164491598771722\n",
      "Iteration 1074, the loss is 4.524082473377255, parameters k is 8.089447915594864 and b is -28.16468922723022\n",
      "Iteration 1075, the loss is 4.524079671663387, parameters k is 8.089502876069172 and b is -28.164847329997016\n",
      "Iteration 1076, the loss is 4.524076869949531, parameters k is 8.08955783654348 and b is -28.165005432763813\n",
      "Iteration 1077, the loss is 4.524074068235669, parameters k is 8.08961279701779 and b is -28.16516353553061\n",
      "Iteration 1078, the loss is 4.524071266521813, parameters k is 8.089667757492098 and b is -28.165321638297407\n",
      "Iteration 1079, the loss is 4.524068464807948, parameters k is 8.089722717966406 and b is -28.165479741064203\n",
      "Iteration 1080, the loss is 4.524065663094085, parameters k is 8.089777678440715 and b is -28.165637843831\n",
      "Iteration 1081, the loss is 4.524062861380231, parameters k is 8.089832638915023 and b is -28.165795946597797\n",
      "Iteration 1082, the loss is 4.524060506754393, parameters k is 8.089887599389332 and b is -28.165954049364593\n",
      "Iteration 1083, the loss is 4.524057972077393, parameters k is 8.089698923500004 and b is -28.16615167782309\n",
      "Iteration 1084, the loss is 4.52405517036353, parameters k is 8.089753883974312 and b is -28.166309780589888\n",
      "Iteration 1085, the loss is 4.524052368649672, parameters k is 8.089808844448621 and b is -28.166467883356685\n",
      "Iteration 1086, the loss is 4.524049566935812, parameters k is 8.08986380492293 and b is -28.16662598612348\n",
      "Iteration 1087, the loss is 4.524046765221953, parameters k is 8.089918765397238 and b is -28.166784088890278\n",
      "Iteration 1088, the loss is 4.524043963508094, parameters k is 8.089973725871547 and b is -28.166942191657075\n",
      "Iteration 1089, the loss is 4.524041161794227, parameters k is 8.090028686345855 and b is -28.16710029442387\n",
      "Iteration 1090, the loss is 4.524038428071712, parameters k is 8.090083646820164 and b is -28.16725839719067\n",
      "Iteration 1091, the loss is 4.524036272491398, parameters k is 8.089894970930835 and b is -28.167456025649166\n",
      "Iteration 1092, the loss is 4.524033470777538, parameters k is 8.089949931405144 and b is -28.167614128415963\n",
      "Iteration 1093, the loss is 4.524030669063678, parameters k is 8.090004891879452 and b is -28.16777223118276\n",
      "Iteration 1094, the loss is 4.524027867349821, parameters k is 8.090059852353761 and b is -28.167930333949556\n",
      "Iteration 1095, the loss is 4.524025065635955, parameters k is 8.09011481282807 and b is -28.168088436716353\n",
      "Iteration 1096, the loss is 4.524022263922093, parameters k is 8.090169773302378 and b is -28.16824653948315\n",
      "Iteration 1097, the loss is 4.524019462208234, parameters k is 8.090224733776687 and b is -28.168404642249946\n",
      "Iteration 1098, the loss is 4.5240166604943735, parameters k is 8.090279694250995 and b is -28.168562745016743\n",
      "Iteration 1099, the loss is 4.524014261800056, parameters k is 8.090334654725304 and b is -28.16872084778354\n",
      "Iteration 1100, the loss is 4.524011771191542, parameters k is 8.090145978835976 and b is -28.168918476242037\n",
      "Iteration 1101, the loss is 4.524008969477688, parameters k is 8.090200939310284 and b is -28.169076579008834\n",
      "Iteration 1102, the loss is 4.524006167763819, parameters k is 8.090255899784593 and b is -28.16923468177563\n",
      "Iteration 1103, the loss is 4.52400336604996, parameters k is 8.090310860258901 and b is -28.169392784542428\n",
      "Iteration 1104, the loss is 4.524000564336099, parameters k is 8.09036582073321 and b is -28.169550887309224\n",
      "Iteration 1105, the loss is 4.523997762622239, parameters k is 8.090420781207518 and b is -28.16970899007602\n",
      "Iteration 1106, the loss is 4.523994960908383, parameters k is 8.090475741681827 and b is -28.169867092842818\n",
      "Iteration 1107, the loss is 4.523992183117368, parameters k is 8.090530702156135 and b is -28.170025195609615\n",
      "Iteration 1108, the loss is 4.523990071605542, parameters k is 8.090342026266807 and b is -28.170222824068112\n",
      "Iteration 1109, the loss is 4.523987269891691, parameters k is 8.090396986741116 and b is -28.17038092683491\n",
      "Iteration 1110, the loss is 4.523984468177831, parameters k is 8.090451947215424 and b is -28.170539029601706\n",
      "Iteration 1111, the loss is 4.523981666463965, parameters k is 8.090506907689733 and b is -28.170697132368502\n",
      "Iteration 1112, the loss is 4.523978864750104, parameters k is 8.090561868164041 and b is -28.1708552351353\n",
      "Iteration 1113, the loss is 4.523976063036244, parameters k is 8.09061682863835 and b is -28.171013337902096\n",
      "Iteration 1114, the loss is 4.523973261322383, parameters k is 8.090671789112658 and b is -28.171171440668893\n",
      "Iteration 1115, the loss is 4.523970459608524, parameters k is 8.090726749586967 and b is -28.17132954343569\n",
      "Iteration 1116, the loss is 4.523968016845708, parameters k is 8.090781710061275 and b is -28.171487646202486\n",
      "Iteration 1117, the loss is 4.523965570305694, parameters k is 8.090593034171947 and b is -28.171685274660984\n",
      "Iteration 1118, the loss is 4.523962768591829, parameters k is 8.090647994646256 and b is -28.17184337742778\n",
      "Iteration 1119, the loss is 4.523959966877971, parameters k is 8.090702955120564 and b is -28.172001480194577\n",
      "Iteration 1120, the loss is 4.523957165164109, parameters k is 8.090757915594873 and b is -28.172159582961374\n",
      "Iteration 1121, the loss is 4.523954363450246, parameters k is 8.090812876069181 and b is -28.17231768572817\n",
      "Iteration 1122, the loss is 4.523951561736389, parameters k is 8.09086783654349 and b is -28.172475788494967\n",
      "Iteration 1123, the loss is 4.523948760022532, parameters k is 8.090922797017798 and b is -28.172633891261764\n",
      "Iteration 1124, the loss is 4.52394595830867, parameters k is 8.090977757492107 and b is -28.17279199402856\n",
      "Iteration 1125, the loss is 4.523943850574049, parameters k is 8.091032717966415 and b is -28.172950096795358\n",
      "Iteration 1126, the loss is 4.523941069005838, parameters k is 8.090844042077087 and b is -28.173147725253855\n",
      "Iteration 1127, the loss is 4.523938267291974, parameters k is 8.090899002551396 and b is -28.173305828020652\n",
      "Iteration 1128, the loss is 4.5239354655781145, parameters k is 8.090953963025704 and b is -28.17346393078745\n",
      "Iteration 1129, the loss is 4.523932663864257, parameters k is 8.091008923500013 and b is -28.173622033554246\n",
      "Iteration 1130, the loss is 4.523929862150396, parameters k is 8.091063883974321 and b is -28.173780136321042\n",
      "Iteration 1131, the loss is 4.523927060436531, parameters k is 8.09111884444863 and b is -28.17393823908784\n",
      "Iteration 1132, the loss is 4.5239242587226745, parameters k is 8.091173804922938 and b is -28.174096341854636\n",
      "Iteration 1133, the loss is 4.5239217718913665, parameters k is 8.091228765397247 and b is -28.174254444621432\n",
      "Iteration 1134, the loss is 4.523919369419841, parameters k is 8.091040089507919 and b is -28.17445207307993\n",
      "Iteration 1135, the loss is 4.523916567705978, parameters k is 8.091095049982227 and b is -28.174610175846727\n",
      "Iteration 1136, the loss is 4.523913765992119, parameters k is 8.091150010456536 and b is -28.174768278613524\n",
      "Iteration 1137, the loss is 4.523910964278261, parameters k is 8.091204970930844 and b is -28.17492638138032\n",
      "Iteration 1138, the loss is 4.523908162564402, parameters k is 8.091259931405153 and b is -28.175084484147117\n",
      "Iteration 1139, the loss is 4.523905360850536, parameters k is 8.091314891879462 and b is -28.175242586913914\n",
      "Iteration 1140, the loss is 4.523902559136675, parameters k is 8.09136985235377 and b is -28.17540068968071\n",
      "Iteration 1141, the loss is 4.523899757422821, parameters k is 8.091424812828079 and b is -28.175558792447507\n",
      "Iteration 1142, the loss is 4.523897605619714, parameters k is 8.091479773302387 and b is -28.175716895214304\n",
      "Iteration 1143, the loss is 4.523894868119982, parameters k is 8.091291097413059 and b is -28.1759145236728\n",
      "Iteration 1144, the loss is 4.523892066406124, parameters k is 8.091346057887367 and b is -28.1760726264396\n",
      "Iteration 1145, the loss is 4.52388926469226, parameters k is 8.091401018361676 and b is -28.176230729206395\n",
      "Iteration 1146, the loss is 4.523886462978405, parameters k is 8.091455978835985 and b is -28.176388831973192\n",
      "Iteration 1147, the loss is 4.5238836612645414, parameters k is 8.091510939310293 and b is -28.17654693473999\n",
      "Iteration 1148, the loss is 4.523880859550677, parameters k is 8.091565899784602 and b is -28.176705037506785\n",
      "Iteration 1149, the loss is 4.523878057836822, parameters k is 8.09162086025891 and b is -28.176863140273582\n",
      "Iteration 1150, the loss is 4.523875526937036, parameters k is 8.091675820733219 and b is -28.17702124304038\n",
      "Iteration 1151, the loss is 4.523873168533987, parameters k is 8.09148714484389 and b is -28.177218871498876\n",
      "Iteration 1152, the loss is 4.523870366820128, parameters k is 8.091542105318199 and b is -28.177376974265673\n",
      "Iteration 1153, the loss is 4.52386756510627, parameters k is 8.091597065792508 and b is -28.17753507703247\n",
      "Iteration 1154, the loss is 4.523864763392403, parameters k is 8.091652026266816 and b is -28.177693179799267\n",
      "Iteration 1155, the loss is 4.523861961678547, parameters k is 8.091706986741125 and b is -28.177851282566063\n",
      "Iteration 1156, the loss is 4.523859159964685, parameters k is 8.091761947215433 and b is -28.17800938533286\n",
      "Iteration 1157, the loss is 4.523856358250827, parameters k is 8.091816907689742 and b is -28.178167488099657\n",
      "Iteration 1158, the loss is 4.523853556536964, parameters k is 8.09187186816405 and b is -28.178325590866454\n",
      "Iteration 1159, the loss is 4.523851360665371, parameters k is 8.091926828638359 and b is -28.17848369363325\n",
      "Iteration 1160, the loss is 4.523848667234131, parameters k is 8.09173815274903 and b is -28.178681322091748\n",
      "Iteration 1161, the loss is 4.523845865520271, parameters k is 8.09179311322334 and b is -28.178839424858545\n",
      "Iteration 1162, the loss is 4.523843063806414, parameters k is 8.091848073697648 and b is -28.17899752762534\n",
      "Iteration 1163, the loss is 4.523840262092547, parameters k is 8.091903034171956 and b is -28.179155630392138\n",
      "Iteration 1164, the loss is 4.5238374603786875, parameters k is 8.091957994646265 and b is -28.179313733158935\n",
      "Iteration 1165, the loss is 4.523834658664832, parameters k is 8.092012955120573 and b is -28.17947183592573\n",
      "Iteration 1166, the loss is 4.523831856950969, parameters k is 8.092067915594882 and b is -28.17962993869253\n",
      "Iteration 1167, the loss is 4.5238292819826835, parameters k is 8.09212287606919 and b is -28.179788041459325\n",
      "Iteration 1168, the loss is 4.523826967648135, parameters k is 8.091934200179862 and b is -28.179985669917823\n",
      "Iteration 1169, the loss is 4.523824165934276, parameters k is 8.09198916065417 and b is -28.18014377268462\n",
      "Iteration 1170, the loss is 4.523821364220421, parameters k is 8.09204412112848 and b is -28.180301875451416\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1171, the loss is 4.523818562506557, parameters k is 8.092099081602788 and b is -28.180459978218213\n",
      "Iteration 1172, the loss is 4.523815760792698, parameters k is 8.092154042077096 and b is -28.18061808098501\n",
      "Iteration 1173, the loss is 4.5238129590788345, parameters k is 8.092209002551405 and b is -28.180776183751806\n",
      "Iteration 1174, the loss is 4.523810157364973, parameters k is 8.092263963025713 and b is -28.180934286518603\n",
      "Iteration 1175, the loss is 4.523807355651114, parameters k is 8.092318923500022 and b is -28.1810923892854\n",
      "Iteration 1176, the loss is 4.523805115711029, parameters k is 8.09237388397433 and b is -28.181250492052197\n",
      "Iteration 1177, the loss is 4.5238024663482825, parameters k is 8.092185208085002 and b is -28.181448120510694\n",
      "Iteration 1178, the loss is 4.523799664634415, parameters k is 8.09224016855931 and b is -28.18160622327749\n",
      "Iteration 1179, the loss is 4.523796862920558, parameters k is 8.09229512903362 and b is -28.181764326044288\n",
      "Iteration 1180, the loss is 4.523794061206696, parameters k is 8.092350089507928 and b is -28.181922428811085\n",
      "Iteration 1181, the loss is 4.523791259492839, parameters k is 8.092405049982236 and b is -28.18208053157788\n",
      "Iteration 1182, the loss is 4.523788457778979, parameters k is 8.092460010456545 and b is -28.182238634344678\n",
      "Iteration 1183, the loss is 4.523785656065116, parameters k is 8.092514970930853 and b is -28.182396737111475\n",
      "Iteration 1184, the loss is 4.523783037028346, parameters k is 8.092569931405162 and b is -28.18255483987827\n",
      "Iteration 1185, the loss is 4.523780766762283, parameters k is 8.092381255515834 and b is -28.18275246833677\n",
      "Iteration 1186, the loss is 4.523777965048425, parameters k is 8.092436215990142 and b is -28.182910571103566\n",
      "Iteration 1187, the loss is 4.52377516333456, parameters k is 8.092491176464451 and b is -28.183068673870363\n",
      "Iteration 1188, the loss is 4.5237723616207015, parameters k is 8.09254613693876 and b is -28.18322677663716\n",
      "Iteration 1189, the loss is 4.523769559906841, parameters k is 8.092601097413068 and b is -28.183384879403956\n",
      "Iteration 1190, the loss is 4.523766758192986, parameters k is 8.092656057887377 and b is -28.183542982170753\n",
      "Iteration 1191, the loss is 4.5237639564791206, parameters k is 8.092711018361685 and b is -28.18370108493755\n",
      "Iteration 1192, the loss is 4.523761154765264, parameters k is 8.092765978835994 and b is -28.183859187704346\n",
      "Iteration 1193, the loss is 4.523758870756691, parameters k is 8.092820939310302 and b is -28.184017290471143\n",
      "Iteration 1194, the loss is 4.523756265462431, parameters k is 8.092632263420974 and b is -28.18421491892964\n",
      "Iteration 1195, the loss is 4.52375346374857, parameters k is 8.092687223895283 and b is -28.184373021696437\n",
      "Iteration 1196, the loss is 4.523750662034705, parameters k is 8.092742184369591 and b is -28.184531124463234\n",
      "Iteration 1197, the loss is 4.523747860320847, parameters k is 8.0927971448439 and b is -28.18468922723003\n",
      "Iteration 1198, the loss is 4.5237450586069885, parameters k is 8.092852105318208 and b is -28.184847329996828\n",
      "Iteration 1199, the loss is 4.5237422568931205, parameters k is 8.092907065792517 and b is -28.185005432763624\n",
      "Iteration 1200, the loss is 4.523739455179265, parameters k is 8.092962026266825 and b is -28.18516353553042\n",
      "Iteration 1201, the loss is 4.523736792074003, parameters k is 8.093016986741134 and b is -28.185321638297218\n",
      "Iteration 1202, the loss is 4.523734565876433, parameters k is 8.092828310851806 and b is -28.185519266755716\n",
      "Iteration 1203, the loss is 4.523731764162574, parameters k is 8.092883271326114 and b is -28.185677369522512\n",
      "Iteration 1204, the loss is 4.523728962448714, parameters k is 8.092938231800423 and b is -28.18583547228931\n",
      "Iteration 1205, the loss is 4.523726160734849, parameters k is 8.092993192274731 and b is -28.185993575056106\n",
      "Iteration 1206, the loss is 4.523723359020992, parameters k is 8.09304815274904 and b is -28.186151677822902\n",
      "Iteration 1207, the loss is 4.523720557307128, parameters k is 8.093103113223348 and b is -28.1863097805897\n",
      "Iteration 1208, the loss is 4.523717755593271, parameters k is 8.093158073697657 and b is -28.186467883356496\n",
      "Iteration 1209, the loss is 4.5237149538794075, parameters k is 8.093213034171965 and b is -28.186625986123293\n",
      "Iteration 1210, the loss is 4.523712625802345, parameters k is 8.093267994646274 and b is -28.18678408889009\n",
      "Iteration 1211, the loss is 4.523710064576578, parameters k is 8.093079318756946 and b is -28.186981717348587\n",
      "Iteration 1212, the loss is 4.523707262862718, parameters k is 8.093134279231254 and b is -28.187139820115384\n",
      "Iteration 1213, the loss is 4.523704461148859, parameters k is 8.093189239705563 and b is -28.18729792288218\n",
      "Iteration 1214, the loss is 4.523701659435, parameters k is 8.093244200179871 and b is -28.187456025648977\n",
      "Iteration 1215, the loss is 4.523698857721133, parameters k is 8.09329916065418 and b is -28.187614128415774\n",
      "Iteration 1216, the loss is 4.523696056007278, parameters k is 8.093354121128488 and b is -28.18777223118257\n",
      "Iteration 1217, the loss is 4.523693254293418, parameters k is 8.093409081602797 and b is -28.187930333949367\n",
      "Iteration 1218, the loss is 4.5236905471196565, parameters k is 8.093464042077105 and b is -28.188088436716164\n",
      "Iteration 1219, the loss is 4.523688364990581, parameters k is 8.093275366187777 and b is -28.188286065174662\n",
      "Iteration 1220, the loss is 4.523685563276722, parameters k is 8.093330326662086 and b is -28.18844416794146\n",
      "Iteration 1221, the loss is 4.523682761562862, parameters k is 8.093385287136394 and b is -28.188602270708255\n",
      "Iteration 1222, the loss is 4.523679959849, parameters k is 8.093440247610703 and b is -28.188760373475052\n",
      "Iteration 1223, the loss is 4.523677158135137, parameters k is 8.093495208085011 and b is -28.18891847624185\n",
      "Iteration 1224, the loss is 4.523674356421279, parameters k is 8.09355016855932 and b is -28.189076579008645\n",
      "Iteration 1225, the loss is 4.523671554707421, parameters k is 8.093605129033628 and b is -28.189234681775442\n",
      "Iteration 1226, the loss is 4.523668752993561, parameters k is 8.093660089507937 and b is -28.18939278454224\n",
      "Iteration 1227, the loss is 4.523666380848005, parameters k is 8.093715049982245 and b is -28.189550887309036\n",
      "Iteration 1228, the loss is 4.523663863690726, parameters k is 8.093526374092917 and b is -28.189748515767533\n",
      "Iteration 1229, the loss is 4.523661061976865, parameters k is 8.093581334567226 and b is -28.18990661853433\n",
      "Iteration 1230, the loss is 4.523658260263007, parameters k is 8.093636295041534 and b is -28.190064721301127\n",
      "Iteration 1231, the loss is 4.523655458549148, parameters k is 8.093691255515843 and b is -28.190222824067924\n",
      "Iteration 1232, the loss is 4.523652656835284, parameters k is 8.093746215990151 and b is -28.19038092683472\n",
      "Iteration 1233, the loss is 4.523649855121428, parameters k is 8.09380117646446 and b is -28.190539029601517\n",
      "Iteration 1234, the loss is 4.523647053407563, parameters k is 8.093856136938768 and b is -28.190697132368314\n",
      "Iteration 1235, the loss is 4.523644302165324, parameters k is 8.093911097413077 and b is -28.19085523513511\n",
      "Iteration 1236, the loss is 4.5236421641047295, parameters k is 8.093722421523749 and b is -28.191052863593608\n",
      "Iteration 1237, the loss is 4.523639362390873, parameters k is 8.093777381998057 and b is -28.191210966360405\n",
      "Iteration 1238, the loss is 4.523636560677003, parameters k is 8.093832342472366 and b is -28.1913690691272\n",
      "Iteration 1239, the loss is 4.523633758963149, parameters k is 8.093887302946674 and b is -28.191527171894\n",
      "Iteration 1240, the loss is 4.523630957249291, parameters k is 8.093942263420983 and b is -28.191685274660795\n",
      "Iteration 1241, the loss is 4.523628155535427, parameters k is 8.093997223895292 and b is -28.191843377427592\n",
      "Iteration 1242, the loss is 4.523625353821566, parameters k is 8.0940521843696 and b is -28.19200148019439\n",
      "Iteration 1243, the loss is 4.523622552107709, parameters k is 8.094107144843909 and b is -28.192159582961185\n",
      "Iteration 1244, the loss is 4.523620135893661, parameters k is 8.094162105318217 and b is -28.192317685727982\n",
      "Iteration 1245, the loss is 4.52361766280487, parameters k is 8.093973429428889 and b is -28.19251531418648\n",
      "Iteration 1246, the loss is 4.523614861091016, parameters k is 8.094028389903198 and b is -28.192673416953276\n",
      "Iteration 1247, the loss is 4.523612059377152, parameters k is 8.094083350377506 and b is -28.192831519720073\n",
      "Iteration 1248, the loss is 4.523609257663295, parameters k is 8.094138310851815 and b is -28.19298962248687\n",
      "Iteration 1249, the loss is 4.523606455949433, parameters k is 8.094193271326123 and b is -28.193147725253667\n",
      "Iteration 1250, the loss is 4.523603654235572, parameters k is 8.094248231800432 and b is -28.193305828020463\n",
      "Iteration 1251, the loss is 4.52360085252171, parameters k is 8.09430319227474 and b is -28.19346393078726\n",
      "Iteration 1252, the loss is 4.523598057210977, parameters k is 8.094358152749049 and b is -28.193622033554057\n",
      "Iteration 1253, the loss is 4.523595963218879, parameters k is 8.09416947685972 and b is -28.193819662012555\n",
      "Iteration 1254, the loss is 4.52359316150502, parameters k is 8.094224437334029 and b is -28.19397776477935\n",
      "Iteration 1255, the loss is 4.523590359791166, parameters k is 8.094279397808338 and b is -28.194135867546148\n",
      "Iteration 1256, the loss is 4.523587558077293, parameters k is 8.094334358282646 and b is -28.194293970312945\n",
      "Iteration 1257, the loss is 4.523584756363435, parameters k is 8.094389318756955 and b is -28.19445207307974\n",
      "Iteration 1258, the loss is 4.523581954649577, parameters k is 8.094444279231263 and b is -28.194610175846538\n",
      "Iteration 1259, the loss is 4.523579152935719, parameters k is 8.094499239705572 and b is -28.194768278613335\n",
      "Iteration 1260, the loss is 4.523576351221854, parameters k is 8.09455420017988 and b is -28.19492638138013\n",
      "Iteration 1261, the loss is 4.523573890939319, parameters k is 8.094609160654189 and b is -28.19508448414693\n",
      "Iteration 1262, the loss is 4.52357146191902, parameters k is 8.09442048476486 and b is -28.195282112605426\n",
      "Iteration 1263, the loss is 4.52356866020516, parameters k is 8.09447544523917 and b is -28.195440215372223\n",
      "Iteration 1264, the loss is 4.523565858491306, parameters k is 8.094530405713478 and b is -28.19559831813902\n",
      "Iteration 1265, the loss is 4.523563056777439, parameters k is 8.094585366187786 and b is -28.195756420905816\n",
      "Iteration 1266, the loss is 4.523560255063579, parameters k is 8.094640326662095 and b is -28.195914523672613\n",
      "Iteration 1267, the loss is 4.5235574533497225, parameters k is 8.094695287136403 and b is -28.19607262643941\n",
      "Iteration 1268, the loss is 4.523554651635863, parameters k is 8.094750247610712 and b is -28.196230729206206\n",
      "Iteration 1269, the loss is 4.523551849922002, parameters k is 8.09480520808502 and b is -28.196388831973003\n",
      "Iteration 1270, the loss is 4.5235497246676655, parameters k is 8.094860168559329 and b is -28.1965469347398\n",
      "Iteration 1271, the loss is 4.523546960619169, parameters k is 8.09467149267 and b is -28.196744563198298\n",
      "Iteration 1272, the loss is 4.523544158905309, parameters k is 8.09472645314431 and b is -28.196902665965094\n",
      "Iteration 1273, the loss is 4.523541357191444, parameters k is 8.094781413618618 and b is -28.19706076873189\n",
      "Iteration 1274, the loss is 4.523538555477587, parameters k is 8.094836374092926 and b is -28.197218871498688\n",
      "Iteration 1275, the loss is 4.5235357537637215, parameters k is 8.094891334567235 and b is -28.197376974265485\n",
      "Iteration 1276, the loss is 4.523532952049858, parameters k is 8.094946295041543 and b is -28.19753507703228\n",
      "Iteration 1277, the loss is 4.523530150336007, parameters k is 8.095001255515852 and b is -28.197693179799078\n",
      "Iteration 1278, the loss is 4.523527645984976, parameters k is 8.09505621599016 and b is -28.197851282565875\n",
      "Iteration 1279, the loss is 4.523525261033174, parameters k is 8.094867540100832 and b is -28.198048911024372\n",
      "Iteration 1280, the loss is 4.5235224593193095, parameters k is 8.09492250057514 and b is -28.19820701379117\n",
      "Iteration 1281, the loss is 4.523519657605449, parameters k is 8.09497746104945 and b is -28.198365116557966\n",
      "Iteration 1282, the loss is 4.523516855891589, parameters k is 8.095032421523758 and b is -28.198523219324763\n",
      "Iteration 1283, the loss is 4.523514054177729, parameters k is 8.095087381998066 and b is -28.19868132209156\n",
      "Iteration 1284, the loss is 4.5235112524638685, parameters k is 8.095142342472375 and b is -28.198839424858356\n",
      "Iteration 1285, the loss is 4.523508450750011, parameters k is 8.095197302946683 and b is -28.198997527625153\n",
      "Iteration 1286, the loss is 4.523505649036143, parameters k is 8.095252263420992 and b is -28.19915563039195\n",
      "Iteration 1287, the loss is 4.523503479713319, parameters k is 8.0953072238953 and b is -28.199313733158746\n",
      "Iteration 1288, the loss is 4.523500759733322, parameters k is 8.095118548005972 and b is -28.199511361617244\n",
      "Iteration 1289, the loss is 4.523497958019456, parameters k is 8.095173508480281 and b is -28.19966946438404\n",
      "Iteration 1290, the loss is 4.523495156305591, parameters k is 8.09522846895459 and b is -28.199827567150837\n",
      "Iteration 1291, the loss is 4.523492354591737, parameters k is 8.095283429428898 and b is -28.199985669917634\n",
      "Iteration 1292, the loss is 4.523489552877875, parameters k is 8.095338389903207 and b is -28.20014377268443\n",
      "Iteration 1293, the loss is 4.523486751164009, parameters k is 8.095393350377515 and b is -28.200301875451228\n",
      "Iteration 1294, the loss is 4.523483949450153, parameters k is 8.095448310851824 and b is -28.200459978218024\n",
      "Iteration 1295, the loss is 4.5234814010306374, parameters k is 8.095503271326132 and b is -28.20061808098482\n",
      "Iteration 1296, the loss is 4.523479060147316, parameters k is 8.095314595436804 and b is -28.20081570944332\n",
      "Iteration 1297, the loss is 4.52347625843346, parameters k is 8.095369555911113 and b is -28.200973812210115\n",
      "Iteration 1298, the loss is 4.523473456719597, parameters k is 8.095424516385421 and b is -28.201131914976912\n",
      "Iteration 1299, the loss is 4.523470655005742, parameters k is 8.09547947685973 and b is -28.20129001774371\n",
      "Iteration 1300, the loss is 4.523467853291876, parameters k is 8.095534437334038 and b is -28.201448120510506\n",
      "Iteration 1301, the loss is 4.523465051578015, parameters k is 8.095589397808347 and b is -28.201606223277302\n",
      "Iteration 1302, the loss is 4.523462249864155, parameters k is 8.095644358282655 and b is -28.2017643260441\n",
      "Iteration 1303, the loss is 4.523459448150298, parameters k is 8.095699318756964 and b is -28.201922428810896\n",
      "Iteration 1304, the loss is 4.523457234758984, parameters k is 8.095754279231272 and b is -28.202080531577693\n",
      "Iteration 1305, the loss is 4.523454558847462, parameters k is 8.095565603341944 and b is -28.20227816003619\n",
      "Iteration 1306, the loss is 4.523451757133604, parameters k is 8.095620563816253 and b is -28.202436262802987\n",
      "Iteration 1307, the loss is 4.523448955419741, parameters k is 8.095675524290561 and b is -28.202594365569784\n",
      "Iteration 1308, the loss is 4.523446153705883, parameters k is 8.09573048476487 and b is -28.20275246833658\n",
      "Iteration 1309, the loss is 4.523443351992026, parameters k is 8.095785445239178 and b is -28.202910571103377\n",
      "Iteration 1310, the loss is 4.52344055027816, parameters k is 8.095840405713487 and b is -28.203068673870174\n",
      "Iteration 1311, the loss is 4.5234377485643025, parameters k is 8.095895366187795 and b is -28.20322677663697\n",
      "Iteration 1312, the loss is 4.523435156076294, parameters k is 8.095950326662104 and b is -28.203384879403767\n",
      "Iteration 1313, the loss is 4.523432859261468, parameters k is 8.095761650772776 and b is -28.203582507862265\n",
      "Iteration 1314, the loss is 4.5234300575476, parameters k is 8.095816611247084 and b is -28.203740610629062\n",
      "Iteration 1315, the loss is 4.52342725583375, parameters k is 8.095871571721393 and b is -28.20389871339586\n",
      "Iteration 1316, the loss is 4.523424454119886, parameters k is 8.095926532195701 and b is -28.204056816162655\n",
      "Iteration 1317, the loss is 4.523421652406024, parameters k is 8.09598149267001 and b is -28.204214918929452\n",
      "Iteration 1318, the loss is 4.523418850692168, parameters k is 8.096036453144318 and b is -28.20437302169625\n",
      "Iteration 1319, the loss is 4.523416048978303, parameters k is 8.096091413618627 and b is -28.204531124463045\n",
      "Iteration 1320, the loss is 4.52341324726445, parameters k is 8.096146374092935 and b is -28.204689227229842\n",
      "Iteration 1321, the loss is 4.523410989804635, parameters k is 8.096201334567244 and b is -28.20484732999664\n",
      "Iteration 1322, the loss is 4.52340835796162, parameters k is 8.096012658677916 and b is -28.205044958455137\n",
      "Iteration 1323, the loss is 4.52340555624775, parameters k is 8.096067619152224 and b is -28.205203061221933\n",
      "Iteration 1324, the loss is 4.523402754533893, parameters k is 8.096122579626533 and b is -28.20536116398873\n",
      "Iteration 1325, the loss is 4.523399952820031, parameters k is 8.096177540100841 and b is -28.205519266755527\n",
      "Iteration 1326, the loss is 4.523397151106166, parameters k is 8.09623250057515 and b is -28.205677369522324\n",
      "Iteration 1327, the loss is 4.5233943493923086, parameters k is 8.096287461049458 and b is -28.20583547228912\n",
      "Iteration 1328, the loss is 4.523391547678452, parameters k is 8.096342421523767 and b is -28.205993575055917\n",
      "Iteration 1329, the loss is 4.523388911121957, parameters k is 8.096397381998075 and b is -28.206151677822714\n",
      "Iteration 1330, the loss is 4.523386658375622, parameters k is 8.096208706108747 and b is -28.20634930628121\n",
      "Iteration 1331, the loss is 4.523383856661753, parameters k is 8.096263666583056 and b is -28.206507409048008\n",
      "Iteration 1332, the loss is 4.523381054947896, parameters k is 8.096318627057364 and b is -28.206665511814805\n",
      "Iteration 1333, the loss is 4.523378253234037, parameters k is 8.096373587531673 and b is -28.2068236145816\n",
      "Iteration 1334, the loss is 4.523375451520171, parameters k is 8.096428548005981 and b is -28.2069817173484\n",
      "Iteration 1335, the loss is 4.52337264980631, parameters k is 8.09648350848029 and b is -28.207139820115195\n",
      "Iteration 1336, the loss is 4.523369848092455, parameters k is 8.096538468954598 and b is -28.207297922881992\n",
      "Iteration 1337, the loss is 4.52336704637859, parameters k is 8.096593429428907 and b is -28.20745602564879\n",
      "Iteration 1338, the loss is 4.523364744850294, parameters k is 8.096648389903216 and b is -28.207614128415585\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1339, the loss is 4.523362157075757, parameters k is 8.096459714013887 and b is -28.207811756874083\n",
      "Iteration 1340, the loss is 4.523359355361898, parameters k is 8.096514674488196 and b is -28.20796985964088\n",
      "Iteration 1341, the loss is 4.5233565536480365, parameters k is 8.096569634962504 and b is -28.208127962407676\n",
      "Iteration 1342, the loss is 4.523353751934177, parameters k is 8.096624595436813 and b is -28.208286065174473\n",
      "Iteration 1343, the loss is 4.52335095022032, parameters k is 8.096679555911122 and b is -28.20844416794127\n",
      "Iteration 1344, the loss is 4.523348148506459, parameters k is 8.09673451638543 and b is -28.208602270708067\n",
      "Iteration 1345, the loss is 4.523345346792594, parameters k is 8.096789476859739 and b is -28.208760373474863\n",
      "Iteration 1346, the loss is 4.52334266616761, parameters k is 8.096844437334047 and b is -28.20891847624166\n",
      "Iteration 1347, the loss is 4.523340457489765, parameters k is 8.096655761444719 and b is -28.209116104700158\n",
      "Iteration 1348, the loss is 4.523337655775904, parameters k is 8.096710721919028 and b is -28.209274207466954\n",
      "Iteration 1349, the loss is 4.523334854062042, parameters k is 8.096765682393336 and b is -28.20943231023375\n",
      "Iteration 1350, the loss is 4.523332052348184, parameters k is 8.096820642867645 and b is -28.209590413000548\n",
      "Iteration 1351, the loss is 4.523329250634325, parameters k is 8.096875603341953 and b is -28.209748515767345\n",
      "Iteration 1352, the loss is 4.523326448920458, parameters k is 8.096930563816262 and b is -28.20990661853414\n",
      "Iteration 1353, the loss is 4.5233236472066025, parameters k is 8.09698552429057 and b is -28.210064721300938\n",
      "Iteration 1354, the loss is 4.523320845492745, parameters k is 8.097040484764879 and b is -28.210222824067735\n",
      "Iteration 1355, the loss is 4.523318499895956, parameters k is 8.097095445239187 and b is -28.21038092683453\n",
      "Iteration 1356, the loss is 4.523315956189911, parameters k is 8.096906769349859 and b is -28.21057855529303\n",
      "Iteration 1357, the loss is 4.523313154476053, parameters k is 8.096961729824168 and b is -28.210736658059826\n",
      "Iteration 1358, the loss is 4.523310352762189, parameters k is 8.097016690298476 and b is -28.210894760826623\n",
      "Iteration 1359, the loss is 4.523307551048326, parameters k is 8.097071650772785 and b is -28.21105286359342\n",
      "Iteration 1360, the loss is 4.523304749334464, parameters k is 8.097126611247093 and b is -28.211210966360216\n",
      "Iteration 1361, the loss is 4.523301947620603, parameters k is 8.097181571721402 and b is -28.211369069127013\n",
      "Iteration 1362, the loss is 4.523299145906745, parameters k is 8.09723653219571 and b is -28.21152717189381\n",
      "Iteration 1363, the loss is 4.52329642121327, parameters k is 8.097291492670019 and b is -28.211685274660606\n",
      "Iteration 1364, the loss is 4.523294256603912, parameters k is 8.09710281678069 and b is -28.211882903119104\n",
      "Iteration 1365, the loss is 4.523291454890047, parameters k is 8.097157777255 and b is -28.2120410058859\n",
      "Iteration 1366, the loss is 4.523288653176195, parameters k is 8.097212737729308 and b is -28.212199108652698\n",
      "Iteration 1367, the loss is 4.5232858514623295, parameters k is 8.097267698203616 and b is -28.212357211419494\n",
      "Iteration 1368, the loss is 4.523283049748474, parameters k is 8.097322658677925 and b is -28.21251531418629\n",
      "Iteration 1369, the loss is 4.523280248034611, parameters k is 8.097377619152233 and b is -28.212673416953088\n",
      "Iteration 1370, the loss is 4.523277446320747, parameters k is 8.097432579626542 and b is -28.212831519719884\n",
      "Iteration 1371, the loss is 4.523274644606893, parameters k is 8.09748754010085 and b is -28.21298962248668\n",
      "Iteration 1372, the loss is 4.5232722549416104, parameters k is 8.097542500575159 and b is -28.213147725253478\n",
      "Iteration 1373, the loss is 4.523269755304057, parameters k is 8.09735382468583 and b is -28.213345353711976\n",
      "Iteration 1374, the loss is 4.523266953590196, parameters k is 8.09740878516014 and b is -28.213503456478772\n",
      "Iteration 1375, the loss is 4.523264151876339, parameters k is 8.097463745634448 and b is -28.21366155924557\n",
      "Iteration 1376, the loss is 4.523261350162472, parameters k is 8.097518706108756 and b is -28.213819662012366\n",
      "Iteration 1377, the loss is 4.523258548448614, parameters k is 8.097573666583065 and b is -28.213977764779163\n",
      "Iteration 1378, the loss is 4.5232557467347565, parameters k is 8.097628627057373 and b is -28.21413586754596\n",
      "Iteration 1379, the loss is 4.523252945020898, parameters k is 8.097683587531682 and b is -28.214293970312756\n",
      "Iteration 1380, the loss is 4.523250176258925, parameters k is 8.09773854800599 and b is -28.214452073079553\n",
      "Iteration 1381, the loss is 4.523248055718065, parameters k is 8.097549872116662 and b is -28.21464970153805\n",
      "Iteration 1382, the loss is 4.523245254004204, parameters k is 8.09760483259097 and b is -28.214807804304847\n",
      "Iteration 1383, the loss is 4.523242452290339, parameters k is 8.09765979306528 and b is -28.214965907071644\n",
      "Iteration 1384, the loss is 4.523239650576482, parameters k is 8.097714753539588 and b is -28.21512400983844\n",
      "Iteration 1385, the loss is 4.523236848862617, parameters k is 8.097769714013896 and b is -28.215282112605237\n",
      "Iteration 1386, the loss is 4.52323404714876, parameters k is 8.097824674488205 and b is -28.215440215372034\n",
      "Iteration 1387, the loss is 4.523231245434896, parameters k is 8.097879634962514 and b is -28.21559831813883\n",
      "Iteration 1388, the loss is 4.523228443721037, parameters k is 8.097934595436822 and b is -28.215756420905628\n",
      "Iteration 1389, the loss is 4.523226009987269, parameters k is 8.09798955591113 and b is -28.215914523672424\n",
      "Iteration 1390, the loss is 4.5232235544182045, parameters k is 8.097800880021802 and b is -28.216112152130922\n",
      "Iteration 1391, the loss is 4.523220752704343, parameters k is 8.097855840496111 and b is -28.21627025489772\n",
      "Iteration 1392, the loss is 4.523217950990484, parameters k is 8.09791080097042 and b is -28.216428357664515\n",
      "Iteration 1393, the loss is 4.523215149276625, parameters k is 8.097965761444728 and b is -28.216586460431312\n",
      "Iteration 1394, the loss is 4.523212347562765, parameters k is 8.098020721919037 and b is -28.21674456319811\n",
      "Iteration 1395, the loss is 4.523209545848902, parameters k is 8.098075682393345 and b is -28.216902665964906\n",
      "Iteration 1396, the loss is 4.52320674413504, parameters k is 8.098130642867654 and b is -28.217060768731702\n",
      "Iteration 1397, the loss is 4.5232039424211825, parameters k is 8.098185603341962 and b is -28.2172188714985\n",
      "Iteration 1398, the loss is 4.523201843715615, parameters k is 8.09824056381627 and b is -28.217376974265296\n",
      "Iteration 1399, the loss is 4.5231990531183515, parameters k is 8.098051887926943 and b is -28.217574602723793\n",
      "Iteration 1400, the loss is 4.5231962514044834, parameters k is 8.098106848401251 and b is -28.21773270549059\n",
      "Iteration 1401, the loss is 4.523193449690626, parameters k is 8.09816180887556 and b is -28.217890808257387\n",
      "Iteration 1402, the loss is 4.523190647976769, parameters k is 8.098216769349868 and b is -28.218048911024184\n",
      "Iteration 1403, the loss is 4.523187846262906, parameters k is 8.098271729824177 and b is -28.21820701379098\n",
      "Iteration 1404, the loss is 4.523185044549049, parameters k is 8.098326690298485 and b is -28.218365116557777\n",
      "Iteration 1405, the loss is 4.523182242835189, parameters k is 8.098381650772794 and b is -28.218523219324574\n",
      "Iteration 1406, the loss is 4.523179765032931, parameters k is 8.098436611247102 and b is -28.21868132209137\n",
      "Iteration 1407, the loss is 4.5231773535323505, parameters k is 8.098247935357774 and b is -28.21887895054987\n",
      "Iteration 1408, the loss is 4.523174551818495, parameters k is 8.098302895832083 and b is -28.219037053316665\n",
      "Iteration 1409, the loss is 4.523171750104629, parameters k is 8.098357856306391 and b is -28.21919515608346\n",
      "Iteration 1410, the loss is 4.52316894839077, parameters k is 8.0984128167807 and b is -28.21935325885026\n",
      "Iteration 1411, the loss is 4.523166146676912, parameters k is 8.098467777255008 and b is -28.219511361617055\n",
      "Iteration 1412, the loss is 4.5231633449630495, parameters k is 8.098522737729317 and b is -28.219669464383852\n",
      "Iteration 1413, the loss is 4.523160543249192, parameters k is 8.098577698203625 and b is -28.21982756715065\n",
      "Iteration 1414, the loss is 4.523157741535328, parameters k is 8.098632658677934 and b is -28.219985669917445\n",
      "Iteration 1415, the loss is 4.523155598761276, parameters k is 8.098687619152242 and b is -28.220143772684242\n",
      "Iteration 1416, the loss is 4.523152852232503, parameters k is 8.098498943262914 and b is -28.22034140114274\n",
      "Iteration 1417, the loss is 4.5231500505186375, parameters k is 8.098553903737223 and b is -28.220499503909537\n",
      "Iteration 1418, the loss is 4.5231472488047775, parameters k is 8.098608864211531 and b is -28.220657606676333\n",
      "Iteration 1419, the loss is 4.523144447090917, parameters k is 8.09866382468584 and b is -28.22081570944313\n",
      "Iteration 1420, the loss is 4.523141645377056, parameters k is 8.098718785160148 and b is -28.220973812209927\n",
      "Iteration 1421, the loss is 4.523138843663194, parameters k is 8.098773745634457 and b is -28.221131914976723\n",
      "Iteration 1422, the loss is 4.523136041949335, parameters k is 8.098828706108765 and b is -28.22129001774352\n",
      "Iteration 1423, the loss is 4.523133520078588, parameters k is 8.098883666583074 and b is -28.221448120510317\n",
      "Iteration 1424, the loss is 4.5231311526465, parameters k is 8.098694990693746 and b is -28.221645748968815\n",
      "Iteration 1425, the loss is 4.5231283509326365, parameters k is 8.098749951168054 and b is -28.22180385173561\n",
      "Iteration 1426, the loss is 4.523125549218784, parameters k is 8.098804911642363 and b is -28.221961954502408\n",
      "Iteration 1427, the loss is 4.523122747504922, parameters k is 8.098859872116671 and b is -28.222120057269205\n",
      "Iteration 1428, the loss is 4.523119945791061, parameters k is 8.09891483259098 and b is -28.222278160036\n",
      "Iteration 1429, the loss is 4.523117144077197, parameters k is 8.098969793065288 and b is -28.2224362628028\n",
      "Iteration 1430, the loss is 4.52311434236334, parameters k is 8.099024753539597 and b is -28.222594365569595\n",
      "Iteration 1431, the loss is 4.523111540649478, parameters k is 8.099079714013905 and b is -28.22275246833639\n",
      "Iteration 1432, the loss is 4.523109353806931, parameters k is 8.099134674488214 and b is -28.22291057110319\n",
      "Iteration 1433, the loss is 4.5231066513466445, parameters k is 8.098945998598886 and b is -28.223108199561686\n",
      "Iteration 1434, the loss is 4.523103849632782, parameters k is 8.099000959073194 and b is -28.223266302328483\n",
      "Iteration 1435, the loss is 4.523101047918926, parameters k is 8.099055919547503 and b is -28.22342440509528\n",
      "Iteration 1436, the loss is 4.52309824620506, parameters k is 8.099110880021811 and b is -28.223582507862076\n",
      "Iteration 1437, the loss is 4.523095444491201, parameters k is 8.09916584049612 and b is -28.223740610628873\n",
      "Iteration 1438, the loss is 4.523092642777344, parameters k is 8.099220800970429 and b is -28.22389871339567\n",
      "Iteration 1439, the loss is 4.523089841063479, parameters k is 8.099275761444737 and b is -28.224056816162467\n",
      "Iteration 1440, the loss is 4.523087275124246, parameters k is 8.099330721919046 and b is -28.224214918929263\n",
      "Iteration 1441, the loss is 4.523084951760654, parameters k is 8.099142046029717 and b is -28.22441254738776\n",
      "Iteration 1442, the loss is 4.523082150046792, parameters k is 8.099197006504026 and b is -28.224570650154558\n",
      "Iteration 1443, the loss is 4.5230793483329315, parameters k is 8.099251966978334 and b is -28.224728752921354\n",
      "Iteration 1444, the loss is 4.523076546619068, parameters k is 8.099306927452643 and b is -28.22488685568815\n",
      "Iteration 1445, the loss is 4.523073744905209, parameters k is 8.099361887926952 and b is -28.225044958454948\n",
      "Iteration 1446, the loss is 4.523070943191348, parameters k is 8.09941684840126 and b is -28.225203061221745\n",
      "Iteration 1447, the loss is 4.523068141477486, parameters k is 8.099471808875569 and b is -28.22536116398854\n",
      "Iteration 1448, the loss is 4.523065339763628, parameters k is 8.099526769349877 and b is -28.225519266755338\n",
      "Iteration 1449, the loss is 4.52306310885259, parameters k is 8.099581729824186 and b is -28.225677369522135\n",
      "Iteration 1450, the loss is 4.523060450460795, parameters k is 8.099393053934858 and b is -28.225874997980632\n",
      "Iteration 1451, the loss is 4.523057648746936, parameters k is 8.099448014409166 and b is -28.22603310074743\n",
      "Iteration 1452, the loss is 4.523054847033073, parameters k is 8.099502974883475 and b is -28.226191203514226\n",
      "Iteration 1453, the loss is 4.523052045319221, parameters k is 8.099557935357783 and b is -28.226349306281023\n",
      "Iteration 1454, the loss is 4.523049243605351, parameters k is 8.099612895832092 and b is -28.22650740904782\n",
      "Iteration 1455, the loss is 4.523046441891493, parameters k is 8.0996678563064 and b is -28.226665511814616\n",
      "Iteration 1456, the loss is 4.523043640177636, parameters k is 8.099722816780709 and b is -28.226823614581413\n",
      "Iteration 1457, the loss is 4.523041030169901, parameters k is 8.099777777255017 and b is -28.22698171734821\n",
      "Iteration 1458, the loss is 4.523038750874796, parameters k is 8.099589101365689 and b is -28.227179345806707\n",
      "Iteration 1459, the loss is 4.523035949160937, parameters k is 8.099644061839998 and b is -28.227337448573504\n",
      "Iteration 1460, the loss is 4.5230331474470775, parameters k is 8.099699022314306 and b is -28.2274955513403\n",
      "Iteration 1461, the loss is 4.523030345733225, parameters k is 8.099753982788615 and b is -28.227653654107097\n",
      "Iteration 1462, the loss is 4.5230275440193575, parameters k is 8.099808943262923 and b is -28.227811756873894\n",
      "Iteration 1463, the loss is 4.523024742305493, parameters k is 8.099863903737232 and b is -28.22796985964069\n",
      "Iteration 1464, the loss is 4.523021940591634, parameters k is 8.09991886421154 and b is -28.228127962407488\n",
      "Iteration 1465, the loss is 4.523019138877783, parameters k is 8.099973824685849 and b is -28.228286065174284\n",
      "Iteration 1466, the loss is 4.523016863898252, parameters k is 8.100028785160157 and b is -28.22844416794108\n",
      "Iteration 1467, the loss is 4.523014249574941, parameters k is 8.09984010927083 and b is -28.22864179639958\n",
      "Iteration 1468, the loss is 4.523011447861085, parameters k is 8.099895069745138 and b is -28.228799899166376\n",
      "Iteration 1469, the loss is 4.5230086461472245, parameters k is 8.099950030219446 and b is -28.228958001933172\n",
      "Iteration 1470, the loss is 4.523005844433361, parameters k is 8.100004990693755 and b is -28.22911610469997\n",
      "Iteration 1471, the loss is 4.5230030427195045, parameters k is 8.100059951168063 and b is -28.229274207466766\n",
      "Iteration 1472, the loss is 4.523000241005638, parameters k is 8.100114911642372 and b is -28.229432310233562\n",
      "Iteration 1473, the loss is 4.522997439291779, parameters k is 8.10016987211668 and b is -28.22959041300036\n",
      "Iteration 1474, the loss is 4.522994785215563, parameters k is 8.100224832590989 and b is -28.229748515767156\n",
      "Iteration 1475, the loss is 4.52299254998895, parameters k is 8.10003615670166 and b is -28.229946144225654\n",
      "Iteration 1476, the loss is 4.522989748275087, parameters k is 8.10009111717597 and b is -28.23010424699245\n",
      "Iteration 1477, the loss is 4.522986946561227, parameters k is 8.100146077650278 and b is -28.230262349759247\n",
      "Iteration 1478, the loss is 4.522984144847366, parameters k is 8.100201038124586 and b is -28.230420452526044\n",
      "Iteration 1479, the loss is 4.522981343133507, parameters k is 8.100255998598895 and b is -28.23057855529284\n",
      "Iteration 1480, the loss is 4.522978541419646, parameters k is 8.100310959073203 and b is -28.230736658059637\n",
      "Iteration 1481, the loss is 4.522975739705781, parameters k is 8.100365919547512 and b is -28.230894760826434\n",
      "Iteration 1482, the loss is 4.522972937991927, parameters k is 8.10042088002182 and b is -28.23105286359323\n",
      "Iteration 1483, the loss is 4.522970618943907, parameters k is 8.100475840496129 and b is -28.231210966360027\n",
      "Iteration 1484, the loss is 4.522968048689091, parameters k is 8.1002871646068 and b is -28.231408594818525\n",
      "Iteration 1485, the loss is 4.522965246975227, parameters k is 8.10034212508111 and b is -28.231566697585322\n",
      "Iteration 1486, the loss is 4.522962445261375, parameters k is 8.100397085555418 and b is -28.23172480035212\n",
      "Iteration 1487, the loss is 4.522959643547507, parameters k is 8.100452046029726 and b is -28.231882903118915\n",
      "Iteration 1488, the loss is 4.522956841833648, parameters k is 8.100507006504035 and b is -28.232041005885712\n",
      "Iteration 1489, the loss is 4.522954040119793, parameters k is 8.100561966978344 and b is -28.23219910865251\n",
      "Iteration 1490, the loss is 4.52295123840593, parameters k is 8.100616927452652 and b is -28.232357211419306\n",
      "Iteration 1491, the loss is 4.522948540261217, parameters k is 8.10067188792696 and b is -28.232515314186102\n",
      "Iteration 1492, the loss is 4.522946349103099, parameters k is 8.100483212037632 and b is -28.2327129426446\n",
      "Iteration 1493, the loss is 4.522943547389232, parameters k is 8.100538172511941 and b is -28.232871045411397\n",
      "Iteration 1494, the loss is 4.522940745675373, parameters k is 8.10059313298625 and b is -28.233029148178193\n",
      "Iteration 1495, the loss is 4.522937943961515, parameters k is 8.100648093460558 and b is -28.23318725094499\n",
      "Iteration 1496, the loss is 4.522935142247653, parameters k is 8.100703053934867 and b is -28.233345353711787\n",
      "Iteration 1497, the loss is 4.522932340533793, parameters k is 8.100758014409175 and b is -28.233503456478584\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1498, the loss is 4.522929538819937, parameters k is 8.100812974883484 and b is -28.23366155924538\n",
      "Iteration 1499, the loss is 4.52292673710607, parameters k is 8.100867935357792 and b is -28.233819662012177\n",
      "Iteration 1500, the loss is 4.522924373989564, parameters k is 8.1009228958321 and b is -28.233977764778974\n",
      "Iteration 1501, the loss is 4.522921847803237, parameters k is 8.100734219942773 and b is -28.23417539323747\n",
      "Iteration 1502, the loss is 4.52291904608938, parameters k is 8.100789180417081 and b is -28.23433349600427\n",
      "Iteration 1503, the loss is 4.522916244375518, parameters k is 8.10084414089139 and b is -28.234491598771065\n",
      "Iteration 1504, the loss is 4.522913442661658, parameters k is 8.100899101365698 and b is -28.23464970153786\n",
      "Iteration 1505, the loss is 4.522910640947794, parameters k is 8.100954061840007 and b is -28.23480780430466\n",
      "Iteration 1506, the loss is 4.522907839233935, parameters k is 8.101009022314315 and b is -28.234965907071455\n",
      "Iteration 1507, the loss is 4.522905037520079, parameters k is 8.101063982788624 and b is -28.235124009838252\n",
      "Iteration 1508, the loss is 4.522902295306877, parameters k is 8.101118943262932 and b is -28.23528211260505\n",
      "Iteration 1509, the loss is 4.522900148217242, parameters k is 8.100930267373604 and b is -28.235479741063546\n",
      "Iteration 1510, the loss is 4.522897346503381, parameters k is 8.100985227847913 and b is -28.235637843830343\n",
      "Iteration 1511, the loss is 4.522894544789525, parameters k is 8.101040188322221 and b is -28.23579594659714\n",
      "Iteration 1512, the loss is 4.522891743075662, parameters k is 8.10109514879653 and b is -28.235954049363936\n",
      "Iteration 1513, the loss is 4.522888941361805, parameters k is 8.101150109270838 and b is -28.236112152130733\n",
      "Iteration 1514, the loss is 4.522886139647941, parameters k is 8.101205069745147 and b is -28.23627025489753\n",
      "Iteration 1515, the loss is 4.522883337934083, parameters k is 8.101260030219455 and b is -28.236428357664327\n",
      "Iteration 1516, the loss is 4.522880536220218, parameters k is 8.101314990693764 and b is -28.236586460431123\n",
      "Iteration 1517, the loss is 4.522878129035221, parameters k is 8.101369951168072 and b is -28.23674456319792\n",
      "Iteration 1518, the loss is 4.522875646917386, parameters k is 8.101181275278744 and b is -28.236942191656418\n",
      "Iteration 1519, the loss is 4.522872845203528, parameters k is 8.101236235753053 and b is -28.237100294423215\n",
      "Iteration 1520, the loss is 4.522870043489665, parameters k is 8.101291196227361 and b is -28.23725839719001\n",
      "Iteration 1521, the loss is 4.522867241775802, parameters k is 8.10134615670167 and b is -28.237416499956808\n",
      "Iteration 1522, the loss is 4.522864440061949, parameters k is 8.101401117175978 and b is -28.237574602723605\n",
      "Iteration 1523, the loss is 4.522861638348088, parameters k is 8.101456077650287 and b is -28.2377327054904\n",
      "Iteration 1524, the loss is 4.522858836634226, parameters k is 8.101511038124595 and b is -28.237890808257198\n",
      "Iteration 1525, the loss is 4.522856050352537, parameters k is 8.101565998598904 and b is -28.238048911023995\n",
      "Iteration 1526, the loss is 4.522853947331394, parameters k is 8.101377322709576 and b is -28.238246539482493\n",
      "Iteration 1527, the loss is 4.522851145617533, parameters k is 8.101432283183884 and b is -28.23840464224929\n",
      "Iteration 1528, the loss is 4.522848343903676, parameters k is 8.101487243658193 and b is -28.238562745016086\n",
      "Iteration 1529, the loss is 4.522845542189814, parameters k is 8.101542204132501 and b is -28.238720847782883\n",
      "Iteration 1530, the loss is 4.522842740475951, parameters k is 8.10159716460681 and b is -28.23887895054968\n",
      "Iteration 1531, the loss is 4.522839938762093, parameters k is 8.101652125081118 and b is -28.239037053316476\n",
      "Iteration 1532, the loss is 4.522837137048226, parameters k is 8.101707085555427 and b is -28.239195156083273\n",
      "Iteration 1533, the loss is 4.522834335334365, parameters k is 8.101762046029735 and b is -28.23935325885007\n",
      "Iteration 1534, the loss is 4.522831884080875, parameters k is 8.101817006504044 and b is -28.239511361616866\n",
      "Iteration 1535, the loss is 4.522829446031539, parameters k is 8.101628330614716 and b is -28.239708990075364\n",
      "Iteration 1536, the loss is 4.52282664431767, parameters k is 8.101683291089024 and b is -28.23986709284216\n",
      "Iteration 1537, the loss is 4.522823842603815, parameters k is 8.101738251563333 and b is -28.240025195608958\n",
      "Iteration 1538, the loss is 4.5228210408899585, parameters k is 8.101793212037641 and b is -28.240183298375754\n",
      "Iteration 1539, the loss is 4.522818239176095, parameters k is 8.10184817251195 and b is -28.24034140114255\n",
      "Iteration 1540, the loss is 4.52281543746223, parameters k is 8.101903132986259 and b is -28.240499503909348\n",
      "Iteration 1541, the loss is 4.522812635748375, parameters k is 8.101958093460567 and b is -28.240657606676145\n",
      "Iteration 1542, the loss is 4.522809834034509, parameters k is 8.102013053934876 and b is -28.24081570944294\n",
      "Iteration 1543, the loss is 4.522807717809223, parameters k is 8.102068014409184 and b is -28.240973812209738\n",
      "Iteration 1544, the loss is 4.5228049447316785, parameters k is 8.101879338519856 and b is -28.241171440668236\n",
      "Iteration 1545, the loss is 4.522802143017821, parameters k is 8.101934298994165 and b is -28.241329543435032\n",
      "Iteration 1546, the loss is 4.522799341303961, parameters k is 8.101989259468473 and b is -28.24148764620183\n",
      "Iteration 1547, the loss is 4.52279653959009, parameters k is 8.102044219942782 and b is -28.241645748968626\n",
      "Iteration 1548, the loss is 4.5227937378762375, parameters k is 8.10209918041709 and b is -28.241803851735423\n",
      "Iteration 1549, the loss is 4.522790936162377, parameters k is 8.102154140891399 and b is -28.24196195450222\n",
      "Iteration 1550, the loss is 4.52278813444852, parameters k is 8.102209101365707 and b is -28.242120057269016\n",
      "Iteration 1551, the loss is 4.522785639126537, parameters k is 8.102264061840016 and b is -28.242278160035813\n",
      "Iteration 1552, the loss is 4.522783245145693, parameters k is 8.102075385950688 and b is -28.24247578849431\n",
      "Iteration 1553, the loss is 4.522780443431824, parameters k is 8.102130346424996 and b is -28.242633891261107\n",
      "Iteration 1554, the loss is 4.522777641717963, parameters k is 8.102185306899305 and b is -28.242791994027904\n",
      "Iteration 1555, the loss is 4.522774840004101, parameters k is 8.102240267373613 and b is -28.2429500967947\n",
      "Iteration 1556, the loss is 4.52277203829024, parameters k is 8.102295227847922 and b is -28.243108199561497\n",
      "Iteration 1557, the loss is 4.522769236576384, parameters k is 8.10235018832223 and b is -28.243266302328294\n",
      "Iteration 1558, the loss is 4.522766434862524, parameters k is 8.102405148796539 and b is -28.24342440509509\n",
      "Iteration 1559, the loss is 4.522763633148663, parameters k is 8.102460109270847 and b is -28.243582507861888\n",
      "Iteration 1560, the loss is 4.522761472854882, parameters k is 8.102515069745156 and b is -28.243740610628684\n",
      "Iteration 1561, the loss is 4.522758743845831, parameters k is 8.102326393855828 and b is -28.243938239087182\n",
      "Iteration 1562, the loss is 4.522755942131969, parameters k is 8.102381354330136 and b is -28.24409634185398\n",
      "Iteration 1563, the loss is 4.52275314041811, parameters k is 8.102436314804445 and b is -28.244254444620776\n",
      "Iteration 1564, the loss is 4.522750338704249, parameters k is 8.102491275278753 and b is -28.244412547387572\n",
      "Iteration 1565, the loss is 4.522747536990383, parameters k is 8.102546235753062 and b is -28.24457065015437\n",
      "Iteration 1566, the loss is 4.522744735276526, parameters k is 8.10260119622737 and b is -28.244728752921166\n",
      "Iteration 1567, the loss is 4.522741933562668, parameters k is 8.102656156701679 and b is -28.244886855687962\n",
      "Iteration 1568, the loss is 4.522739394172195, parameters k is 8.102711117175987 and b is -28.24504495845476\n",
      "Iteration 1569, the loss is 4.522737044259837, parameters k is 8.10252244128666 and b is -28.245242586913257\n",
      "Iteration 1570, the loss is 4.522734242545977, parameters k is 8.102577401760968 and b is -28.245400689680054\n",
      "Iteration 1571, the loss is 4.522731440832108, parameters k is 8.102632362235276 and b is -28.24555879244685\n",
      "Iteration 1572, the loss is 4.522728639118251, parameters k is 8.102687322709585 and b is -28.245716895213647\n",
      "Iteration 1573, the loss is 4.52272583740439, parameters k is 8.102742283183893 and b is -28.245874997980444\n",
      "Iteration 1574, the loss is 4.522723035690528, parameters k is 8.102797243658202 and b is -28.24603310074724\n",
      "Iteration 1575, the loss is 4.522720233976671, parameters k is 8.10285220413251 and b is -28.246191203514037\n",
      "Iteration 1576, the loss is 4.522717432262809, parameters k is 8.102907164606819 and b is -28.246349306280834\n",
      "Iteration 1577, the loss is 4.522715227900542, parameters k is 8.102962125081127 and b is -28.24650740904763\n",
      "Iteration 1578, the loss is 4.522712542959975, parameters k is 8.1027734491918 and b is -28.24670503750613\n",
      "Iteration 1579, the loss is 4.5227097412461195, parameters k is 8.102828409666108 and b is -28.246863140272925\n",
      "Iteration 1580, the loss is 4.522706939532256, parameters k is 8.102883370140416 and b is -28.247021243039722\n",
      "Iteration 1581, the loss is 4.522704137818392, parameters k is 8.102938330614725 and b is -28.24717934580652\n",
      "Iteration 1582, the loss is 4.522701336104532, parameters k is 8.102993291089033 and b is -28.247337448573315\n",
      "Iteration 1583, the loss is 4.52269853439068, parameters k is 8.103048251563342 and b is -28.247495551340112\n",
      "Iteration 1584, the loss is 4.522695732676814, parameters k is 8.10310321203765 and b is -28.24765365410691\n",
      "Iteration 1585, the loss is 4.522693149217855, parameters k is 8.103158172511959 and b is -28.247811756873705\n",
      "Iteration 1586, the loss is 4.522690843373978, parameters k is 8.10296949662263 and b is -28.248009385332203\n",
      "Iteration 1587, the loss is 4.522688041660121, parameters k is 8.10302445709694 and b is -28.248167488099\n",
      "Iteration 1588, the loss is 4.522685239946258, parameters k is 8.103079417571248 and b is -28.248325590865797\n",
      "Iteration 1589, the loss is 4.522682438232402, parameters k is 8.103134378045556 and b is -28.248483693632593\n",
      "Iteration 1590, the loss is 4.522679636518542, parameters k is 8.103189338519865 and b is -28.24864179639939\n",
      "Iteration 1591, the loss is 4.522676834804675, parameters k is 8.103244298994174 and b is -28.248799899166187\n",
      "Iteration 1592, the loss is 4.522674033090824, parameters k is 8.103299259468482 and b is -28.248958001932984\n",
      "Iteration 1593, the loss is 4.52267123137696, parameters k is 8.10335421994279 and b is -28.24911610469978\n",
      "Iteration 1594, the loss is 4.522668982946198, parameters k is 8.1034091804171 and b is -28.249274207466577\n",
      "Iteration 1595, the loss is 4.522666342074127, parameters k is 8.103220504527771 and b is -28.249471835925075\n",
      "Iteration 1596, the loss is 4.522663540360266, parameters k is 8.10327546500208 and b is -28.24962993869187\n",
      "Iteration 1597, the loss is 4.5226607386464055, parameters k is 8.103330425476388 and b is -28.249788041458668\n",
      "Iteration 1598, the loss is 4.522657936932542, parameters k is 8.103385385950697 and b is -28.249946144225465\n",
      "Iteration 1599, the loss is 4.522655135218684, parameters k is 8.103440346425005 and b is -28.25010424699226\n",
      "Iteration 1600, the loss is 4.522652333504822, parameters k is 8.103495306899314 and b is -28.25026234975906\n",
      "Iteration 1601, the loss is 4.5226495317909645, parameters k is 8.103550267373622 and b is -28.250420452525855\n",
      "Iteration 1602, the loss is 4.52264690426351, parameters k is 8.10360522784793 and b is -28.250578555292652\n",
      "Iteration 1603, the loss is 4.522644642488131, parameters k is 8.103416551958603 and b is -28.25077618375115\n",
      "Iteration 1604, the loss is 4.522641840774267, parameters k is 8.103471512432911 and b is -28.250934286517946\n",
      "Iteration 1605, the loss is 4.522639039060407, parameters k is 8.10352647290722 and b is -28.251092389284743\n",
      "Iteration 1606, the loss is 4.522636237346547, parameters k is 8.103581433381528 and b is -28.25125049205154\n",
      "Iteration 1607, the loss is 4.522633435632688, parameters k is 8.103636393855837 and b is -28.251408594818336\n",
      "Iteration 1608, the loss is 4.522630633918827, parameters k is 8.103691354330145 and b is -28.251566697585133\n",
      "Iteration 1609, the loss is 4.522627832204966, parameters k is 8.103746314804454 and b is -28.25172480035193\n",
      "Iteration 1610, the loss is 4.522625030491106, parameters k is 8.103801275278762 and b is -28.251882903118727\n",
      "Iteration 1611, the loss is 4.522622737991852, parameters k is 8.10385623575307 and b is -28.252041005885523\n",
      "Iteration 1612, the loss is 4.522620141188273, parameters k is 8.103667559863743 and b is -28.25223863434402\n",
      "Iteration 1613, the loss is 4.522617339474415, parameters k is 8.103722520338051 and b is -28.252396737110818\n",
      "Iteration 1614, the loss is 4.522614537760551, parameters k is 8.10377748081236 and b is -28.252554839877615\n",
      "Iteration 1615, the loss is 4.522611736046693, parameters k is 8.103832441286668 and b is -28.25271294264441\n",
      "Iteration 1616, the loss is 4.522608934332833, parameters k is 8.103887401760977 and b is -28.252871045411208\n",
      "Iteration 1617, the loss is 4.522606132618969, parameters k is 8.103942362235285 and b is -28.253029148178005\n",
      "Iteration 1618, the loss is 4.522603330905112, parameters k is 8.103997322709594 and b is -28.2531872509448\n",
      "Iteration 1619, the loss is 4.522600659309177, parameters k is 8.104052283183902 and b is -28.253345353711598\n",
      "Iteration 1620, the loss is 4.522598723557088, parameters k is 8.103863607294574 and b is -28.253542982170096\n",
      "Iteration 1621, the loss is 4.522596263387891, parameters k is 8.104150504527777 and b is -28.253661559245195\n",
      "Iteration 1622, the loss is 4.52259290247614, parameters k is 8.103961828638448 and b is -28.253859187703693\n",
      "Iteration 1623, the loss is 4.522590100762285, parameters k is 8.104016789112757 and b is -28.25401729047049\n",
      "Iteration 1624, the loss is 4.522587299048421, parameters k is 8.104071749587066 and b is -28.254175393237286\n",
      "Iteration 1625, the loss is 4.5225844973345595, parameters k is 8.104126710061374 and b is -28.254333496004083\n",
      "Iteration 1626, the loss is 4.522581695620702, parameters k is 8.104181670535683 and b is -28.25449159877088\n",
      "Iteration 1627, the loss is 4.5225788939068385, parameters k is 8.104236631009991 and b is -28.254649701537677\n",
      "Iteration 1628, the loss is 4.522576272294186, parameters k is 8.1042915914843 and b is -28.254807804304473\n",
      "Iteration 1629, the loss is 4.522574516556773, parameters k is 8.104102915594972 and b is -28.25500543276297\n",
      "Iteration 1630, the loss is 4.522571876372909, parameters k is 8.104389812828174 and b is -28.25512400983807\n",
      "Iteration 1631, the loss is 4.522568465477865, parameters k is 8.104201136938846 and b is -28.255321638296568\n",
      "Iteration 1632, the loss is 4.522565663764, parameters k is 8.104256097413154 and b is -28.255479741063365\n",
      "Iteration 1633, the loss is 4.522562862050145, parameters k is 8.104311057887463 and b is -28.25563784383016\n",
      "Iteration 1634, the loss is 4.522560060336285, parameters k is 8.104366018361771 and b is -28.255795946596958\n",
      "Iteration 1635, the loss is 4.522557258622423, parameters k is 8.10442097883608 and b is -28.255954049363755\n",
      "Iteration 1636, the loss is 4.522554456908562, parameters k is 8.104475939310388 and b is -28.25611215213055\n",
      "Iteration 1637, the loss is 4.522551885279196, parameters k is 8.104530899784697 and b is -28.25627025489735\n",
      "Iteration 1638, the loss is 4.522550309556479, parameters k is 8.104342223895369 and b is -28.256467883355846\n",
      "Iteration 1639, the loss is 4.522547489357919, parameters k is 8.104629121128571 and b is -28.256586460430945\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1640, the loss is 4.522544028479596, parameters k is 8.104440445239243 and b is -28.256784088889443\n",
      "Iteration 1641, the loss is 4.522541226765735, parameters k is 8.104495405713552 and b is -28.25694219165624\n",
      "Iteration 1642, the loss is 4.5225384250518745, parameters k is 8.10455036618786 and b is -28.257100294423037\n",
      "Iteration 1643, the loss is 4.522535623338012, parameters k is 8.104605326662169 and b is -28.257258397189833\n",
      "Iteration 1644, the loss is 4.522532821624149, parameters k is 8.104660287136477 and b is -28.25741649995663\n",
      "Iteration 1645, the loss is 4.52253001991029, parameters k is 8.104715247610786 and b is -28.257574602723427\n",
      "Iteration 1646, the loss is 4.522527498264198, parameters k is 8.104770208085094 and b is -28.257732705490223\n",
      "Iteration 1647, the loss is 4.522526102556173, parameters k is 8.104581532195766 and b is -28.25793033394872\n",
      "Iteration 1648, the loss is 4.522523102342924, parameters k is 8.104868429428969 and b is -28.25804891102382\n",
      "Iteration 1649, the loss is 4.52251959148132, parameters k is 8.10467975353964 and b is -28.258246539482318\n",
      "Iteration 1650, the loss is 4.522516789767456, parameters k is 8.104734714013949 and b is -28.258404642249115\n",
      "Iteration 1651, the loss is 4.5225139880535945, parameters k is 8.104789674488257 and b is -28.25856274501591\n",
      "Iteration 1652, the loss is 4.522511186339739, parameters k is 8.104844634962566 and b is -28.25872084778271\n",
      "Iteration 1653, the loss is 4.522508384625879, parameters k is 8.104899595436875 and b is -28.258878950549505\n",
      "Iteration 1654, the loss is 4.522505582912018, parameters k is 8.104954555911183 and b is -28.259037053316302\n",
      "Iteration 1655, the loss is 4.52250311124921, parameters k is 8.105009516385492 and b is -28.2591951560831\n",
      "Iteration 1656, the loss is 4.522501895555865, parameters k is 8.104820840496163 and b is -28.259392784541596\n",
      "Iteration 1657, the loss is 4.522498715327939, parameters k is 8.105107737729366 and b is -28.259511361616696\n",
      "Iteration 1658, the loss is 4.522495328139953, parameters k is 8.104919061840038 and b is -28.259708990075193\n",
      "Iteration 1659, the loss is 4.522494319406658, parameters k is 8.10520595907324 and b is -28.259827567150293\n",
      "Iteration 1660, the loss is 4.522489615356907, parameters k is 8.105017283183912 and b is -28.26002519560879\n",
      "Iteration 1661, the loss is 4.52248681364305, parameters k is 8.10507224365822 and b is -28.260183298375587\n",
      "Iteration 1662, the loss is 4.52248401192919, parameters k is 8.105127204132529 and b is -28.260341401142384\n",
      "Iteration 1663, the loss is 4.522481210215328, parameters k is 8.105182164606838 and b is -28.26049950390918\n",
      "Iteration 1664, the loss is 4.522478503490893, parameters k is 8.105237125081146 and b is -28.260657606675977\n",
      "Iteration 1665, the loss is 4.5224780242139815, parameters k is 8.105048449191818 and b is -28.260855235134475\n",
      "Iteration 1666, the loss is 4.522474107569617, parameters k is 8.10533534642502 and b is -28.260973812209574\n",
      "Iteration 1667, the loss is 4.5224714567980735, parameters k is 8.105146670535692 and b is -28.261171440668072\n",
      "Iteration 1668, the loss is 4.522469711648332, parameters k is 8.105433567768895 and b is -28.26129001774317\n",
      "Iteration 1669, the loss is 4.522465242660219, parameters k is 8.105244891879567 and b is -28.26148764620167\n",
      "Iteration 1670, the loss is 4.522462440946359, parameters k is 8.105299852353875 and b is -28.261645748968466\n",
      "Iteration 1671, the loss is 4.522459639232491, parameters k is 8.105354812828184 and b is -28.261803851735262\n",
      "Iteration 1672, the loss is 4.522456837518637, parameters k is 8.105409773302492 and b is -28.26196195450206\n",
      "Iteration 1673, the loss is 4.522454035804775, parameters k is 8.1054647337768 and b is -28.262120057268856\n",
      "Iteration 1674, the loss is 4.522451808143597, parameters k is 8.10551969425111 and b is -28.262278160035653\n",
      "Iteration 1675, the loss is 4.522450701334936, parameters k is 8.105331018361781 and b is -28.26247578849415\n",
      "Iteration 1676, the loss is 4.52244741222232, parameters k is 8.105617915594983 and b is -28.26259436556925\n",
      "Iteration 1677, the loss is 4.522444133919021, parameters k is 8.105429239705655 and b is -28.262791994027747\n",
      "Iteration 1678, the loss is 4.522443016301042, parameters k is 8.105716136938858 and b is -28.262910571102847\n",
      "Iteration 1679, the loss is 4.522438068249661, parameters k is 8.10552746104953 and b is -28.263108199561344\n",
      "Iteration 1680, the loss is 4.522435266535804, parameters k is 8.105582421523838 and b is -28.26326630232814\n",
      "Iteration 1681, the loss is 4.522432464821946, parameters k is 8.105637381998147 and b is -28.263424405094938\n",
      "Iteration 1682, the loss is 4.522429663108081, parameters k is 8.105692342472455 and b is -28.263582507861734\n",
      "Iteration 1683, the loss is 4.522427200385272, parameters k is 8.105747302946764 and b is -28.26374061062853\n",
      "Iteration 1684, the loss is 4.5224268299930515, parameters k is 8.105558627057436 and b is -28.26393823908703\n",
      "Iteration 1685, the loss is 4.522422804463992, parameters k is 8.105845524290638 and b is -28.264056816162128\n",
      "Iteration 1686, the loss is 4.5224202625771435, parameters k is 8.10565684840131 and b is -28.264254444620626\n",
      "Iteration 1687, the loss is 4.522418408542717, parameters k is 8.105943745634512 and b is -28.264373021695725\n",
      "Iteration 1688, the loss is 4.5224136955529755, parameters k is 8.105755069745184 and b is -28.264570650154223\n",
      "Iteration 1689, the loss is 4.522410893839115, parameters k is 8.105810030219493 and b is -28.26472875292102\n",
      "Iteration 1690, the loss is 4.522408092125253, parameters k is 8.105864990693801 and b is -28.264886855687816\n",
      "Iteration 1691, the loss is 4.52240529041139, parameters k is 8.10591995116811 and b is -28.265044958454613\n",
      "Iteration 1692, the loss is 4.522402592626949, parameters k is 8.105974911642418 and b is -28.26520306122141\n",
      "Iteration 1693, the loss is 4.522402958651172, parameters k is 8.10578623575309 and b is -28.265400689679907\n",
      "Iteration 1694, the loss is 4.522398196705674, parameters k is 8.106073132986293 and b is -28.265519266755007\n",
      "Iteration 1695, the loss is 4.522396391235267, parameters k is 8.105884457096964 and b is -28.265716895213505\n",
      "Iteration 1696, the loss is 4.522393800784391, parameters k is 8.106171354330167 and b is -28.265835472288604\n",
      "Iteration 1697, the loss is 4.52238982381935, parameters k is 8.105982678440839 and b is -28.2660331007471\n",
      "Iteration 1698, the loss is 4.52238940486312, parameters k is 8.106269575674041 and b is -28.2661516778222\n",
      "Iteration 1699, the loss is 4.522383783730143, parameters k is 8.106080899784713 and b is -28.2663493062807\n",
      "Iteration 1700, the loss is 4.522380982016285, parameters k is 8.106135860259021 and b is -28.266507409047495\n",
      "Iteration 1701, the loss is 4.522378180302424, parameters k is 8.10619082073333 and b is -28.266665511814292\n",
      "Iteration 1702, the loss is 4.522375676536324, parameters k is 8.106245781207639 and b is -28.26682361458109\n",
      "Iteration 1703, the loss is 4.522375971430548, parameters k is 8.10605710531831 and b is -28.267021243039586\n",
      "Iteration 1704, the loss is 4.522371280615044, parameters k is 8.106344002551513 and b is -28.267139820114686\n",
      "Iteration 1705, the loss is 4.522369404014637, parameters k is 8.106155326662185 and b is -28.267337448573183\n",
      "Iteration 1706, the loss is 4.522366884693764, parameters k is 8.106442223895387 and b is -28.267456025648283\n",
      "Iteration 1707, the loss is 4.522362836598723, parameters k is 8.106253548006059 and b is -28.26765365410678\n",
      "Iteration 1708, the loss is 4.52236248877249, parameters k is 8.106540445239261 and b is -28.26777223118188\n",
      "Iteration 1709, the loss is 4.522356673621171, parameters k is 8.106351769349933 and b is -28.267969859640377\n",
      "Iteration 1710, the loss is 4.522353871907319, parameters k is 8.106406729824242 and b is -28.268127962407174\n",
      "Iteration 1711, the loss is 4.5223510701934515, parameters k is 8.10646169029855 and b is -28.26828606517397\n",
      "Iteration 1712, the loss is 4.522348760445692, parameters k is 8.106516650772859 and b is -28.268444167940768\n",
      "Iteration 1713, the loss is 4.522348984209922, parameters k is 8.10632797488353 and b is -28.268641796399265\n",
      "Iteration 1714, the loss is 4.522344364524419, parameters k is 8.106614872116733 and b is -28.268760373474365\n",
      "Iteration 1715, the loss is 4.52234241679401, parameters k is 8.106426196227405 and b is -28.268958001932862\n",
      "Iteration 1716, the loss is 4.522339968603138, parameters k is 8.106713093460607 and b is -28.26907657900796\n",
      "Iteration 1717, the loss is 4.522335849378099, parameters k is 8.10652441757128 and b is -28.26927420746646\n",
      "Iteration 1718, the loss is 4.522335572681859, parameters k is 8.106811314804482 and b is -28.26939278454156\n",
      "Iteration 1719, the loss is 4.522329563512208, parameters k is 8.106622638915153 and b is -28.269590413000056\n",
      "Iteration 1720, the loss is 4.522326761798344, parameters k is 8.106677599389462 and b is -28.269748515766853\n",
      "Iteration 1721, the loss is 4.522323960084484, parameters k is 8.10673255986377 and b is -28.26990661853365\n",
      "Iteration 1722, the loss is 4.522321844355064, parameters k is 8.106787520338079 and b is -28.270064721300447\n",
      "Iteration 1723, the loss is 4.5223219969893, parameters k is 8.106598844448751 and b is -28.270262349758944\n",
      "Iteration 1724, the loss is 4.52231744843379, parameters k is 8.106885741681953 and b is -28.270380926834044\n",
      "Iteration 1725, the loss is 4.5223154295733865, parameters k is 8.106697065792625 and b is -28.27057855529254\n",
      "Iteration 1726, the loss is 4.522313052512514, parameters k is 8.106983963025828 and b is -28.27069713236764\n",
      "Iteration 1727, the loss is 4.522308862157476, parameters k is 8.1067952871365 and b is -28.27089476082614\n",
      "Iteration 1728, the loss is 4.522308656591229, parameters k is 8.107082184369702 and b is -28.271013337901238\n",
      "Iteration 1729, the loss is 4.522302453403241, parameters k is 8.106893508480374 and b is -28.271210966359735\n",
      "Iteration 1730, the loss is 4.522299651689377, parameters k is 8.106948468954682 and b is -28.271369069126532\n",
      "Iteration 1731, the loss is 4.522297015853411, parameters k is 8.10700342942899 and b is -28.27152717189333\n",
      "Iteration 1732, the loss is 4.522298461305838, parameters k is 8.106814753539663 and b is -28.271724800351826\n",
      "Iteration 1733, the loss is 4.5222926199321325, parameters k is 8.107101650772865 and b is -28.271843377426926\n",
      "Iteration 1734, the loss is 4.522291893889927, parameters k is 8.106912974883537 and b is -28.272041005885423\n",
      "Iteration 1735, the loss is 4.522288224010856, parameters k is 8.10719987211674 and b is -28.272159582960523\n",
      "Iteration 1736, the loss is 4.522285326474012, parameters k is 8.107011196227411 and b is -28.27235721141902\n",
      "Iteration 1737, the loss is 4.522283828089575, parameters k is 8.107298093460614 and b is -28.27247578849412\n",
      "Iteration 1738, the loss is 4.522278759058107, parameters k is 8.107109417571285 and b is -28.272673416952617\n",
      "Iteration 1739, the loss is 4.522279432168298, parameters k is 8.107396314804488 and b is -28.272791994027717\n",
      "Iteration 1740, the loss is 4.522272605881995, parameters k is 8.10720763891516 and b is -28.272989622486215\n",
      "Iteration 1741, the loss is 4.522269879019452, parameters k is 8.107262599389468 and b is -28.27314772525301\n",
      "Iteration 1742, the loss is 4.522271809743635, parameters k is 8.10707392350014 and b is -28.27334535371151\n",
      "Iteration 1743, the loss is 4.522265483098175, parameters k is 8.107360820733343 and b is -28.27346393078661\n",
      "Iteration 1744, the loss is 4.522265242327728, parameters k is 8.107172144844014 and b is -28.273661559245106\n",
      "Iteration 1745, the loss is 4.522261087176895, parameters k is 8.107459042077217 and b is -28.273780136320205\n",
      "Iteration 1746, the loss is 4.522258674911815, parameters k is 8.107270366187889 and b is -28.273977764778703\n",
      "Iteration 1747, the loss is 4.522256691255616, parameters k is 8.107557263421091 and b is -28.274096341853802\n",
      "Iteration 1748, the loss is 4.522252107495904, parameters k is 8.107368587531763 and b is -28.2742939703123\n",
      "Iteration 1749, the loss is 4.522252295334342, parameters k is 8.107655484764965 and b is -28.2744125473874\n",
      "Iteration 1750, the loss is 4.5222455600746025, parameters k is 8.107466808875637 and b is -28.274610175845897\n",
      "Iteration 1751, the loss is 4.522242758360741, parameters k is 8.107521769349946 and b is -28.274768278612694\n",
      "Iteration 1752, the loss is 4.522240654596523, parameters k is 8.107576729824254 and b is -28.27492638137949\n",
      "Iteration 1753, the loss is 4.522241706644266, parameters k is 8.107388053934926 and b is -28.275124009837988\n",
      "Iteration 1754, the loss is 4.522236258675235, parameters k is 8.107674951168129 and b is -28.275242586913087\n",
      "Iteration 1755, the loss is 4.522235139228354, parameters k is 8.1074862752788 and b is -28.275440215371585\n",
      "Iteration 1756, the loss is 4.52223186275396, parameters k is 8.107773172512003 and b is -28.275558792446684\n",
      "Iteration 1757, the loss is 4.522228571812447, parameters k is 8.107584496622675 and b is -28.275756420905182\n",
      "Iteration 1758, the loss is 4.522227466832686, parameters k is 8.107871393855877 and b is -28.27587499798028\n",
      "Iteration 1759, the loss is 4.52222200439653, parameters k is 8.107682717966549 and b is -28.27607262643878\n",
      "Iteration 1760, the loss is 4.5222230709114095, parameters k is 8.107969615199751 and b is -28.27619120351388\n",
      "Iteration 1761, the loss is 4.522215712553355, parameters k is 8.107780939310423 and b is -28.276388831972376\n",
      "Iteration 1762, the loss is 4.522213517762557, parameters k is 8.107835899784732 and b is -28.276546934739173\n",
      "Iteration 1763, the loss is 4.522215055082061, parameters k is 8.107647223895404 and b is -28.27674456319767\n",
      "Iteration 1764, the loss is 4.522209121841283, parameters k is 8.107934121128606 and b is -28.27686314027277\n",
      "Iteration 1765, the loss is 4.522208487666148, parameters k is 8.107745445239278 and b is -28.277060768731268\n",
      "Iteration 1766, the loss is 4.522204725919998, parameters k is 8.10803234247248 and b is -28.277179345806367\n",
      "Iteration 1767, the loss is 4.52220192025024, parameters k is 8.107843666583152 and b is -28.277376974264865\n",
      "Iteration 1768, the loss is 4.522200329998728, parameters k is 8.108130563816355 and b is -28.277495551339964\n",
      "Iteration 1769, the loss is 4.522195352834333, parameters k is 8.107941887927026 and b is -28.27769317979846\n",
      "Iteration 1770, the loss is 4.5221959340774465, parameters k is 8.108228785160229 and b is -28.27781175687356\n",
      "Iteration 1771, the loss is 4.522188785418424, parameters k is 8.1080401092709 and b is -28.27800938533206\n",
      "Iteration 1772, the loss is 4.522191538156173, parameters k is 8.108327006504103 and b is -28.278127962407158\n",
      "Iteration 1773, the loss is 4.522184072596289, parameters k is 8.108138330614775 and b is -28.278325590865656\n",
      "Iteration 1774, the loss is 4.522185287641119, parameters k is 8.107949654725447 and b is -28.278523219324153\n",
      "Iteration 1775, the loss is 4.52217967667501, parameters k is 8.10823655195865 and b is -28.278641796399253\n",
      "Iteration 1776, the loss is 4.522178720225206, parameters k is 8.108047876069321 and b is -28.27883942485775\n",
      "Iteration 1777, the loss is 4.522175280753733, parameters k is 8.108334773302524 and b is -28.27895800193285\n",
      "Iteration 1778, the loss is 4.522172152809298, parameters k is 8.108146097413195 and b is -28.279155630391347\n",
      "Iteration 1779, the loss is 4.522170884832459, parameters k is 8.108432994646398 and b is -28.279274207466447\n",
      "Iteration 1780, the loss is 4.522165585393382, parameters k is 8.10824431875707 and b is -28.279471835924944\n",
      "Iteration 1781, the loss is 4.522166488911182, parameters k is 8.108531215990272 and b is -28.279590413000044\n",
      "Iteration 1782, the loss is 4.522159157802471, parameters k is 8.108342540100944 and b is -28.27978804145854\n",
      "Iteration 1783, the loss is 4.522156715018995, parameters k is 8.10838580097051 and b is -28.279946144225338\n",
      "Iteration 1784, the loss is 4.522158971737335, parameters k is 8.108197125081182 and b is -28.280143772683836\n",
      "Iteration 1785, the loss is 4.522152319097717, parameters k is 8.108484022314384 and b is -28.280262349758935\n",
      "Iteration 1786, the loss is 4.522152404321428, parameters k is 8.108295346425056 and b is -28.280459978217433\n",
      "Iteration 1787, the loss is 4.522147923176443, parameters k is 8.108582243658258 and b is -28.280578555292532\n",
      "Iteration 1788, the loss is 4.522145836905518, parameters k is 8.10839356776893 and b is -28.28077618375103\n",
      "Iteration 1789, the loss is 4.5221435272551656, parameters k is 8.108680465002132 and b is -28.28089476082613\n",
      "Iteration 1790, the loss is 4.522139269489608, parameters k is 8.108491789112804 and b is -28.281092389284627\n",
      "Iteration 1791, the loss is 4.522139131333885, parameters k is 8.108778686346007 and b is -28.281210966359726\n",
      "Iteration 1792, the loss is 4.522132702073698, parameters k is 8.108590010456679 and b is -28.281408594818224\n",
      "Iteration 1793, the loss is 4.5221347354126165, parameters k is 8.108876907689881 and b is -28.281527171893323\n",
      "Iteration 1794, the loss is 4.522127269852728, parameters k is 8.108688231800553 and b is -28.28172480035182\n",
      "Iteration 1795, the loss is 4.522129204296392, parameters k is 8.108499555911225 and b is -28.28192242881032\n",
      "Iteration 1796, the loss is 4.522122873931452, parameters k is 8.108786453144427 and b is -28.282041005885418\n",
      "Iteration 1797, the loss is 4.522122636880482, parameters k is 8.108597777255099 and b is -28.282238634343916\n",
      "Iteration 1798, the loss is 4.522118478010174, parameters k is 8.108884674488301 and b is -28.282357211419015\n",
      "Iteration 1799, the loss is 4.522116069464575, parameters k is 8.108695998598973 and b is -28.282554839877513\n",
      "Iteration 1800, the loss is 4.5221140820889, parameters k is 8.108982895832176 and b is -28.282673416952612\n",
      "Iteration 1801, the loss is 4.522109502048665, parameters k is 8.108794219942848 and b is -28.28287104541111\n",
      "Iteration 1802, the loss is 4.5221096861676235, parameters k is 8.10908111717605 and b is -28.28298962248621\n",
      "Iteration 1803, the loss is 4.522103036439659, parameters k is 8.108892441286722 and b is -28.283187250944707\n",
      "Iteration 1804, the loss is 4.522100349640881, parameters k is 8.108935702156288 and b is -28.283345353711503\n",
      "Iteration 1805, the loss is 4.522097662842112, parameters k is 8.108978963025853 and b is -28.2835034564783\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1806, the loss is 4.522095295610825, parameters k is 8.109022223895419 and b is -28.283661559245097\n",
      "Iteration 1807, the loss is 4.522096656635123, parameters k is 8.108833548006091 and b is -28.283859187703595\n",
      "Iteration 1808, the loss is 4.5220908996895535, parameters k is 8.109120445239293 and b is -28.283977764778694\n",
      "Iteration 1809, the loss is 4.522090089219218, parameters k is 8.108931769349965 and b is -28.28417539323719\n",
      "Iteration 1810, the loss is 4.522086503768274, parameters k is 8.109218666583168 and b is -28.28429397031229\n",
      "Iteration 1811, the loss is 4.522083521803305, parameters k is 8.10902999069384 and b is -28.28449159877079\n",
      "Iteration 1812, the loss is 4.522082107847003, parameters k is 8.109316887927042 and b is -28.284610175845888\n",
      "Iteration 1813, the loss is 4.522076954387395, parameters k is 8.109128212037714 and b is -28.284807804304386\n",
      "Iteration 1814, the loss is 4.522077711925714, parameters k is 8.109415109270916 and b is -28.284926381379485\n",
      "Iteration 1815, the loss is 4.522070970866848, parameters k is 8.109226433381588 and b is -28.285124009837983\n",
      "Iteration 1816, the loss is 4.522068284068074, parameters k is 8.109269694251154 and b is -28.28528211260478\n",
      "Iteration 1817, the loss is 4.522065629701223, parameters k is 8.10931295512072 and b is -28.285440215371576\n",
      "Iteration 1818, the loss is 4.522067224852607, parameters k is 8.109124279231391 and b is -28.285637843830074\n",
      "Iteration 1819, the loss is 4.522061233779956, parameters k is 8.109411176464594 and b is -28.285756420905173\n",
      "Iteration 1820, the loss is 4.5220606574366915, parameters k is 8.109222500575266 and b is -28.28595404936367\n",
      "Iteration 1821, the loss is 4.522056837858672, parameters k is 8.109509397808468 and b is -28.28607262643877\n",
      "Iteration 1822, the loss is 4.522054090020787, parameters k is 8.10932072191914 and b is -28.286270254897268\n",
      "Iteration 1823, the loss is 4.5220524419373955, parameters k is 8.109607619152342 and b is -28.286388831972367\n",
      "Iteration 1824, the loss is 4.522047522604879, parameters k is 8.109418943263014 and b is -28.286586460430865\n",
      "Iteration 1825, the loss is 4.522048046016124, parameters k is 8.109705840496217 and b is -28.286705037505964\n",
      "Iteration 1826, the loss is 4.52204159209281, parameters k is 8.109517164606888 and b is -28.286902665964462\n",
      "Iteration 1827, the loss is 4.522038905294037, parameters k is 8.109560425476454 and b is -28.28706076873126\n",
      "Iteration 1828, the loss is 4.522036218495265, parameters k is 8.10960368634602 and b is -28.287218871498055\n",
      "Iteration 1829, the loss is 4.5220336554593255, parameters k is 8.109646947215586 and b is -28.287376974264852\n",
      "Iteration 1830, the loss is 4.522034677191338, parameters k is 8.109458271326258 and b is -28.28757460272335\n",
      "Iteration 1831, the loss is 4.522029259538046, parameters k is 8.10974516855946 and b is -28.28769317979845\n",
      "Iteration 1832, the loss is 4.522028109775428, parameters k is 8.109556492670132 and b is -28.287890808256947\n",
      "Iteration 1833, the loss is 4.522024863616771, parameters k is 8.109843389903334 and b is -28.288009385332046\n",
      "Iteration 1834, the loss is 4.522021542359514, parameters k is 8.109654714014006 and b is -28.288207013790544\n",
      "Iteration 1835, the loss is 4.522020467695496, parameters k is 8.109941611247208 and b is -28.288325590865643\n",
      "Iteration 1836, the loss is 4.522014974943607, parameters k is 8.10975293535788 and b is -28.28852321932414\n",
      "Iteration 1837, the loss is 4.522017345339415, parameters k is 8.110039832591083 and b is -28.28864179639924\n",
      "Iteration 1838, the loss is 4.522015739499994, parameters k is 8.109579259468553 and b is -28.28887895054944\n",
      "Iteration 1839, the loss is 4.522006961980202, parameters k is 8.109866156701756 and b is -28.288997527624538\n",
      "Iteration 1840, the loss is 4.522004275181439, parameters k is 8.109909417571322 and b is -28.289155630391335\n",
      "Iteration 1841, the loss is 4.522001588382662, parameters k is 8.109952678440887 and b is -28.28931373315813\n",
      "Iteration 1842, the loss is 4.521998901583899, parameters k is 8.109995939310453 and b is -28.289471835924928\n",
      "Iteration 1843, the loss is 4.521996531338339, parameters k is 8.110039200180019 and b is -28.289629938691725\n",
      "Iteration 1844, the loss is 4.521996708569108, parameters k is 8.10985052429069 and b is -28.289827567150223\n",
      "Iteration 1845, the loss is 4.521992135417067, parameters k is 8.110137421523893 and b is -28.289946144225322\n",
      "Iteration 1846, the loss is 4.521990141153209, parameters k is 8.109948745634565 and b is -28.29014377268382\n",
      "Iteration 1847, the loss is 4.521987931738978, parameters k is 8.110235642867767 and b is -28.29026234975892\n",
      "Iteration 1848, the loss is 4.521990905709591, parameters k is 8.109775069745238 and b is -28.290499503909118\n",
      "Iteration 1849, the loss is 4.521981268655078, parameters k is 8.11006196697844 and b is -28.290618080984217\n",
      "Iteration 1850, the loss is 4.521981897991584, parameters k is 8.110348864211643 and b is -28.290736658059316\n",
      "Iteration 1851, the loss is 4.52198203321147, parameters k is 8.109888291089113 and b is -28.290973812209515\n",
      "Iteration 1852, the loss is 4.521972504740087, parameters k is 8.110175188322316 and b is -28.291092389284614\n",
      "Iteration 1853, the loss is 4.521969817941318, parameters k is 8.110218449191882 and b is -28.29125049205141\n",
      "Iteration 1854, the loss is 4.521967131142549, parameters k is 8.110261710061447 and b is -28.291408594818208\n",
      "Iteration 1855, the loss is 4.521964444343778, parameters k is 8.110304970931013 and b is -28.291566697585004\n",
      "Iteration 1856, the loss is 4.5219617575450055, parameters k is 8.110348231800579 and b is -28.2917248003518\n",
      "Iteration 1857, the loss is 4.521959070746236, parameters k is 8.110391492670145 and b is -28.291882903118598\n",
      "Iteration 1858, the loss is 4.521956383947467, parameters k is 8.11043475353971 and b is -28.292041005885395\n",
      "Iteration 1859, the loss is 4.5219540365949475, parameters k is 8.110478014409276 and b is -28.29219910865219\n",
      "Iteration 1860, the loss is 4.52195365464435, parameters k is 8.110289338519948 and b is -28.29239673711069\n",
      "Iteration 1861, the loss is 4.521950188545445, parameters k is 8.11057623575315 and b is -28.29251531418579\n",
      "Iteration 1862, the loss is 4.521954419200743, parameters k is 8.110115662630621 and b is -28.292752468335987\n",
      "Iteration 1863, the loss is 4.52194478214623, parameters k is 8.110402559863823 and b is -28.292871045411086\n",
      "Iteration 1864, the loss is 4.521944154798044, parameters k is 8.110689457097026 and b is -28.292989622486186\n",
      "Iteration 1865, the loss is 4.521945546702615, parameters k is 8.110228883974496 and b is -28.293226776636384\n",
      "Iteration 1866, the loss is 4.521935909648101, parameters k is 8.110515781207699 and b is -28.293345353711484\n",
      "Iteration 1867, the loss is 4.521938121050642, parameters k is 8.110802678440901 and b is -28.293463930786583\n",
      "Iteration 1868, the loss is 4.521936674204496, parameters k is 8.110342105318372 and b is -28.29370108493678\n",
      "Iteration 1869, the loss is 4.5219274225638735, parameters k is 8.110629002551574 and b is -28.29381966201188\n",
      "Iteration 1870, the loss is 4.5219247357651025, parameters k is 8.11067226342114 and b is -28.293977764778678\n",
      "Iteration 1871, the loss is 4.521922048966335, parameters k is 8.110715524290706 and b is -28.294135867545474\n",
      "Iteration 1872, the loss is 4.5219193621675595, parameters k is 8.110758785160272 and b is -28.29429397031227\n",
      "Iteration 1873, the loss is 4.52191667536879, parameters k is 8.110802046029837 and b is -28.294452073079068\n",
      "Iteration 1874, the loss is 4.521913988570017, parameters k is 8.110845306899403 and b is -28.294610175845865\n",
      "Iteration 1875, the loss is 4.521911436866384, parameters k is 8.110888567768969 and b is -28.29476827861266\n",
      "Iteration 1876, the loss is 4.521912557971068, parameters k is 8.110659931405332 and b is -28.29496590707116\n",
      "Iteration 1877, the loss is 4.521906519800509, parameters k is 8.110946828638534 and b is -28.29508448414626\n",
      "Iteration 1878, the loss is 4.5219071370101105, parameters k is 8.110718192274897 and b is -28.295282112604756\n",
      "Iteration 1879, the loss is 4.521901602734632, parameters k is 8.1110050895081 and b is -28.295400689679855\n",
      "Iteration 1880, the loss is 4.521901716049154, parameters k is 8.110776453144462 and b is -28.295598318138353\n",
      "Iteration 1881, the loss is 4.5218966969130205, parameters k is 8.111063350377664 and b is -28.295716895213452\n",
      "Iteration 1882, the loss is 4.521902480605544, parameters k is 8.110602777255135 and b is -28.29595404936365\n",
      "Iteration 1883, the loss is 4.5218928435510275, parameters k is 8.110889674488337 and b is -28.29607262643875\n",
      "Iteration 1884, the loss is 4.521890663165618, parameters k is 8.11117657172154 and b is -28.29619120351385\n",
      "Iteration 1885, the loss is 4.52189360810742, parameters k is 8.11071599859901 and b is -28.29642835766405\n",
      "Iteration 1886, the loss is 4.521883971052904, parameters k is 8.111002895832213 and b is -28.296546934739148\n",
      "Iteration 1887, the loss is 4.521884629418217, parameters k is 8.111289793065415 and b is -28.296665511814247\n",
      "Iteration 1888, the loss is 4.521884735609297, parameters k is 8.110829219942886 and b is -28.296902665964446\n",
      "Iteration 1889, the loss is 4.521875098554777, parameters k is 8.111116117176088 and b is -28.297021243039545\n",
      "Iteration 1890, the loss is 4.521878595670817, parameters k is 8.11140301440929 and b is -28.297139820114644\n",
      "Iteration 1891, the loss is 4.521875863111165, parameters k is 8.110942441286761 and b is -28.297376974264843\n",
      "Iteration 1892, the loss is 4.521866708630954, parameters k is 8.111229338519964 and b is -28.297495551339942\n",
      "Iteration 1893, the loss is 4.521864021832176, parameters k is 8.11127259938953 and b is -28.29765365410674\n",
      "Iteration 1894, the loss is 4.5218613350334085, parameters k is 8.111315860259095 and b is -28.297811756873536\n",
      "Iteration 1895, the loss is 4.5218589230749675, parameters k is 8.111359121128661 and b is -28.297969859640332\n",
      "Iteration 1896, the loss is 4.521861094513978, parameters k is 8.111130484765024 and b is -28.29816748809883\n",
      "Iteration 1897, the loss is 4.52185400600909, parameters k is 8.111417381998226 and b is -28.29828606517393\n",
      "Iteration 1898, the loss is 4.521855673553022, parameters k is 8.111188745634589 and b is -28.298483693632427\n",
      "Iteration 1899, the loss is 4.521849088943206, parameters k is 8.111475642867791 and b is -28.298602270707526\n",
      "Iteration 1900, the loss is 4.521850252592062, parameters k is 8.111247006504154 and b is -28.298799899166024\n",
      "Iteration 1901, the loss is 4.521844171877324, parameters k is 8.111533903737357 and b is -28.298918476241123\n",
      "Iteration 1902, the loss is 4.5218448316311015, parameters k is 8.11130526737372 and b is -28.29911610469962\n",
      "Iteration 1903, the loss is 4.52183925481144, parameters k is 8.111592164606922 and b is -28.29923468177472\n",
      "Iteration 1904, the loss is 4.521839410670144, parameters k is 8.111363528243285 and b is -28.299432310233218\n",
      "Iteration 1905, the loss is 4.521834337745561, parameters k is 8.111650425476487 and b is -28.299550887308317\n",
      "Iteration 1906, the loss is 4.521833989709182, parameters k is 8.11142178911285 and b is -28.299748515766815\n",
      "Iteration 1907, the loss is 4.521829420679681, parameters k is 8.111708686346052 and b is -28.299867092841914\n",
      "Iteration 1908, the loss is 4.521828568748226, parameters k is 8.111480049982415 and b is -28.300064721300412\n",
      "Iteration 1909, the loss is 4.521824503613802, parameters k is 8.111766947215617 and b is -28.30018329837551\n",
      "Iteration 1910, the loss is 4.521823147787269, parameters k is 8.11153831085198 and b is -28.30038092683401\n",
      "Iteration 1911, the loss is 4.521819586547917, parameters k is 8.111825208085182 and b is -28.30049950390911\n",
      "Iteration 1912, the loss is 4.521817726826312, parameters k is 8.111596571721545 and b is -28.300697132367606\n",
      "Iteration 1913, the loss is 4.521814669482033, parameters k is 8.111883468954748 and b is -28.300815709442706\n",
      "Iteration 1914, the loss is 4.521812305865352, parameters k is 8.11165483259111 and b is -28.301013337901203\n",
      "Iteration 1915, the loss is 4.521809752416156, parameters k is 8.111941729824313 and b is -28.301131914976303\n",
      "Iteration 1916, the loss is 4.521806884904393, parameters k is 8.111713093460676 and b is -28.3013295434348\n",
      "Iteration 1917, the loss is 4.521804835350269, parameters k is 8.111999990693878 and b is -28.3014481205099\n",
      "Iteration 1918, the loss is 4.521801463943432, parameters k is 8.11177135433024 and b is -28.301645748968397\n",
      "Iteration 1919, the loss is 4.521799918284392, parameters k is 8.112058251563443 and b is -28.301764326043497\n",
      "Iteration 1920, the loss is 4.521796042982473, parameters k is 8.111829615199806 and b is -28.301961954501994\n",
      "Iteration 1921, the loss is 4.521795001218508, parameters k is 8.112116512433008 and b is -28.302080531577094\n",
      "Iteration 1922, the loss is 4.521790622021516, parameters k is 8.111887876069371 and b is -28.30227816003559\n",
      "Iteration 1923, the loss is 4.52179008415263, parameters k is 8.112174773302574 and b is -28.30239673711069\n",
      "Iteration 1924, the loss is 4.52178520106056, parameters k is 8.111946136938936 and b is -28.30259436556919\n",
      "Iteration 1925, the loss is 4.5217851670867475, parameters k is 8.112233034172139 and b is -28.302712942644288\n",
      "Iteration 1926, the loss is 4.521779780099602, parameters k is 8.112004397808501 and b is -28.302910571102785\n",
      "Iteration 1927, the loss is 4.521780250020866, parameters k is 8.112291295041704 and b is -28.303029148177885\n",
      "Iteration 1928, the loss is 4.52177435913864, parameters k is 8.112062658678067 and b is -28.303226776636382\n",
      "Iteration 1929, the loss is 4.521775332954981, parameters k is 8.112349555911269 and b is -28.30334535371148\n",
      "Iteration 1930, the loss is 4.521768938177679, parameters k is 8.112120919547632 and b is -28.30354298216998\n",
      "Iteration 1931, the loss is 4.521770415889102, parameters k is 8.112407816780834 and b is -28.30366155924508\n",
      "Iteration 1932, the loss is 4.521763517216721, parameters k is 8.112179180417197 and b is -28.303859187703576\n",
      "Iteration 1933, the loss is 4.521765498823221, parameters k is 8.1124660776504 and b is -28.303977764778676\n",
      "Iteration 1934, the loss is 4.521758096255764, parameters k is 8.112237441286762 and b is -28.304175393237173\n",
      "Iteration 1935, the loss is 4.521760581757338, parameters k is 8.112524338519965 and b is -28.304293970312273\n",
      "Iteration 1936, the loss is 4.521752675294808, parameters k is 8.112295702156327 and b is -28.30449159877077\n",
      "Iteration 1937, the loss is 4.521755761598068, parameters k is 8.11258259938953 and b is -28.30461017584587\n",
      "Iteration 1938, the loss is 4.521753439851193, parameters k is 8.112122026267 and b is -28.30484732999607\n",
      "Iteration 1939, the loss is 4.521744969111646, parameters k is 8.112408923500203 and b is -28.304965907071168\n",
      "Iteration 1940, the loss is 4.521742446963167, parameters k is 8.112423923500202 and b is -28.305124009837964\n",
      "Iteration 1941, the loss is 4.521739924814673, parameters k is 8.112438923500202 and b is -28.30528211260476\n",
      "Iteration 1942, the loss is 4.5217374026661865, parameters k is 8.112453923500201 and b is -28.305440215371558\n",
      "Iteration 1943, the loss is 4.521734880517697, parameters k is 8.1124689235002 and b is -28.305598318138355\n",
      "Iteration 1944, the loss is 4.521732358369215, parameters k is 8.1124839235002 and b is -28.30575642090515\n",
      "Iteration 1945, the loss is 4.521729972303393, parameters k is 8.1124989235002 and b is -28.305914523671948\n",
      "Iteration 1946, the loss is 4.5217321900248875, parameters k is 8.112785820733402 and b is -28.306033100747047\n",
      "Iteration 1947, the loss is 4.52172474953245, parameters k is 8.112557184369765 and b is -28.306230729205545\n",
      "Iteration 1948, the loss is 4.521722246260215, parameters k is 8.112572184369764 and b is -28.306388831972342\n",
      "Iteration 1949, the loss is 4.521724491352945, parameters k is 8.112859081602966 and b is -28.30650740904744\n",
      "Iteration 1950, the loss is 4.521717140695681, parameters k is 8.11263044523933 and b is -28.30670503750594\n",
      "Iteration 1951, the loss is 4.521714618547197, parameters k is 8.112645445239329 and b is -28.306863140272736\n",
      "Iteration 1952, the loss is 4.5217122151348335, parameters k is 8.112660445239328 and b is -28.307021243039532\n",
      "Iteration 1953, the loss is 4.521714011074938, parameters k is 8.11294734247253 and b is -28.30713982011463\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1954, the loss is 4.521707009710434, parameters k is 8.112718706108893 and b is -28.30733744857313\n",
      "Iteration 1955, the loss is 4.521704489091654, parameters k is 8.112733706108893 and b is -28.307495551339926\n",
      "Iteration 1956, the loss is 4.521706312402994, parameters k is 8.113020603342095 and b is -28.307614128415025\n",
      "Iteration 1957, the loss is 4.521699400873662, parameters k is 8.112791966978458 and b is -28.307811756873523\n",
      "Iteration 1958, the loss is 4.52169687872518, parameters k is 8.112806966978457 and b is -28.30796985964032\n",
      "Iteration 1959, the loss is 4.521694457966264, parameters k is 8.112821966978457 and b is -28.308127962407116\n",
      "Iteration 1960, the loss is 4.521695832124983, parameters k is 8.113108864211659 and b is -28.308246539482216\n",
      "Iteration 1961, the loss is 4.521689269888416, parameters k is 8.112880227848022 and b is -28.308444167940713\n",
      "Iteration 1962, the loss is 4.521686747739925, parameters k is 8.112895227848021 and b is -28.30860227070751\n",
      "Iteration 1963, the loss is 4.521684426840876, parameters k is 8.11291022784802 and b is -28.308760373474307\n",
      "Iteration 1964, the loss is 4.521685351846976, parameters k is 8.113197125081223 and b is -28.308878950549406\n",
      "Iteration 1965, the loss is 4.521679138903165, parameters k is 8.112968488717586 and b is -28.309076579007904\n",
      "Iteration 1966, the loss is 4.521676700797708, parameters k is 8.112983488717585 and b is -28.3092346817747\n",
      "Iteration 1967, the loss is 4.5216776531750345, parameters k is 8.113270385950788 and b is -28.3093532588498\n",
      "Iteration 1968, the loss is 4.521671530066396, parameters k is 8.11304174958715 and b is -28.309550887308298\n",
      "Iteration 1969, the loss is 4.521669007917912, parameters k is 8.11305674958715 and b is -28.309708990075094\n",
      "Iteration 1970, the loss is 4.521666669672318, parameters k is 8.11307174958715 and b is -28.30986709284189\n",
      "Iteration 1971, the loss is 4.521667172897026, parameters k is 8.113358646820352 and b is -28.30998566991699\n",
      "Iteration 1972, the loss is 4.521661399081146, parameters k is 8.113130010456715 and b is -28.310183298375488\n",
      "Iteration 1973, the loss is 4.5216589436291414, parameters k is 8.113145010456714 and b is -28.310341401142285\n",
      "Iteration 1974, the loss is 4.521659474225085, parameters k is 8.113431907689916 and b is -28.310459978217384\n",
      "Iteration 1975, the loss is 4.521653790244382, parameters k is 8.11320327132628 and b is -28.310657606675882\n",
      "Iteration 1976, the loss is 4.521651268095893, parameters k is 8.113218271326279 and b is -28.31081570944268\n",
      "Iteration 1977, the loss is 4.5216489125037524, parameters k is 8.113233271326278 and b is -28.310973812209475\n",
      "Iteration 1978, the loss is 4.52164899394707, parameters k is 8.11352016855948 and b is -28.311092389284575\n",
      "Iteration 1979, the loss is 4.521643659259131, parameters k is 8.113291532195843 and b is -28.311290017743072\n",
      "Iteration 1980, the loss is 4.521641186460581, parameters k is 8.113306532195843 and b is -28.31144812050987\n",
      "Iteration 1981, the loss is 4.5216412952751295, parameters k is 8.113593429429045 and b is -28.31156669758497\n",
      "Iteration 1982, the loss is 4.521636050422362, parameters k is 8.113364793065408 and b is -28.311764326043466\n",
      "Iteration 1983, the loss is 4.521633528273875, parameters k is 8.113379793065407 and b is -28.311922428810263\n",
      "Iteration 1984, the loss is 4.521631155335189, parameters k is 8.113394793065407 and b is -28.31208053157706\n",
      "Iteration 1985, the loss is 4.521630814997118, parameters k is 8.113681690298609 and b is -28.31219910865216\n",
      "Iteration 1986, the loss is 4.521625919437113, parameters k is 8.113453053934972 and b is -28.312396737110657\n",
      "Iteration 1987, the loss is 4.521623429292016, parameters k is 8.113468053934971 and b is -28.312554839877453\n",
      "Iteration 1988, the loss is 4.521623116325174, parameters k is 8.113754951168174 and b is -28.312673416952553\n",
      "Iteration 1989, the loss is 4.521618310600343, parameters k is 8.113526314804536 and b is -28.31287104541105\n",
      "Iteration 1990, the loss is 4.521615788451855, parameters k is 8.113541314804536 and b is -28.313029148177847\n",
      "Iteration 1991, the loss is 4.521613398166622, parameters k is 8.113556314804535 and b is -28.313187250944644\n",
      "Iteration 1992, the loss is 4.521612636047175, parameters k is 8.113843212037738 and b is -28.313305828019743\n",
      "Iteration 1993, the loss is 4.521608179615095, parameters k is 8.1136145756741 and b is -28.31350345647824\n",
      "Iteration 1994, the loss is 4.5216056721234565, parameters k is 8.1136295756741 and b is -28.313661559245038\n",
      "Iteration 1995, the loss is 4.521604937375219, parameters k is 8.113916472907302 and b is -28.313780136320137\n",
      "Iteration 1996, the loss is 4.521600570778331, parameters k is 8.113687836543665 and b is -28.313977764778635\n",
      "Iteration 1997, the loss is 4.521598048629838, parameters k is 8.113702836543665 and b is -28.31413586754543\n",
      "Iteration 1998, the loss is 4.52159564099807, parameters k is 8.113717836543664 and b is -28.314293970312228\n",
      "Iteration 1999, the loss is 4.521594457097214, parameters k is 8.114004733776866 and b is -28.314412547387327\n"
     ]
    }
   ],
   "source": [
    "#initialized parameters\n",
    "\n",
    "k = random.random() * 200 - 100  # -100 100\n",
    "b = random.random() * 200 - 100  # -100 100\n",
    "\n",
    "learning_rate = 1e-2\n",
    "\n",
    "iteration_num = 2000\n",
    "losses = []\n",
    "for i in range(iteration_num):\n",
    "    \n",
    "    price_use_current_parameters = [price(r, k, b) for r in X_rm]  # \\hat{y}\n",
    "    \n",
    "    current_loss = loss(y, price_use_current_parameters)\n",
    "    losses.append(current_loss)\n",
    "    print(\"Iteration {}, the loss is {}, parameters k is {} and b is {}\".format(i,current_loss,k,b))\n",
    "    \n",
    "    k_gradient = partial_derivative_k(X_rm, y, price_use_current_parameters)\n",
    "    b_gradient = partial_derivative_b(y, price_use_current_parameters)\n",
    "    \n",
    "    k = k + (-1 * k_gradient) * learning_rate\n",
    "    b = b + (-1 * b_gradient) * learning_rate\n",
    "best_k = k\n",
    "best_b = b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7f8fc08e50d0>]"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(list(range(iteration_num)), losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.collections.PathCollection at 0x7f8fb83cadd0>"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "price_use_best_parameters = [price(r, best_k, best_b) for r in X_rm]\n",
    "\n",
    "plt.scatter(X_rm,y)\n",
    "plt.scatter(X_rm,price_use_current_parameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.collections.PathCollection at 0x7f8fe191db10>"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "price_use_best_parameters = [price(r, best_k, best_b) for r in X_rm]\n",
    "\n",
    "plt.scatter(X_rm, y)\n",
    "plt.scatter(X_rm, price_use_current_parameters)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<评阅点>\n",
    "+ 是否将Loss改成了“绝对值”(3')\n",
    "+ 是否完成了偏导的重新定义(5')\n",
    "+ 新的模型Loss是否能够收敛 (11’)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
