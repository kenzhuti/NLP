{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. 复习上课内容以及复现课程代码"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在本部分，你需要复习上课内容和课程代码后，自己复现课程代码。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "class Node:\n",
    "    # 该类为所有其他图节点的父类\n",
    "    def __init__(self, inputs=[]):\n",
    "        # 定义每个节点的输入和输出\n",
    "        self.inputs = inputs\n",
    "        self.outputs = []\n",
    "        \n",
    "        # 每个节点都是其输入节点的输出节点\n",
    "        for n in self.inputs:\n",
    "            n.outputs.append(self)\n",
    "        \n",
    "        self.value = None\n",
    "        \n",
    "        self.gradients = {}\n",
    "        \n",
    "    def forward(self):\n",
    "        # 前向传播函数 继承该类的其他类会复写该函数\n",
    "        raise NotImplemented\n",
    "        \n",
    "    def backward(self):\n",
    "        raise NotImplemented\n",
    "\n",
    "class Input(Node):\n",
    "    # 输入节点，包括神经网络输入节点，权重节点，和偏差节点\n",
    "    def __init__(self):\n",
    "        Node.__init__(self)\n",
    "    \n",
    "    def forward(self, value=None):\n",
    "        # 定义节点数值\n",
    "        if value is not None:\n",
    "            self.value = value\n",
    "    \n",
    "    def backward(self):\n",
    "        # 计算节点梯度\n",
    "        self.gradients = {self:0}\n",
    "        for n in self.outputs:\n",
    "            # 以下计算该节点的输出节点对该节点的梯度\n",
    "            grad_cost = n.gradients[self]\n",
    "            self.gradients[self] = grad_cost * 1\n",
    "            \n",
    "class Add(Node):\n",
    "    def __init__(self, *nodes):\n",
    "        Node.__init__(self, nodes)\n",
    "    \n",
    "    def forward(self):\n",
    "        self.value = sum(map(lambda n: n.value, self.inputs))\n",
    "        \n",
    "class Linear(Node):\n",
    "    # 全连接网络层的计算\n",
    "    def __init__(self, nodes, weights, bias):\n",
    "        Node.__init__(self, [nodes, weights, bias])\n",
    "    \n",
    "    def forward(self):\n",
    "        # 前向传播计算 y = w*x + b\n",
    "        inputs = self.inputs[0].value\n",
    "        weights = self.inputs[1].value\n",
    "        bias = self.inputs[2].value\n",
    "        \n",
    "        self.value = np.dot(inputs, weights) + bias\n",
    "        \n",
    "    def backward(self):\n",
    "        # 反向传播计算\n",
    "        self.gradients = {n: np.zeros_like(n.value) for n in self.inputs}\n",
    "        for n in self.outputs:\n",
    "            # Get the partial of the cost w.r.t this node\n",
    "            grad_cost = n.gradients[self]\n",
    "            # 以下分别计算对inputs，weights，bias的梯度\n",
    "            self.gradients[self.inputs[0]] = np.dot(grad_cost, self.inputs[1].value.T)\n",
    "            self.gradients[self.inputs[1]] = np.dot(self.inputs[0].value.T, grad_cost)\n",
    "            self.gradients[self.inputs[2]] = np.sum(grad_cost, axis=0, keepdims=False)\n",
    "            \n",
    "class Sigmoid(Node):\n",
    "    # 定义sigmoid函数\n",
    "    def __init__(self, node):\n",
    "        Node.__init__(self, [node])\n",
    "        \n",
    "    def _sigmoid(self, x):\n",
    "        return 1./(1 + np.exp(-1 * x))\n",
    "    \n",
    "    def forward(self):\n",
    "        self.x = self.inputs[0].value\n",
    "        self.value = self._sigmoid(self.x)\n",
    "    \n",
    "    def backward(self):\n",
    "        self.partial = self._sigmoid(self.x) * (1 - self._sigmoid(self.x))\n",
    "        self.gradients = {n: np.zeros_like(n.value) for n in self.inputs}\n",
    "        \n",
    "        for n in self.outputs:\n",
    "            grad_cost = n.gradients[self]\n",
    "            self.gradients[self.inputs[0]] = grad_cost * self.partial\n",
    "\n",
    "class MSE(Node):\n",
    "    \n",
    "    # 定义平均平方误差\n",
    "    def __init__(self, y, a):\n",
    "        Node.__init__(self, [y, a])\n",
    "        \n",
    "    def forward(self):\n",
    "        # 前向传播计算\n",
    "        y = self.inputs[0].value.reshape(-1, 1)\n",
    "        a = self.inputs[1].value.reshape(-1, 1)\n",
    "        assert(y.shape == a.shape)\n",
    "        \n",
    "        self.m = self.inputs[0].value.shape[0]\n",
    "        self.diff = y - a\n",
    "        \n",
    "        self.value = np.mean(self.diff**2)\n",
    "        \n",
    "    def backward(self):\n",
    "        # 反向计算相应梯度\n",
    "        self.gradients[self.inputs[0]] = (2 / self.m) * self.diff\n",
    "        self.gradients[self.inputs[1]] = (-2 / self.m) * self.diff\n",
    "        \n",
    "def forward_and_backward(outputnode, graph):\n",
    "    for n in graph:\n",
    "        n.forward()\n",
    "    \n",
    "    for n in graph[::-1]:\n",
    "        n.backward()\n",
    "        \n",
    "def topological_sort(feed_dict):\n",
    "    input_nodes = [n for n in feed_dict.keys()]\n",
    "    \n",
    "    G = {}\n",
    "    nodes = [n for n in input_nodes]\n",
    "    while len(nodes) > 0:\n",
    "        n = nodes.pop(0)\n",
    "        if n not in G:\n",
    "            G[n] = {'in': set(), 'out': set()}\n",
    "        for m in n.outputs:\n",
    "            if m not in G:\n",
    "                G[m] = {'in': set(), 'out': set()}\n",
    "            G[n]['out'].add(m)\n",
    "            G[m]['in'].add(n)\n",
    "            nodes.append(m)\n",
    "            \n",
    "    L = []\n",
    "    S = set(input_nodes)\n",
    "    while len(S) > 0:\n",
    "        n = S.pop()\n",
    "\n",
    "        if isinstance(n, Input):\n",
    "            n.value = feed_dict[n]\n",
    "        L.append(n)\n",
    "        for m in n.outputs:\n",
    "            G[n]['out'].remove(m)\n",
    "            G[m]['in'].remove(n)\n",
    "            if len(G[m]['in']) == 0:\n",
    "                S.add(m)\n",
    "    return L\n",
    "    \n",
    "def sgd_update(trainables, learning_rate=1e-2):\n",
    "    for t in trainables:\n",
    "        t.value += -1 * learning_rate * t.gradients[t]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'numpy.ndarray'> [[[ 0  1  2  3]\n",
      "  [ 4  5  6  7]]\n",
      "\n",
      " [[ 8  9 10 11]\n",
      "  [12 13 14 15]]\n",
      "\n",
      " [[16 17 18 19]\n",
      "  [20 21 22 23]]]\n",
      "<class 'numpy.ndarray'> [[ 2.  3.  4.  5.]\n",
      " [10. 11. 12. 13.]\n",
      " [18. 19. 20. 21.]]\n"
     ]
    }
   ],
   "source": [
    "x = np.arange(24).reshape(3, 2, 4)\n",
    "y = np.mean(x, axis=1)\n",
    "print(type(x), x)\n",
    "print(type(y), y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of examples = 506\n",
      "Epoch: 1, Loss: 168.817\n",
      "Epoch: 101, Loss: 6.267\n",
      "Epoch: 201, Loss: 4.623\n",
      "Epoch: 301, Loss: 5.409\n",
      "Epoch: 401, Loss: 4.431\n",
      "Epoch: 501, Loss: 4.981\n",
      "Epoch: 601, Loss: 4.812\n",
      "Epoch: 701, Loss: 4.393\n",
      "Epoch: 801, Loss: 4.175\n",
      "Epoch: 901, Loss: 4.378\n",
      "Epoch: 1001, Loss: 4.354\n",
      "Epoch: 1101, Loss: 3.262\n",
      "Epoch: 1201, Loss: 3.862\n",
      "Epoch: 1301, Loss: 3.695\n",
      "Epoch: 1401, Loss: 4.270\n",
      "Epoch: 1501, Loss: 3.652\n",
      "Epoch: 1601, Loss: 4.037\n",
      "Epoch: 1701, Loss: 3.893\n",
      "Epoch: 1801, Loss: 3.910\n",
      "Epoch: 1901, Loss: 3.520\n",
      "Epoch: 2001, Loss: 3.803\n",
      "Epoch: 2101, Loss: 3.647\n",
      "Epoch: 2201, Loss: 3.431\n",
      "Epoch: 2301, Loss: 3.555\n",
      "Epoch: 2401, Loss: 2.925\n",
      "Epoch: 2501, Loss: 3.029\n",
      "Epoch: 2601, Loss: 3.152\n",
      "Epoch: 2701, Loss: 3.456\n",
      "Epoch: 2801, Loss: 3.197\n",
      "Epoch: 2901, Loss: 3.260\n",
      "Epoch: 3001, Loss: 3.754\n",
      "Epoch: 3101, Loss: 3.367\n",
      "Epoch: 3201, Loss: 3.101\n",
      "Epoch: 3301, Loss: 3.388\n",
      "Epoch: 3401, Loss: 3.486\n",
      "Epoch: 3501, Loss: 2.734\n",
      "Epoch: 3601, Loss: 3.464\n",
      "Epoch: 3701, Loss: 2.843\n",
      "Epoch: 3801, Loss: 3.027\n",
      "Epoch: 3901, Loss: 3.066\n",
      "Epoch: 4001, Loss: 2.967\n",
      "Epoch: 4101, Loss: 3.434\n",
      "Epoch: 4201, Loss: 2.618\n",
      "Epoch: 4301, Loss: 2.636\n",
      "Epoch: 4401, Loss: 3.421\n",
      "Epoch: 4501, Loss: 2.950\n",
      "Epoch: 4601, Loss: 2.814\n",
      "Epoch: 4701, Loss: 3.211\n",
      "Epoch: 4801, Loss: 3.074\n",
      "Epoch: 4901, Loss: 3.453\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import load_boston\n",
    "import numpy as np\n",
    "from sklearn.datasets import load_boston\n",
    "from sklearn.utils import shuffle, resample\n",
    "#from miniflow import *\n",
    "\n",
    "# Load data\n",
    "data = load_boston()\n",
    "losses = []\n",
    "\n",
    "X_ = data['data']\n",
    "y_ = data['target']\n",
    "\n",
    "# Normalize data\n",
    "X_ = (X_ - np.mean(X_, axis=0)) / np.std(X_, axis=0)\n",
    "\n",
    "n_features = X_.shape[1]\n",
    "n_hidden = 10\n",
    "W1_ = np.random.randn(n_features, n_hidden)\n",
    "b1_ = np.zeros(n_hidden)\n",
    "W2_ = np.random.randn(n_hidden, 1)\n",
    "b2_ = np.zeros(1)\n",
    "\n",
    "# Neural network\n",
    "X, y = Input(), Input()\n",
    "W1, b1 = Input(), Input()\n",
    "W2, b2 = Input(), Input()\n",
    "\n",
    "l1 = Linear(X, W1, b1)\n",
    "s1 = Sigmoid(l1)\n",
    "l2 = Linear(s1, W2, b2)\n",
    "cost = MSE(y, l2)\n",
    "\n",
    "feed_dict = {\n",
    "    X: X_,\n",
    "    y: y_,\n",
    "    W1: W1_,\n",
    "    b1: b1_,\n",
    "    W2: W2_,\n",
    "    b2: b2_\n",
    "}\n",
    "\n",
    "epochs = 5000\n",
    "\n",
    "# Total number of examples\n",
    "m = X_.shape[0]\n",
    "batch_size = 16\n",
    "steps_per_epoch = m // batch_size\n",
    "\n",
    "graph = topological_sort(feed_dict)\n",
    "trainables = [W1, b1, W2, b2]\n",
    "\n",
    "print(\"Total number of examples = {}\".format(m))\n",
    "\n",
    "# Step 4\n",
    "for i in range(epochs):\n",
    "    loss = 0\n",
    "    for j in range(steps_per_epoch):\n",
    "        X_batch, y_batch = resample(X_, y_, n_samples=batch_size)\n",
    "        # pdb.set_trace()\n",
    "        X.value = X_batch\n",
    "        y.value = y_batch\n",
    "        \n",
    "        _ = None\n",
    "        forward_and_backward(_, graph)\n",
    "        \n",
    "        rate = 1e-2\n",
    "        \n",
    "        sgd_update(trainables, rate)\n",
    "        \n",
    "        loss += graph[-1].value\n",
    "    \n",
    "    if i % 100 == 0:\n",
    "        print(\"Epoch: {}, Loss: {:.3f}\".format(i+1, loss/steps_per_epoch))\n",
    "        losses.append(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "50"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[30.55760592],\n",
       "       [17.55160851],\n",
       "       [49.50239142],\n",
       "       [21.32092866],\n",
       "       [20.92264739],\n",
       "       [19.14517708],\n",
       "       [20.36236633],\n",
       "       [15.38620835],\n",
       "       [24.20614233],\n",
       "       [17.34886253],\n",
       "       [14.65083553],\n",
       "       [20.51381618],\n",
       "       [13.51129777],\n",
       "       [22.09757797],\n",
       "       [49.64593432],\n",
       "       [20.35026791]])"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def forward(outputNode, graph):\n",
    "    for n in graph:\n",
    "        n.forward()\n",
    "    return outputNode.value\n",
    "\n",
    "forward(l2, graph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7fdb98876290>]"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(range(len(losses)), losses)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. 回答一下理论题目"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. What does a neuron compute?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "就像人类神经细胞一样，神经元可以看作一个计算与存储单元。计算是神经元对其输入进行计算功能；存储是神经元会暂存结果，并传递给下一层\n",
    "formula:\n",
    "$$ z = W^Tx + b $$\n",
    "$$ a = \\sigma (z) $$\n",
    "Activation function like sigmoid, tanh, Relu, Leaky Relu, etc."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  2. Why we use non-linear activation funcitons in neural networks?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "因为如果食用线性激活函数，多个隐藏层其实就可以用一个隐藏层代替, 深度没有了意义"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. What is the 'Logistic Loss' ?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "交叉熵在二分类的特殊情况\n",
    "fomula:\n",
    "$$ Loss = -ylog\\hat y - (1-y)log(1 - \\hat y) $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4. Assume that you are building a binary classifier for detecting if an image containing cats, which activation functions would you recommen using for the output layer ?\n",
    "\n",
    "A. ReLU    \n",
    "B. Leaky ReLU    \n",
    "C. sigmoid    \n",
    "D. tanh  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "C\n",
    "sigmoid函数取值范围（0，1），适合二分类场景\n",
    "\n",
    "tanh 效果非常好，几乎适用于所有场合\n",
    "\n",
    "ReLu 最常用的默认函数，学习速度快\n",
    "\n",
    "Leaky ReLu 优化版的ReLu\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5. Why we don't use zero initialization for all parameters ?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "导致神经网络的对称性问题，前向传播值是一样的，反向传播梯度是一样的，无论迭代多少次，loss都不会得到改善, 使神经网络失去意义。解决这个问题的方法就是随机初始化参数"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 6. Can you implement the softmax function using python ? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$ softmax = \\frac{e^{z_j}}{{\\sum _j^n e^{z_j}}} $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 514,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "def softmax(z):\n",
    "    return np.exp(z) / np.sum(np.exp(z), axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[10 20]\n"
     ]
    }
   ],
   "source": [
    "a = [[1,2,3,4], [5,5,5,5]]\n",
    "\n",
    "# print(np.exp(a))\n",
    "\n",
    "print(np.sum(a, axis=1))\n",
    "\n",
    "# np.sum(np.exp(a), axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[3.05799702e-07, 3.05799702e-07, 3.05799702e-07, 3.05799702e-07],\n",
       "       [3.05799702e-07, 1.12497423e-07, 4.13854892e-08, 1.52248707e-08]])"
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "softmax(a)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.实践题"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### In this practical part, you will build a simple digits recognizer to check if the digit in the image is larger than 5. This assignmnet will guide you step by step to finish your first small project in this course ."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1 - Packages  \n",
    "sklearn is a famous package for machine learning.   \n",
    "matplotlib is a common package for vasualization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 396,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import datasets\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2 - Overvie of the dataset  \n",
    "    - a training set has m_train images labeled as 0 if the digit < 5 or 1 if the digit >= 5\n",
    "    - a test set contains m_test images labels as if the digit < 5 or 1 if the digit >= 5\n",
    "    - eah image if of shape (num_px, num_px ). Thus, each image is square(height=num_px and  width = num_px)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading the data \n",
    "import numpy as np\n",
    "digits = datasets.load_digits()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "digits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 10 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Vilizating the data\n",
    "for i in range(1,11):\n",
    "    plt.subplot(2,5,i)\n",
    "    plt.imshow(digits.data[i-1].reshape([8,8]),cmap=plt.cm.gray_r)\n",
    "    plt.text(3,10,str(digits.target[i-1]))\n",
    "    plt.xticks([])\n",
    "    plt.yticks([])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 293,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data into training set and test set \n",
    "X_train, X_test, y_train, y_test = train_test_split(digits.data, digits.target, test_size=0.25)\n",
    "\n",
    "# reformulate the label. \n",
    "# If the digit is smaller than 5, the label is 0.\n",
    "# If the digit is larger than 5, the label is 1.\n",
    "y_train[y_train < 5 ] = 0\n",
    "y_train[y_train >= 5] = 1\n",
    "y_test[y_test < 5] = 0\n",
    "y_test[y_test >= 5] = 1\n",
    "\n",
    "# 标准化处理\n",
    "X_train /= X_train.max()\n",
    "X_test /= X_train.max()\n",
    "\n",
    "# 数据格式处理，为了匹配公示\n",
    "X_train = X_train.T\n",
    "X_test = X_test.T\n",
    "y_train = np.expand_dims(y_train, axis=0)\n",
    "y_test = np.expand_dims(y_test, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.,  0.,  9., ...,  0.,  0.,  0.],\n",
       "       [ 0.,  2.,  9., ...,  0.,  0.,  0.],\n",
       "       [ 0.,  0., 10., ...,  0.,  0.,  0.],\n",
       "       ...,\n",
       "       [ 0.,  0.,  5., ...,  2.,  0.,  0.],\n",
       "       [ 0.,  0.,  3., ..., 16.,  9.,  0.],\n",
       "       [ 0.,  0.,  0., ..., 12.,  8.,  0.]])"
      ]
     },
     "execution_count": 191,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([8, 8, 5, ..., 0, 8, 6])"
      ]
     },
     "execution_count": 137,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'numpy.ndarray'>\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(4, 3)"
      ]
     },
     "execution_count": 248,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "r = [[1, 2, 3], [4, 5, 6], [7, 8, 9], [10, 11, 12]]\n",
    "art = np.array(r)\n",
    "print(type(ar))\n",
    "art.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6"
      ]
     },
     "execution_count": 235,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "art[1, 2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 294,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(64, 1347)\n",
      "(64, 450)\n",
      "(1, 1347)\n",
      "(1, 450)\n"
     ]
    }
   ],
   "source": [
    "print(X_train.shape)\n",
    "print(X_test.shape)\n",
    "print(y_train.shape)\n",
    "print(y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3- Architecture of the neural network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![title](./networks.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/qinliu/AI/00code/04fourth_class\r\n"
     ]
    }
   ],
   "source": [
    "!pwd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Mathematical expression of the algorithm:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For one example $x^{(i)}$:   \n",
    " $$ z^{(i)} = w^T * x^{(i)} +b $$   \n",
    " $$ y^{(i)} = a^{(i)} = sigmoid(z^{(i)})$$   \n",
    " $$L(a^{(i)},y^{(i)}) = -y^{(i)} log(a^{(i)})-(1-y^{(i)})log(1-a^{(i)})$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The total cost over all training examples:\n",
    "$$ J = \\frac{1}{m}\\sum_{i=1}^{m}L(a^{(i)},y^{(i)}) $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4 - Building the algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 4.1- Activation function    \n",
    "###### Exercise:\n",
    "Finish the sigmoid funciton "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 322,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(z):\n",
    "    '''\n",
    "    Compute the sigmoid of z\n",
    "    Arguments: z -- a scalar or numpy array of any size.\n",
    "    \n",
    "    Return:\n",
    "    s -- sigmoid(z)\n",
    "    '''\n",
    "    s = 1./(1 + np.exp(-1 * z))\n",
    "    return s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 2]\n",
      "<class 'numpy.ndarray'>\n",
      "[0.5        0.88079708]\n",
      "sigmoid([0,2]) = [0.5        0.88079708]\n"
     ]
    }
   ],
   "source": [
    "# Test your code \n",
    "# The result should be [0.5 0.88079708]\n",
    "print(\"sigmoid([0,2]) = \" + str(sigmoid(np.array([0,2]))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 4.1-Initializaing parameters\n",
    "###### Exercise:\n",
    "Finishe the initialize_parameters function below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 323,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Random innitialize the parameters\n",
    "\n",
    "def initialize_parameters(dim):\n",
    "    '''\n",
    "    Argument: dim -- size of the w vector\n",
    "    \n",
    "    Returns:\n",
    "    w -- initialized vector of shape (dim,1)\n",
    "    b -- initializaed scalar\n",
    "    '''\n",
    "    \n",
    "    w = np.random.rand(dim, 1)\n",
    "    b = 0\n",
    "    \n",
    "    assert(w.shape == (dim,1))\n",
    "    assert(isinstance(b,float) or isinstance(b,int))\n",
    "    \n",
    "    return w,b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1347"
      ]
     },
     "execution_count": 212,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 347,
   "metadata": {},
   "outputs": [],
   "source": [
    "w1, b1 = initialize_parameters(X_train.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w1, b1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1347, 1)"
      ]
     },
     "execution_count": 217,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w1.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.3-Forward and backward propagation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Some mathematical expressions\n",
    "Forward Propagation:   \n",
    ". X    \n",
    ". A = $\\sigma(w^T*X+b) = (a^{(1)},a^{(2)},...,a^{(m)}$   \n",
    ". J = $-\\frac{1}{m} \\sum_{i=1}^{m}y^{(i)}log(a^{(i)}+(1-y^{(i)})log(1-a^{(i)})$       "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some derivative: \n",
    "$$\\frac{\\partial{J}}{\\partial{w}} = \\frac{1}{m}X*(A-Y)^T$$   \n",
    "$$\\frac{\\partial{J}}{\\partial{b}} = \\frac{1}{m}\\sum_{i=1}^m(a^{(i)}-y^{(i)}) $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Exercise:\n",
    "Finish the function below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pprint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 383,
   "metadata": {},
   "outputs": [],
   "source": [
    "def propagate(w,b,X,Y):\n",
    "    '''\n",
    "    Implement the cost function and its gradient for the propagation\n",
    "    \n",
    "    Arguments:\n",
    "    w - weights\n",
    "    b - bias\n",
    "    X - data\n",
    "    Y - ground truth\n",
    "    '''\n",
    "    m = X.shape[1]\n",
    "    z = np.dot(w.T, X) + b \n",
    "    A = sigmoid(z)\n",
    "    temp0 = Y * np.log(A)\n",
    "    temp1 = (1 - Y) * np.log(1 - A)\n",
    "    \n",
    "    cost = (-1 / m) * np.sum(Y * np.log(A) + (1 - Y) * np.log(1 - A))\n",
    "    \n",
    "    dw = np.dot(X, (A-Y).T) / m\n",
    "    # print('dw', dw)\n",
    "    db = np.sum(A-Y) / m\n",
    "    \n",
    "    assert(dw.shape == w.shape)\n",
    "    assert(db.dtype == float)\n",
    "    cost = np.squeeze(cost)\n",
    "    assert(cost.shape == ())\n",
    "    \n",
    "    grads = {'dw':dw,\n",
    "             'db':db}\n",
    "    return grads, cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 384,
   "metadata": {},
   "outputs": [],
   "source": [
    "grads1, cost1 = propagate(w1, b1, X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 386,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4.800631605060889"
      ]
     },
     "execution_count": 386,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cost1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 4.4 -Optimization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Exercise:\n",
    "Minimizing the cost function using gradient descent.   \n",
    "$$\\theta = \\theta - \\alpha*d\\theta$$ where $\\alpha$ is the learning rate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 412,
   "metadata": {},
   "outputs": [],
   "source": [
    "def optimize(w, b, X, Y, num_iterations, learning_rate, print_cost=False):\n",
    "    '''\n",
    "    This function optimize w and b by running a gradient descen algorithm\n",
    "    \n",
    "    Arguments:\n",
    "    w - weights\n",
    "    b - bias\n",
    "    X - data\n",
    "    Y - ground truth\n",
    "    num_iterations -- number of iterations of the optimization loop\n",
    "    learning_rate -- learning rate of the gradient descent update rule\n",
    "    print_cost -- True to print the loss every 100 steps\n",
    "    \n",
    "    Returns:\n",
    "    params - dictionary containing the weights w and bias b\n",
    "    grads -- dictionary containing the gradients of the weights and bias with respect to the cost function\n",
    "    costs -- list of all the costs computed during the optimization, this will be used to plot the learning curve.\n",
    "    \n",
    "    '''\n",
    "    costs = []\n",
    "    # pdb.set_trace()\n",
    "    for i in range(num_iterations):\n",
    "        \n",
    "        grads, cost = propagate(w, b, X, Y)\n",
    "        \n",
    "        dw = grads['dw']\n",
    "        db = grads['db']\n",
    "        \n",
    "        w = w - learning_rate * dw\n",
    "        b = b - learning_rate * db\n",
    "        if i % 100 == 0:\n",
    "            costs.append(cost)\n",
    "        if print_cost and i % 100 == 0:\n",
    "            print (\"Cost after iteration %i: %f\" %(i, cost))\n",
    "    \n",
    "    params = {\"w\":w,\n",
    "              \"b\":b}\n",
    "    \n",
    "    grads = {\"dw\":dw,\n",
    "             \"db\":db}\n",
    "    \n",
    "    return params, grads, costs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 358,
   "metadata": {},
   "outputs": [],
   "source": [
    "params1, grads1, cost1 = optimize(w1, b1, X_train, y_train, 4000, 1e-1, print_cost=False)\n",
    "last_w1 = params1[\"w\"]\n",
    "last_b1 = params1[\"b\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 379,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "()"
      ]
     },
     "execution_count": 379,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eer = cost1[0]\n",
    "eer.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Exercise\n",
    "The previous function will output the learned w and b. We are able to use w and b to predict the labels for a dataset X. Implement the predict() function.    \n",
    "Two steps to finish this task:   \n",
    "1. Calculate $\\hat{Y} = A = \\sigma(w^T*X+b)$   \n",
    "2. Convert the entries of a into 0 (if activation <= 0.5) or 1 (if activation > 0.5), stores the predictions in a vector Y_prediction. If you wish, you can use an if/else statement in a for loop (though there is also a way to vectorize this)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 361,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(w, b, X):\n",
    "    '''\n",
    "    Predict whether the label is 0 or 1 using learned logistic regression parameters (w, b)\n",
    "    \n",
    "    Arguments:\n",
    "    w -- weights\n",
    "    b -- bias \n",
    "    X -- data \n",
    "    \n",
    "    Returns:\n",
    "    Y_prediction -- a numpy array (vector) containing all predictions (0/1) for the examples in X\n",
    "    '''\n",
    "    m = X.shape[1]\n",
    "    Y_prediction = np.zeros((1,m))\n",
    "    w = w.reshape(X.shape[0],1)\n",
    "    \n",
    "    A = sigmoid(np.dot(w.T, X) + b)\n",
    "    \n",
    "    for i in range(A.shape[1]):\n",
    "        #if A[0,i] <= 0.5:\n",
    "        #print(A[0, i])\n",
    "        Y_prediction[0,i] = 0 if A[0,i] <= 0.5 else 1\n",
    "    \n",
    "    assert(Y_prediction.shape == (1,m))\n",
    "    \n",
    "    return Y_prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 359,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.033377013736795046\n",
      "0.9491875930117785\n",
      "0.992545713471496\n",
      "0.01625384432929697\n",
      "0.8508685658435534\n",
      "0.10131209158175726\n",
      "0.37469389483600046\n",
      "0.008664759761203901\n",
      "0.009803255578506255\n",
      "0.8530726068725002\n",
      "0.9962038548084343\n",
      "0.671151064185281\n",
      "0.3030315408729465\n",
      "0.2120607321274038\n",
      "0.91803442509936\n",
      "0.07866398706457638\n",
      "0.9066942424716905\n",
      "0.06359756922534039\n",
      "0.919781621980052\n",
      "0.11939391064489015\n",
      "0.9924936520140305\n",
      "0.24394947450176874\n",
      "0.7118329703142586\n",
      "0.6451688169030092\n",
      "0.06806482734249318\n",
      "0.00070967699284552\n",
      "0.9984801070689057\n",
      "0.7891486382567015\n",
      "0.24200158037468167\n",
      "0.7169173560462232\n",
      "0.9831886852148027\n",
      "0.11268418134923626\n",
      "0.987597984859463\n",
      "0.181147201143048\n",
      "0.961932012304858\n",
      "0.6930336299405069\n",
      "0.3797862909186739\n",
      "0.017529054717203025\n",
      "0.9134349261122321\n",
      "0.8703076676782896\n",
      "0.02751592873130924\n",
      "0.3483926822653955\n",
      "0.9740740436865057\n",
      "0.9089571168561232\n",
      "0.9969789837624481\n",
      "0.820918745457038\n",
      "0.0034784570803834897\n",
      "0.2709783560524852\n",
      "0.014604692603003662\n",
      "0.7657115097036954\n",
      "0.9777012562507\n",
      "0.7155463834831778\n",
      "0.08807708981787625\n",
      "0.007769390625078629\n",
      "0.9646551136514661\n",
      "0.7497099042733142\n",
      "0.46896510645103184\n",
      "0.5323532572234532\n",
      "0.2142755980350919\n",
      "0.9126377889194093\n",
      "0.41621569953677345\n",
      "0.990463881997168\n",
      "0.6756652480084917\n",
      "0.5163247347983839\n",
      "0.1398069753828578\n",
      "0.7900091163017692\n",
      "0.025050249427798175\n",
      "0.10192611529129682\n",
      "0.15301643473515578\n",
      "0.9746405680337399\n",
      "0.4060333994988029\n",
      "0.9704994613562363\n",
      "0.04500998054459034\n",
      "0.011200198307455207\n",
      "0.007681050741049648\n",
      "0.014737859702979654\n",
      "0.9478765405385484\n",
      "0.9116542151391088\n",
      "0.6253338700808504\n",
      "0.7026130844053006\n",
      "0.0013268946309415634\n",
      "0.9383214817561917\n",
      "0.5487056035364761\n",
      "0.9240031502054382\n",
      "0.13463380555622737\n",
      "0.9892146947156216\n",
      "0.00390638132144041\n",
      "0.20163296268575068\n",
      "0.7665865914643625\n",
      "0.9977827237251609\n",
      "0.9936488008128177\n",
      "0.11091042709818986\n",
      "0.34887918015802927\n",
      "0.8490683771457082\n",
      "0.003592678043374855\n",
      "0.6782760559341373\n",
      "0.04461511718559198\n",
      "0.29344543161755754\n",
      "0.07748916608013927\n",
      "0.6421498524650221\n",
      "0.9855124190805981\n",
      "0.7642055552283541\n",
      "0.1959644764304923\n",
      "0.6774603201916398\n",
      "0.6398941663136298\n",
      "0.9201041438849391\n",
      "0.6035189395484971\n",
      "0.9957792691486599\n",
      "0.938408056801836\n",
      "0.02979764014615511\n",
      "0.11022230640878533\n",
      "0.9878273129570415\n",
      "0.9322907878524268\n",
      "0.6915854676295564\n",
      "0.9904946485798148\n",
      "0.25567245864448795\n",
      "0.8483307868716806\n",
      "0.1671349500950469\n",
      "0.7213659788815848\n",
      "0.0291114833463436\n",
      "0.2387909119527154\n",
      "0.029004762781569687\n",
      "0.9182863546240431\n",
      "0.03402469348335439\n",
      "0.5214554592907207\n",
      "0.17742094510388326\n",
      "0.9624320332574763\n",
      "0.8414278138946554\n",
      "0.06486643354338581\n",
      "0.7624252325418455\n",
      "0.8577546792001167\n",
      "0.8444487116536317\n",
      "0.14494303461274363\n",
      "0.0032151176605213177\n",
      "0.01550973642595232\n",
      "0.9880006015374642\n",
      "0.9441606762985353\n",
      "0.7640864494549481\n",
      "0.013999031491118575\n",
      "0.721250150001995\n",
      "0.9983728223719666\n",
      "0.24045031100845526\n",
      "0.5316633438138145\n",
      "0.055155502385673814\n",
      "0.9610284271102724\n",
      "0.0426576827570299\n",
      "0.0043054116411983066\n",
      "0.03012734262795547\n",
      "0.021352894929235804\n",
      "0.8179554715098225\n",
      "0.23776801339603867\n",
      "0.9950310163181593\n",
      "0.1157764101179315\n",
      "0.027091042846130342\n",
      "0.08363743650461096\n",
      "0.7171267571804238\n",
      "0.46036465328538617\n",
      "0.8640322124125531\n",
      "0.9327779204870285\n",
      "0.09521285246367212\n",
      "0.3895170855438536\n",
      "0.009848137810407458\n",
      "0.9886169060381218\n",
      "0.6597383999179449\n",
      "0.12190642756157748\n",
      "0.433213506170034\n",
      "0.9684320169818829\n",
      "0.9321968581598654\n",
      "0.43251441601530666\n",
      "0.9758973911705322\n",
      "0.37794005803391095\n",
      "0.0015722968668055557\n",
      "0.9855860129547899\n",
      "0.9021408213808678\n",
      "0.7101557508081413\n",
      "0.9397585810822674\n",
      "0.021470431744448418\n",
      "0.9743805523156787\n",
      "0.047958177692025906\n",
      "0.14707571777670295\n",
      "0.5973251684866628\n",
      "0.8445029180486964\n",
      "0.9858775973965216\n",
      "0.4454945353976579\n",
      "0.004702779574278115\n",
      "0.0733834121617115\n",
      "0.39045535686173605\n",
      "0.2866964293414047\n",
      "0.36471972892751225\n",
      "0.8931694652282296\n",
      "0.0013133540028123303\n",
      "0.1132400825165022\n",
      "0.6618558597348669\n",
      "0.9778550750859952\n",
      "0.978515020118257\n",
      "0.012511509038316056\n",
      "0.16368733737728358\n",
      "0.02119845478906957\n",
      "0.13256409927985383\n",
      "0.01467917099184295\n",
      "0.1507511649168355\n",
      "0.8375281433919148\n",
      "0.929279339408267\n",
      "0.026242733640355755\n",
      "0.647446131598921\n",
      "0.9215678496764547\n",
      "0.14427844979468576\n",
      "0.949394117566543\n",
      "0.12872685379198837\n",
      "0.9691930485875344\n",
      "0.9974675223487621\n",
      "0.025646458181513038\n",
      "0.04256007040233129\n",
      "0.9855372981736722\n",
      "0.8387287791991823\n",
      "0.22533214035887314\n",
      "0.41144667554343556\n",
      "0.2442128744284024\n",
      "0.4689108228619865\n",
      "0.4289393847701911\n",
      "0.08145590707623733\n",
      "0.9783011944315191\n",
      "0.9669613504674353\n",
      "0.10243320388751154\n",
      "0.860087188255095\n",
      "0.22341933901922315\n",
      "0.9916958008164338\n",
      "0.757636206861785\n",
      "0.00500554741295593\n",
      "0.5179152731586002\n",
      "0.9418327395660191\n",
      "0.001747448389501311\n",
      "0.991776621869825\n",
      "0.13412047382194606\n",
      "0.7686900982143773\n",
      "0.9270548709922277\n",
      "0.9622184955206159\n",
      "0.9932559467887815\n",
      "0.15788371701452542\n",
      "0.8522814898716532\n",
      "0.9869416063812351\n",
      "0.9250119213221932\n",
      "0.9415240382585991\n",
      "0.03075480765399284\n",
      "0.387488628417863\n",
      "0.9476042905656761\n",
      "0.0016309548790343971\n",
      "0.8579033696305288\n",
      "0.051870673856898\n",
      "0.24704173208521443\n",
      "0.8178355754343551\n",
      "0.10727298671806357\n",
      "0.8591579973580995\n",
      "0.9811761563775916\n",
      "0.8861709664104241\n",
      "0.3024808309219364\n",
      "0.19769091183248802\n",
      "0.9353978560320255\n",
      "0.009659545013057871\n",
      "0.9299601745639788\n",
      "0.8083552508838115\n",
      "0.27482094028995696\n",
      "0.04056250683787407\n",
      "0.007514765172169139\n",
      "0.08099542510993149\n",
      "0.5305955142826032\n",
      "0.748132043105859\n",
      "0.0014938866268385572\n",
      "0.9957231076160946\n",
      "0.8424116895022984\n",
      "0.19633570154601576\n",
      "0.945545992749578\n",
      "0.9919030621001634\n",
      "0.38794142878183124\n",
      "0.08706478409739092\n",
      "0.0783055431892288\n",
      "0.9146489200847121\n",
      "0.4624139127465152\n",
      "0.29372953872986096\n",
      "0.010189449435823597\n",
      "0.891768795270599\n",
      "0.2170020391341868\n",
      "0.08183472806423317\n",
      "0.13694093436998223\n",
      "0.07345392791061302\n",
      "0.8489308291304101\n",
      "0.9892503462424884\n",
      "0.0885212841327253\n",
      "0.828642958593801\n",
      "0.2149397932694793\n",
      "0.815071685736594\n",
      "0.0020919267646558022\n",
      "0.07315100080363895\n",
      "0.0730821183786047\n",
      "0.9863760618953272\n",
      "0.06611347908016765\n",
      "0.5805740046222324\n",
      "0.05426788664405703\n",
      "0.07177480946035035\n",
      "0.008024880652368314\n",
      "0.25798976822723896\n",
      "0.47243565263709164\n",
      "0.9918246289836291\n",
      "0.9627508485301872\n",
      "0.02177603645978555\n",
      "0.7736795254042612\n",
      "0.06603318645338316\n",
      "0.3671104404785804\n",
      "0.8909913279313207\n",
      "0.9786089992778569\n",
      "0.9133267565947062\n",
      "0.993359300547045\n",
      "0.0041158376019057195\n",
      "0.04435118249747826\n",
      "0.04133891177703084\n",
      "0.9518272962621\n",
      "0.33792483770724596\n",
      "0.06875691382242322\n",
      "0.9715953187526596\n",
      "0.9941561073847662\n",
      "0.9850841270018096\n",
      "0.9296868872455061\n",
      "0.023209631250560678\n",
      "0.9654741851081442\n",
      "0.18651215530405518\n",
      "0.04554415960872398\n",
      "0.8986082526981337\n",
      "0.02587351708991102\n",
      "0.9812157199249393\n",
      "0.7361254773769587\n",
      "0.7609595127897402\n",
      "0.9903116634607325\n",
      "0.9135582483340449\n",
      "0.37712723018484745\n",
      "0.36913127618103075\n",
      "0.6958425006321975\n",
      "0.9181734653298073\n",
      "0.04793237342058313\n",
      "0.11016905417566114\n",
      "0.520725443499391\n",
      "0.9673807792748914\n",
      "0.9897627035320933\n",
      "0.25098901780522304\n",
      "0.001619449487715133\n",
      "0.013459246629153005\n",
      "0.05516080961548574\n",
      "0.9903801546030256\n",
      "0.14856814059071916\n",
      "0.9483477480769154\n",
      "0.012987460642939567\n",
      "0.9294476911920778\n",
      "0.08607886414255424\n",
      "0.0057197475896154535\n",
      "0.08424962479077558\n",
      "0.10477972092949905\n",
      "0.2308036291890807\n",
      "0.985139958452234\n",
      "0.9054041019278144\n",
      "0.010821549426505355\n",
      "0.10163494086013712\n",
      "0.9868387623623032\n",
      "0.20813430250456735\n",
      "0.9733292234275589\n",
      "0.028819364751519395\n",
      "0.33810288131629834\n",
      "0.10025986707105863\n",
      "0.9411917030376431\n",
      "0.9690803157693824\n",
      "0.008532796814188096\n",
      "0.04785889634890544\n",
      "0.9510433469832409\n",
      "0.2253685280220992\n",
      "0.8657200137935038\n",
      "0.9852978151685124\n",
      "0.35605708034387806\n",
      "0.9936892141877128\n",
      "0.25486144656573906\n",
      "0.9693603285080921\n",
      "0.09518708751630076\n",
      "0.14516408885369492\n",
      "0.9746997343130551\n",
      "0.0016474264278047105\n",
      "0.5819691741183232\n",
      "0.9636690257748399\n",
      "0.9686046335728828\n",
      "0.07052431466301615\n",
      "0.0020879793953952333\n",
      "0.9743967468780461\n",
      "0.190501365777314\n",
      "0.2935251184378094\n",
      "0.04039736631275975\n",
      "0.6164459516748885\n",
      "0.03432874145916291\n",
      "0.589065160640516\n",
      "0.93376945657392\n",
      "0.9912660786153763\n",
      "0.07318695803117706\n",
      "0.932806048949928\n",
      "0.9767015999048211\n",
      "0.12438147228475585\n",
      "0.0286596545221614\n",
      "0.04249490261672186\n",
      "0.18297080226184573\n",
      "0.6918889966490458\n",
      "0.047034183438929004\n",
      "0.013408807872428452\n",
      "0.840194911958816\n",
      "0.9703443451347185\n",
      "0.8701998238873091\n",
      "0.9546784375595212\n",
      "0.9245655933446052\n",
      "0.9895013457157803\n",
      "0.37682922546786246\n",
      "0.08449642762939309\n",
      "0.9338709701781268\n",
      "0.9543828670829821\n",
      "0.03853343080122694\n",
      "0.7084309943473209\n",
      "0.8540016485327594\n",
      "0.853716988386746\n",
      "0.07790116910562259\n",
      "0.9596850129454251\n",
      "0.3961515315817107\n",
      "0.913843973548972\n",
      "0.08498546729514851\n",
      "0.05091572256854866\n",
      "0.8620796053304599\n",
      "0.8016326523122894\n",
      "0.0166391771246357\n",
      "0.3920585268684586\n",
      "0.9581889441365657\n",
      "0.9961236836978746\n",
      "0.03286599859257049\n",
      "0.8297804044044346\n",
      "0.9916779274000999\n",
      "0.8936132570688914\n",
      "0.10537395956814637\n",
      "0.12343619961761698\n",
      "0.2507943960829786\n",
      "0.9911869762165045\n",
      "0.9127847579371894\n",
      "0.9775421781713225\n",
      "0.2795193376164115\n",
      "0.016179690030914257\n",
      "0.9423753547486717\n",
      "0.7743902931681718\n",
      "0.13572754756462577\n",
      "0.04965338855728617\n",
      "0.9676764890885636\n",
      "0.0010973562297217911\n",
      "0.004477016403545564\n",
      "0.26130411295982353\n",
      "0.014231942453848796\n",
      "0.93411857577884\n",
      "0.47525106613838525\n",
      "0.9262212771984871\n",
      "0.35508769121044237\n",
      "0.038560924327615875\n",
      "0.222276107946261\n",
      "0.296418171534194\n",
      "0.012662583969691991\n",
      "0.5355625258067167\n",
      "0.8863392270076558\n",
      "0.9582900304781795\n",
      "0.0016458838381117327\n",
      "0.8368765846561985\n",
      "0.8794712193986454\n",
      "0.016668268435980654\n",
      "0.0507330573788645\n",
      "0.9072384672326042\n",
      "0.07072678717505601\n",
      "0.030661894711404507\n",
      "0.9500373400693619\n",
      "0.21356878007264693\n",
      "0.026712894382430817\n",
      "0.035177735074991646\n",
      "0.15002071577799367\n",
      "0.9599116749592441\n",
      "0.09700065571807795\n",
      "0.43153048104109726\n",
      "0.5976864097574578\n",
      "0.7160359622231187\n",
      "0.8431779817707364\n",
      "0.21936090824731083\n",
      "0.07545244002759831\n",
      "0.9856632850044647\n",
      "0.22620279057852433\n",
      "0.3930758106151817\n",
      "0.18730369194423133\n",
      "0.9706060204257292\n",
      "0.7528220486313391\n",
      "0.18978100853673496\n",
      "0.5929743669013087\n",
      "0.7741501733420248\n",
      "0.33830777345956137\n",
      "0.9765700845617423\n",
      "0.8416162971046712\n",
      "0.7322789191004785\n",
      "0.9844272820168763\n",
      "0.03530096172766472\n",
      "0.9275127542300174\n",
      "0.9312269548464792\n",
      "0.7922629170281834\n",
      "0.5917065208158202\n",
      "0.7461189216351415\n",
      "0.6446427775143623\n",
      "0.7398616368406457\n",
      "0.8281920523034372\n",
      "0.36222403592497193\n",
      "0.1638881227220762\n",
      "0.01198328896326785\n",
      "0.19406033174517048\n",
      "0.2757928769275439\n",
      "0.1502869403700755\n",
      "0.008180893122761159\n",
      "0.008403056154160763\n",
      "0.048851156799461413\n",
      "0.9978509181305812\n",
      "0.037644877989540075\n",
      "0.8538819044505174\n",
      "0.44767077036831976\n",
      "0.09065397183092408\n",
      "0.02358801762232513\n",
      "0.5561741703011587\n",
      "0.9846020455876593\n",
      "0.9822739953218623\n",
      "0.9729042936458691\n",
      "0.997896286050184\n",
      "0.007959867650139217\n",
      "0.9780435957975697\n",
      "0.9756469111292263\n",
      "0.12119677731496148\n",
      "0.9080466247965636\n",
      "0.7133293772578058\n",
      "0.6486169473655964\n",
      "0.9730289356343962\n",
      "0.0014718661964439432\n",
      "0.29332089827728625\n",
      "0.0427916728887531\n",
      "0.3117290949064621\n",
      "0.18899733931619062\n",
      "0.798056176790868\n",
      "0.11678198354480956\n",
      "0.22323891961906944\n",
      "0.10983013255903264\n",
      "0.0054945581944674075\n",
      "0.9744334662239141\n",
      "0.02644791119753839\n",
      "0.6525249350225388\n",
      "0.3530679650624206\n",
      "0.9697084390934115\n",
      "0.9515169243251285\n",
      "0.9999582479532059\n",
      "0.8328469334752732\n",
      "0.20165495555558333\n",
      "0.9514649151244597\n",
      "0.008610450675810344\n",
      "0.9649151329100193\n",
      "0.07364108988178789\n",
      "0.16939297966557468\n",
      "0.09346328863137847\n",
      "0.1651586449108733\n",
      "0.9882925549482678\n",
      "0.9703632392389017\n",
      "0.03556533746667751\n",
      "0.31125965128788347\n",
      "0.14786222372254273\n",
      "0.9998110455228042\n",
      "0.18137133126653082\n",
      "0.8598290041680038\n",
      "0.6849147974716782\n",
      "0.3648994230425985\n",
      "0.8764153654104551\n",
      "0.02207651197792279\n",
      "0.6528698696941739\n",
      "0.7491359865855244\n",
      "0.6264233171508177\n",
      "0.9342260179494234\n",
      "0.6879708366556591\n",
      "0.8017709958974113\n",
      "0.12855790876995574\n",
      "0.2922342780887179\n",
      "0.5680105811274095\n",
      "0.906971444798763\n",
      "0.6441877967747979\n",
      "0.04592824639698357\n",
      "0.403702141064805\n",
      "0.6402990852127585\n",
      "0.025534183738273344\n",
      "0.3068165331794684\n",
      "0.7911747288378478\n",
      "0.02374863370770035\n",
      "0.04784857247330137\n",
      "0.011037038417597202\n",
      "0.9477707773979434\n",
      "0.8341956216191991\n",
      "0.9992778831708488\n",
      "0.9425547607267016\n",
      "0.18917401312074303\n",
      "0.9868071305544712\n",
      "0.934664750243827\n",
      "0.9936677375692193\n",
      "0.012689676582041644\n",
      "0.0035601400354759905\n",
      "0.4147620556218862\n",
      "0.3292663140406208\n",
      "0.9904857833438147\n",
      "0.847178543014481\n",
      "0.12339859801167155\n",
      "0.2030622793882827\n",
      "0.06027064412261897\n",
      "0.3384582801371264\n",
      "0.07271037526195108\n",
      "0.011790974231139487\n",
      "0.5458933831173525\n",
      "0.09044669560750272\n",
      "0.8230161494020588\n",
      "0.9890278225881954\n",
      "0.028463151819260294\n",
      "0.4155756525072229\n",
      "0.07952029069398286\n",
      "0.05019854946963993\n",
      "0.02205308923803314\n",
      "0.9970424429768897\n",
      "0.10787121234927208\n",
      "0.821638847671891\n",
      "0.9028465187477768\n",
      "0.5431488748971494\n",
      "0.7440371026224086\n",
      "0.6923477373346816\n",
      "0.855709576494193\n",
      "0.06429975822733779\n",
      "0.009952026590693558\n",
      "0.9716903335323686\n",
      "0.9898980000288207\n",
      "0.9580099288350757\n",
      "0.5789468002424535\n",
      "0.07810921534300191\n",
      "0.5247715766688872\n",
      "0.03147545359664383\n",
      "0.04120610470912933\n",
      "0.27123674082149674\n",
      "0.8624249019802408\n",
      "0.7502815856658509\n",
      "0.9429644025067078\n",
      "0.17460230420251852\n",
      "0.9707544721329241\n",
      "0.9906705226582216\n",
      "0.7608728680489959\n",
      "0.08896217473576973\n",
      "0.9631650496974006\n",
      "0.16126612300991855\n",
      "0.29554662969896917\n",
      "0.9920057465854191\n",
      "0.6213431244156352\n",
      "0.7627403146728517\n",
      "0.045925516651276116\n",
      "0.09166017114667122\n",
      "0.7039901222009494\n",
      "0.7822397344828301\n",
      "0.6527523227736338\n",
      "0.8243833709272641\n",
      "0.004907211992529744\n",
      "0.11459619442377178\n",
      "0.11254850391664317\n",
      "0.8341824745527625\n",
      "0.7454343695461014\n",
      "0.07034520363320705\n",
      "0.009971270211754818\n",
      "0.9923937132684003\n",
      "0.9284937824395214\n",
      "0.3846694594916256\n",
      "0.1279068597199433\n",
      "0.9567853781900499\n",
      "0.9144992694514634\n",
      "0.5782586141949171\n",
      "0.35611878445102646\n",
      "0.009985593325299202\n",
      "0.4467397778870521\n",
      "0.22570694697481022\n",
      "0.9005912831369932\n",
      "0.9907914938169281\n",
      "0.1082158278640353\n",
      "0.03013092583536424\n",
      "0.8245977108666853\n",
      "0.9283149087716505\n",
      "0.9852684194477338\n",
      "0.07894868995443495\n",
      "0.11586744579820817\n",
      "0.34168301658534966\n",
      "0.19084055032529285\n",
      "0.3525230031907505\n",
      "0.9539440992395544\n",
      "0.9787771481184159\n",
      "0.0012933413146207842\n",
      "0.928684460603988\n",
      "0.014321697242344233\n",
      "0.9751599375496283\n",
      "0.9814101730798598\n",
      "0.06482426011685241\n",
      "0.09577499293311041\n",
      "0.013080594383416555\n",
      "0.9990141269221231\n",
      "0.7118488726582258\n",
      "0.0735886396132482\n",
      "0.01573731411139708\n",
      "0.982360237536783\n",
      "0.03957623064241802\n",
      "0.34602232030438995\n",
      "0.900784890501128\n",
      "0.09506799196356013\n",
      "0.049449833765367664\n",
      "0.0390117746904052\n",
      "0.017249790853510007\n",
      "0.9933128247114695\n",
      "0.04626179446876012\n",
      "0.038055473591960556\n",
      "0.994239781886762\n",
      "0.9145404860742472\n",
      "0.9228697453569116\n",
      "0.048251215739415564\n",
      "0.9995127433171349\n",
      "0.9990642206617387\n",
      "0.3968263657064429\n",
      "0.7018310011438249\n",
      "0.7744645909113078\n",
      "0.9282132449829342\n",
      "0.08446766731615951\n",
      "0.016512578168633663\n",
      "0.3244459105475298\n",
      "0.21530529844029145\n",
      "0.8688081411743879\n",
      "0.9480916496890178\n",
      "0.9866560673528094\n",
      "0.45915221302190895\n",
      "0.574534886250437\n",
      "0.1632710812844959\n",
      "0.4603468490354344\n",
      "0.1005497884629884\n",
      "0.005745950924292311\n",
      "0.9385411703144965\n",
      "0.08162426744650465\n",
      "0.3208856183271142\n",
      "0.773795248411287\n",
      "0.001812232537328843\n",
      "0.9798207095383085\n",
      "0.04353442586678425\n",
      "0.2041043288922054\n",
      "0.0029478472308042407\n",
      "0.09018283185234265\n",
      "0.6198136463391524\n",
      "0.485568051834649\n",
      "0.02024100175465769\n",
      "0.9870122980353513\n",
      "0.0057010682973429126\n",
      "0.38205057505489043\n",
      "0.16465630811568918\n",
      "0.9599961737874092\n",
      "0.0030262318981423312\n",
      "0.01770408735356444\n",
      "0.9636212509144987\n",
      "0.8237641482105456\n",
      "0.7995966289816752\n",
      "0.9648211084524687\n",
      "0.8094358368580902\n",
      "0.028694173846801792\n",
      "0.5304765162627122\n",
      "0.9627239586900043\n",
      "0.9684869527149325\n",
      "0.01065560393917658\n",
      "0.9998579722155385\n",
      "0.11137447156341522\n",
      "0.5513430831154987\n",
      "0.012308814884522056\n",
      "0.15017189312035936\n",
      "0.04946980892477144\n",
      "0.7576831967867182\n",
      "0.14921554997562714\n",
      "0.00672571438035269\n",
      "0.1450679719341453\n",
      "0.9576471424712504\n",
      "0.7113425550156522\n",
      "0.9641340224923316\n",
      "0.9197039913370385\n",
      "0.06380076787274132\n",
      "0.9990832066300056\n",
      "0.9663466553906184\n",
      "0.5340312061941896\n",
      "0.4760195128254732\n",
      "0.15878728553683444\n",
      "0.9390534856582976\n",
      "0.9883258811201904\n",
      "0.045747722649827315\n",
      "0.28364177019223646\n",
      "0.992341609647125\n",
      "0.09711863615055676\n",
      "0.008110688726304426\n",
      "0.029902084263503628\n",
      "0.9915467375591909\n",
      "0.40641836344004406\n",
      "0.035507794577151156\n",
      "0.962896178708528\n",
      "0.1313360344590407\n",
      "0.17063222772690836\n",
      "0.9196214716627563\n",
      "0.16699134569557647\n",
      "0.9949682449391415\n",
      "0.9347646474699817\n",
      "0.5117021303826272\n",
      "0.9507508041670444\n",
      "0.7541733188522557\n",
      "0.926590182702955\n",
      "0.9964639679695284\n",
      "0.7177449018086411\n",
      "0.2796160276227697\n",
      "0.9923186578451673\n",
      "0.907742175587364\n",
      "0.9497508274682107\n",
      "0.6123337937824127\n",
      "0.6621858571533846\n",
      "0.08249339521587663\n",
      "0.9742087198817642\n",
      "0.23282122583217976\n",
      "0.3147655649997707\n",
      "0.389699512801164\n",
      "0.36805108659822666\n",
      "0.9220554632341597\n",
      "0.6390865014413049\n",
      "0.0150300015239726\n",
      "0.9857363668095722\n",
      "0.00799297140707883\n",
      "0.7703072304134115\n",
      "0.9758949805451614\n",
      "0.5334666389721764\n",
      "0.4134491356214578\n",
      "0.006040333657367722\n",
      "0.3369350426972109\n",
      "0.8612551038945307\n",
      "0.38591253146399396\n",
      "0.2652839917247365\n",
      "0.9802341860582608\n",
      "0.014182256995914468\n",
      "0.9650791567391894\n",
      "0.9403693494986914\n",
      "0.13391464843887024\n",
      "0.05516141347729322\n",
      "0.6357619631352335\n",
      "0.4072382648593125\n",
      "0.5611912927891743\n",
      "0.7207278471248864\n",
      "0.6838555167195162\n",
      "0.8759560149324225\n",
      "0.950806239431509\n",
      "0.4122031611871175\n",
      "0.8758358671433462\n",
      "0.9218007713584284\n",
      "0.4052494410628387\n",
      "0.019093776988559172\n",
      "0.3642845858602719\n",
      "0.04255242521792381\n",
      "0.7489192306788424\n",
      "0.9886757265774166\n",
      "0.9254889287606315\n",
      "0.000980255368061459\n",
      "0.935646534895927\n",
      "0.9662176211047255\n",
      "0.03995781914410274\n",
      "0.591389543094687\n",
      "0.012446045514077539\n",
      "0.17289645816922303\n",
      "0.7527769994726751\n",
      "0.9974351125104033\n",
      "0.7201410217981765\n",
      "0.006137923189983458\n",
      "0.8072139534899332\n",
      "0.10949836296422609\n",
      "0.10994441490650035\n",
      "0.14510547825194267\n",
      "0.7957704587551605\n",
      "0.9934634292529689\n",
      "0.9055613257832846\n",
      "0.8374033666542289\n",
      "0.16264086680481968\n",
      "0.003155980743833075\n",
      "0.00015902368941851557\n",
      "0.8666822358074393\n",
      "0.7337523514621709\n",
      "0.9486226036068887\n",
      "0.8946469101062925\n",
      "0.023332202928670946\n",
      "0.2867384032867832\n",
      "0.501800253285326\n",
      "0.007231724963782604\n",
      "0.933974901645849\n",
      "0.004051221318882766\n",
      "0.05314201973390589\n",
      "0.2944929181899719\n",
      "0.044618467826289684\n",
      "0.5515616631028044\n",
      "0.9834309380889354\n",
      "0.24252942548332787\n",
      "0.2732895856233193\n",
      "0.9620076654812414\n",
      "0.9820276451331763\n",
      "0.996179314479192\n",
      "0.21487429524010815\n",
      "0.4064544203724509\n",
      "0.9106732255903109\n",
      "0.026440222476810965\n",
      "0.24586656756444047\n",
      "0.5857679591533234\n",
      "0.004761501576022856\n",
      "0.8484374021428408\n",
      "0.9545190217583649\n",
      "0.8633798824799929\n",
      "0.06636172963610344\n",
      "0.9838890984440167\n",
      "0.919878081732571\n",
      "0.7587305953482733\n",
      "0.004328002960805821\n",
      "0.035697308393034206\n",
      "0.16771724599125537\n",
      "0.9338328414461712\n",
      "0.9473501204205735\n",
      "0.01392636594142208\n",
      "0.3958029183457806\n",
      "0.9109966773936338\n",
      "0.3286995470632077\n",
      "0.8827302121634587\n",
      "0.0340516774781014\n",
      "0.0341966175823542\n",
      "0.8943712075472592\n",
      "0.702592446585068\n",
      "0.26944466761234037\n",
      "0.9977681647048363\n",
      "0.011690922443781619\n",
      "0.017387364312630212\n",
      "0.9427477225400964\n",
      "0.00096043239662696\n",
      "0.7774059735844995\n",
      "0.9753856953485021\n",
      "0.003500584622373661\n",
      "0.9470541216282792\n",
      "0.7935364390554064\n",
      "0.1531446070461217\n",
      "0.04589057867489111\n",
      "0.07965138208925761\n",
      "0.8286379730836109\n",
      "0.6526254518965392\n",
      "0.018548749065990407\n",
      "0.9455529308506131\n",
      "0.25065604232123817\n",
      "0.8279333668974672\n",
      "0.8809877353144455\n",
      "0.06761618851861267\n",
      "0.0042592921709696556\n",
      "0.1376616899651013\n",
      "0.22162456931036786\n",
      "0.1342673164396321\n",
      "0.44496912826521035\n",
      "0.04248974456837088\n",
      "0.9982699588781999\n",
      "0.0031870770789822335\n",
      "0.8517737160282375\n",
      "0.00043348872221831846\n",
      "0.9757539493204002\n",
      "0.9308422356745537\n",
      "0.1833417512325285\n",
      "0.1585950973121268\n",
      "0.027985920481769403\n",
      "0.8091984886894906\n",
      "0.9325692461280328\n",
      "0.05100755121618378\n",
      "0.8426837406572484\n",
      "0.8061618146212322\n",
      "0.8853371889221514\n",
      "0.2838118477380967\n",
      "0.9670429585934526\n",
      "0.0642845635828994\n",
      "0.3548185289682674\n",
      "0.9647875087539881\n",
      "0.056825786349371875\n",
      "0.5879433677602052\n",
      "0.9413149247241983\n",
      "0.9593994405290907\n",
      "0.8581086727882146\n",
      "0.000987334780681776\n",
      "0.3535797169625662\n",
      "0.14023398611651916\n",
      "0.7710075059643057\n",
      "0.10284946595949075\n",
      "0.13784921959196914\n",
      "0.8921729849625639\n",
      "0.3894557011439273\n",
      "0.007365669737828108\n",
      "0.02501437176433509\n",
      "0.43479167379307726\n",
      "0.2761501715970514\n",
      "0.46090217535076267\n",
      "0.02903817749420826\n",
      "0.29550883986673115\n",
      "0.7060943297361895\n",
      "0.6454146838660585\n",
      "0.9982963609683645\n",
      "0.8549212091715863\n",
      "0.35690357017078767\n",
      "0.010869271955389009\n",
      "0.983589914372504\n",
      "0.9885753297631665\n",
      "0.9936066742892625\n",
      "0.23032323817967607\n",
      "0.9589802496839546\n",
      "0.17778148641779756\n",
      "0.26889498759050723\n",
      "0.1456692027157363\n",
      "0.000811164364160359\n",
      "0.8534740174009101\n",
      "0.11765130720423224\n",
      "0.8876186691102959\n",
      "0.030975396892663484\n",
      "0.8159209602594306\n",
      "0.027079323397528957\n",
      "0.9027041081817225\n",
      "0.012328752370917904\n",
      "0.7357906752512288\n",
      "0.0049671627980478845\n",
      "0.9736309592922755\n",
      "0.04361587288933471\n",
      "0.15166051091249236\n",
      "0.16654930196003892\n",
      "0.5236666461698365\n",
      "0.9797897306734792\n",
      "0.06930384779194312\n",
      "0.046976607142103194\n",
      "0.1365291554863381\n",
      "0.7723770911022866\n",
      "0.9468437204532171\n",
      "0.20672609700024022\n",
      "0.993645206031006\n",
      "0.11020340896522027\n",
      "0.09370733218481954\n",
      "0.04331766631236365\n",
      "0.4433667456135941\n",
      "0.0030897852522748996\n",
      "0.9990143520482113\n",
      "0.05167986970722523\n",
      "0.061445664518637165\n",
      "0.7412856221797991\n",
      "0.020480185460349735\n",
      "0.018041573718931888\n",
      "0.3152571251701284\n",
      "0.9861139650231461\n",
      "0.07203525534011489\n",
      "0.6442420046504419\n",
      "0.9213628955762516\n",
      "0.9655366119539007\n",
      "0.5999679301095802\n",
      "0.674539413564922\n",
      "0.003946898800280474\n",
      "0.9343033656595199\n",
      "0.9885950620338984\n",
      "0.3534880074032055\n",
      "0.018450806382513268\n",
      "0.8221662114021372\n",
      "0.9404485550575241\n",
      "0.8743962911809954\n",
      "0.5029652670887774\n",
      "0.4462725626029683\n",
      "0.0010672797471561694\n",
      "0.12246082523896865\n",
      "0.047909806814144314\n",
      "0.7380275315451202\n",
      "0.14440865876190484\n",
      "0.9954668766650595\n",
      "0.9384718487130681\n",
      "0.036476716864940126\n",
      "0.09744660310840728\n",
      "0.997240011578173\n",
      "0.44490006154420275\n",
      "0.5585862994003485\n",
      "0.1763278935304791\n",
      "0.1597816882459061\n",
      "0.9797794067986633\n",
      "0.009507680847038753\n",
      "0.8968013430359086\n",
      "0.07608981935346311\n",
      "0.02054832413636362\n",
      "0.09562622164097843\n",
      "0.9939648739838527\n",
      "0.8921459686385903\n",
      "0.9822966367818591\n",
      "0.6118114668518804\n",
      "0.7306207279258715\n",
      "0.008905346697793545\n",
      "0.025355776412007033\n",
      "0.6237953217280905\n",
      "0.2452733673253398\n",
      "0.2051480172297548\n",
      "0.8808373756135522\n",
      "0.045565231036576094\n",
      "0.004011987046455025\n",
      "0.025634480299859028\n",
      "0.012080372680710907\n",
      "0.1623863028646316\n",
      "0.016023484538582048\n",
      "0.001654097429342192\n",
      "0.0023554141808069143\n",
      "0.26023205620790857\n",
      "0.4510644834858579\n",
      "0.39719451813203466\n",
      "0.30945043597236377\n",
      "0.29887215586199173\n",
      "0.6207027341367174\n",
      "0.026318162189473943\n",
      "0.4251845871108086\n",
      "0.17009366316328853\n",
      "0.01234295827068888\n",
      "0.9203970596772395\n",
      "0.9629010388102738\n",
      "0.9832625597057139\n",
      "0.8261720946207537\n",
      "0.18780908401761487\n",
      "0.913889813824745\n",
      "0.9691660055151707\n",
      "0.9838201624796042\n",
      "0.9116358051380148\n",
      "0.8564562094054197\n",
      "0.9961196704244547\n",
      "0.974220513109012\n",
      "0.916692305513741\n",
      "0.0012101144158339682\n",
      "0.06390044427611231\n",
      "0.9062161427619688\n",
      "0.9641430614392129\n",
      "0.8469185020046303\n",
      "0.3714215458160256\n",
      "0.13955568457960213\n",
      "0.9091247117486914\n",
      "0.46237463676171603\n",
      "0.948429027990144\n",
      "0.7362210114176406\n",
      "0.11340545790773147\n",
      "0.8436532254725865\n",
      "0.13405138661833219\n",
      "0.9780506501861127\n",
      "0.667049072978767\n",
      "0.9820031063959898\n",
      "0.7128768642809735\n",
      "0.8809698229960483\n",
      "0.1635932926560227\n",
      "0.036601693952136895\n",
      "0.30549826580759837\n",
      "0.9698854141027738\n",
      "0.5462836336035864\n",
      "0.9150964983689887\n",
      "0.792683680200798\n",
      "0.9516304853530213\n",
      "0.9898338408616127\n",
      "0.01132064926495825\n",
      "0.01643785380317168\n",
      "0.06044457571303008\n",
      "0.22854789162914155\n",
      "0.5364859082927217\n",
      "0.7948239499019202\n",
      "0.006079702813247818\n",
      "0.5465435026096556\n",
      "0.031229634803117596\n",
      "0.9902509376369296\n",
      "0.9774950387494598\n",
      "0.6011355300901302\n",
      "0.8085936298317743\n",
      "0.12781772680360196\n",
      "0.9886279634778533\n",
      "0.0076600019649482294\n",
      "0.8435822100219277\n",
      "0.029330557269986967\n",
      "0.022869998299992828\n",
      "0.033635305494334564\n",
      "0.005356386099366003\n",
      "0.006717367106537183\n",
      "0.2427883433839479\n",
      "0.004289659990169328\n",
      "0.7865058257647237\n",
      "0.9123586677585904\n",
      "0.009773107570138932\n",
      "0.25218660008732924\n",
      "0.9825085604040107\n",
      "0.1506031198240236\n",
      "0.10015666369714467\n",
      "0.9910857844351554\n",
      "0.005354812174162302\n",
      "0.011450845235207825\n",
      "0.3592186062028418\n",
      "0.011460253786427512\n",
      "0.10666409307124379\n",
      "0.927148602236928\n",
      "0.8031850265623632\n",
      "0.41880949282596347\n",
      "0.230926798538119\n",
      "0.004115020079794226\n",
      "0.01131715228887338\n",
      "0.8736196195186071\n",
      "0.9199849234383143\n",
      "0.14017497867203313\n",
      "0.4798324125141544\n",
      "0.13461323526634242\n",
      "0.9585863381894929\n",
      "0.4270597236524865\n",
      "0.062196702071809856\n",
      "0.04012905907772482\n",
      "0.3430866732478445\n",
      "0.2597345846820639\n",
      "0.9295730400436196\n",
      "0.08274243233748615\n",
      "0.9974180870362158\n",
      "0.04293690672246997\n",
      "0.7340528271757242\n",
      "0.9868259092295922\n",
      "0.06758194719773329\n",
      "0.006298531605984428\n",
      "0.9570556309586707\n",
      "0.9768767651878696\n",
      "0.9660243382110562\n",
      "0.9945204201822183\n",
      "0.022849814016361047\n",
      "0.6274153772687361\n",
      "0.6207989593917418\n",
      "0.366573304190541\n",
      "0.007606635572704351\n",
      "0.968267625908126\n",
      "0.9873720887535211\n",
      "0.9456679218858324\n",
      "0.9820418825180202\n",
      "0.9669665391712896\n",
      "0.11772016165757528\n",
      "0.8693375824468398\n",
      "0.019458991638385742\n",
      "0.04445597454182193\n",
      "0.3080579682943034\n",
      "0.9575738467543373\n",
      "0.2801244293308361\n",
      "0.02822592840934535\n",
      "0.9919983199642263\n",
      "0.020280861712392007\n",
      "0.9220175122411379\n",
      "0.023090481683081924\n",
      "0.5871737248985527\n",
      "0.7942458437712132\n",
      "0.15629532365592302\n",
      "0.011081463828768071\n",
      "0.9863469373956842\n",
      "0.8300673541259044\n",
      "0.1026211545513709\n",
      "0.9708494073632885\n",
      "0.33687126446852345\n",
      "0.9357191094124425\n",
      "0.5267856383421492\n",
      "0.9326788717646571\n",
      "0.0164071197086883\n",
      "0.5010112606104885\n",
      "0.9927102716566286\n",
      "0.9905851708902735\n",
      "0.5319919355518447\n",
      "0.998090913674268\n",
      "0.2727319206072885\n",
      "0.24591400718102283\n",
      "0.9735518257737549\n",
      "0.0046368415281709445\n",
      "0.7783876588321269\n",
      "0.9817855501104243\n",
      "0.954327979892241\n",
      "0.06416843799651636\n",
      "0.7719692407652732\n",
      "0.9954220233376302\n",
      "0.0016553293635525569\n",
      "0.8703331028269927\n",
      "0.015790443906340317\n",
      "0.14578939637358543\n",
      "0.975432369437167\n",
      "0.9957994863090666\n",
      "0.10120608605239798\n",
      "0.23557332610966875\n",
      "0.9754600445152086\n",
      "0.010398686680184564\n",
      "0.977417480895541\n",
      "0.09970531213954635\n",
      "0.9164515228141237\n",
      "0.04069999423175851\n",
      "0.026768259808062876\n",
      "0.9289970664430984\n",
      "0.7004332249711713\n",
      "0.01788708517324239\n",
      "0.027055207118145012\n",
      "0.9683157808553922\n",
      "0.01511778534180204\n",
      "0.01999182947549729\n",
      "0.09858754322190313\n",
      "0.996631731269128\n",
      "0.040415316042543455\n",
      "0.217444497511499\n",
      "0.9919793117351831\n",
      "0.038028034676960705\n",
      "0.6889001783249225\n",
      "0.2767203836254085\n",
      "0.9546496620795107\n",
      "0.894554546487256\n",
      "0.9296172369543072\n",
      "0.04986184929886151\n",
      "0.6875229491141436\n",
      "0.020297689063898318\n",
      "0.27677884469803415\n",
      "0.002771224626574339\n",
      "0.9385247689210783\n",
      "0.1465091275763757\n",
      "0.738735808256691\n",
      "0.008924231760549007\n",
      "0.06065931429811453\n",
      "0.9762267573656063\n",
      "0.010545144580089408\n",
      "0.9351453804826534\n",
      "0.9782813513278248\n",
      "0.004364100200167646\n",
      "0.05007516579648622\n",
      "0.3570014060958255\n",
      "0.9046224913258128\n",
      "0.06367749117223796\n",
      "0.12026085342248238\n",
      "0.8993204422866057\n",
      "0.7442194764691268\n",
      "0.7988503264499035\n",
      "0.06148375187345029\n",
      "0.004000319624356328\n",
      "0.011660097517470476\n",
      "0.6629895150471395\n",
      "0.8806142042742316\n",
      "0.03541544580836509\n",
      "0.02953309904307625\n",
      "0.6427257635867945\n",
      "0.433170263879229\n",
      "0.825406056336567\n",
      "0.38220774141898745\n",
      "0.9955482683443672\n",
      "0.9758872267350157\n",
      "0.9231927810688471\n",
      "0.18520219741495209\n",
      "0.9486748476506741\n",
      "0.9483213574014266\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[0., 1., 1., ..., 0., 1., 1.]])"
      ]
     },
     "execution_count": 359,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict(last_w1, last_b1, X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 340,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 340,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sef[0, 303]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 5- Merge all functions into a model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Congratulations !! You have finished all the necessary components for constructing a model. Now, Let's take the challenge to merge all the implemented function into one model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 392,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pdb\n",
    "def model(X_train, Y_train, X_test, Y_test, num_iterations, learning_rate, print_cost):\n",
    "    \"\"\"\n",
    "    Build the logistic regression model by calling all the functions you have implemented.\n",
    "    Arguments:\n",
    "    X_train - training set\n",
    "    Y_train - training label\n",
    "    X_test - test set\n",
    "    Y_test - test label\n",
    "    num_iteration - hyperparameter representing the number of iterations to optimize the parameters\n",
    "    learning_rate -- hyperparameter representing the learning rate used in the update rule of optimize()\n",
    "    print_cost -- Set to true to print the cost every 100 iterations\n",
    "    \n",
    "    Returns:\n",
    "    d - dictionary should contain following information w,b,training_accuracy, test_accuracy,cost\n",
    "    eg: d = {\"w\":w,\n",
    "             \"b\":b,\n",
    "             \"training_accuracy\": traing_accuracy,\n",
    "             \"test_accuracy\":test_accuracy,\n",
    "             \"cost\":cost}\n",
    "    \"\"\"\n",
    "    # 初始化w，b\n",
    "    dim = X_train.shape[0]\n",
    "    w_, b_ = initialize_parameters(dim)\n",
    "    \n",
    "    # 训练模型\n",
    "    params, grads, costs = optimize(w_, b_, X_train, Y_train, num_iterations, learning_rate, print_cost)\n",
    "    \n",
    "    w = params['w']\n",
    "    b = params['b']\n",
    "    \n",
    "    # 预测\n",
    "    Y_train_pred = predict(w, b, X_train)\n",
    "    Y_test_pred = predict(w, b, X_test)\n",
    "    \n",
    "    # 计算精确度\n",
    "    training_accuracy = np.mean(Y_train_pred == Y_train)\n",
    "    test_accuracy = np.mean(Y_test_pred == Y_test)\n",
    "    \n",
    "    # 返回结果\n",
    "    d = {\"w\":w,\n",
    "         \"b\":b,\n",
    "         \"training_accuracy\": training_accuracy,\n",
    "         \"test_accuracy\":test_accuracy,\n",
    "         \"cost\":costs}\n",
    "    return d\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 297,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 1347)"
      ]
     },
     "execution_count": 297,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 401,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cost after iteration 0: 4.615184\n",
      "Cost after iteration 100: 0.555811\n",
      "Cost after iteration 200: 0.470154\n",
      "Cost after iteration 300: 0.424429\n",
      "Cost after iteration 400: 0.395410\n",
      "Cost after iteration 500: 0.375023\n",
      "Cost after iteration 600: 0.359726\n",
      "Cost after iteration 700: 0.347716\n",
      "Cost after iteration 800: 0.337970\n",
      "Cost after iteration 900: 0.329864\n",
      "Cost after iteration 1000: 0.322990\n",
      "Cost after iteration 1100: 0.317071\n",
      "Cost after iteration 1200: 0.311910\n",
      "Cost after iteration 1300: 0.307361\n",
      "Cost after iteration 1400: 0.303318\n",
      "Cost after iteration 1500: 0.299696\n",
      "Cost after iteration 1600: 0.296430\n",
      "Cost after iteration 1700: 0.293468\n",
      "Cost after iteration 1800: 0.290769\n",
      "Cost after iteration 1900: 0.288298\n",
      "Cost after iteration 2000: 0.286026\n",
      "Cost after iteration 2100: 0.283931\n",
      "Cost after iteration 2200: 0.281992\n",
      "Cost after iteration 2300: 0.280192\n",
      "Cost after iteration 2400: 0.278517\n",
      "Cost after iteration 2500: 0.276954\n",
      "Cost after iteration 2600: 0.275492\n",
      "Cost after iteration 2700: 0.274123\n",
      "Cost after iteration 2800: 0.272837\n",
      "Cost after iteration 2900: 0.271627\n",
      "Cost after iteration 3000: 0.270488\n",
      "Cost after iteration 3100: 0.269413\n",
      "Cost after iteration 3200: 0.268396\n",
      "Cost after iteration 3300: 0.267435\n",
      "Cost after iteration 3400: 0.266524\n",
      "Cost after iteration 3500: 0.265659\n",
      "Cost after iteration 3600: 0.264838\n",
      "Cost after iteration 3700: 0.264057\n",
      "Cost after iteration 3800: 0.263314\n",
      "Cost after iteration 3900: 0.262606\n",
      "Cost after iteration 4000: 0.261931\n",
      "Cost after iteration 4100: 0.261286\n",
      "Cost after iteration 4200: 0.260670\n",
      "Cost after iteration 4300: 0.260081\n",
      "Cost after iteration 4400: 0.259517\n",
      "Cost after iteration 4500: 0.258977\n",
      "Cost after iteration 4600: 0.258460\n",
      "Cost after iteration 4700: 0.257964\n",
      "Cost after iteration 4800: 0.257488\n",
      "Cost after iteration 4900: 0.257030\n",
      "Cost after iteration 5000: 0.256591\n",
      "Cost after iteration 5100: 0.256168\n",
      "Cost after iteration 5200: 0.255762\n",
      "Cost after iteration 5300: 0.255370\n",
      "Cost after iteration 5400: 0.254993\n",
      "Cost after iteration 5500: 0.254630\n",
      "Cost after iteration 5600: 0.254280\n",
      "Cost after iteration 5700: 0.253942\n",
      "Cost after iteration 5800: 0.253616\n",
      "Cost after iteration 5900: 0.253301\n",
      "Cost after iteration 6000: 0.252997\n",
      "Cost after iteration 6100: 0.252702\n",
      "Cost after iteration 6200: 0.252418\n",
      "Cost after iteration 6300: 0.252143\n",
      "Cost after iteration 6400: 0.251877\n",
      "Cost after iteration 6500: 0.251619\n",
      "Cost after iteration 6600: 0.251370\n",
      "Cost after iteration 6700: 0.251128\n",
      "Cost after iteration 6800: 0.250893\n",
      "Cost after iteration 6900: 0.250666\n",
      "Cost after iteration 7000: 0.250446\n",
      "Cost after iteration 7100: 0.250232\n",
      "Cost after iteration 7200: 0.250024\n",
      "Cost after iteration 7300: 0.249823\n",
      "Cost after iteration 7400: 0.249627\n",
      "Cost after iteration 7500: 0.249437\n",
      "Cost after iteration 7600: 0.249252\n",
      "Cost after iteration 7700: 0.249073\n",
      "Cost after iteration 7800: 0.248898\n",
      "Cost after iteration 7900: 0.248728\n",
      "Cost after iteration 8000: 0.248563\n",
      "Cost after iteration 8100: 0.248402\n",
      "Cost after iteration 8200: 0.248245\n",
      "Cost after iteration 8300: 0.248092\n",
      "Cost after iteration 8400: 0.247944\n",
      "Cost after iteration 8500: 0.247799\n",
      "Cost after iteration 8600: 0.247658\n",
      "Cost after iteration 8700: 0.247520\n",
      "Cost after iteration 8800: 0.247386\n",
      "Cost after iteration 8900: 0.247255\n",
      "Cost after iteration 9000: 0.247128\n",
      "Cost after iteration 9100: 0.247003\n",
      "Cost after iteration 9200: 0.246882\n",
      "Cost after iteration 9300: 0.246763\n",
      "Cost after iteration 9400: 0.246648\n",
      "Cost after iteration 9500: 0.246534\n",
      "Cost after iteration 9600: 0.246424\n",
      "Cost after iteration 9700: 0.246316\n",
      "Cost after iteration 9800: 0.246211\n",
      "Cost after iteration 9900: 0.246108\n",
      "Cost after iteration 10000: 0.246007\n",
      "Cost after iteration 10100: 0.245908\n",
      "Cost after iteration 10200: 0.245812\n",
      "Cost after iteration 10300: 0.245718\n",
      "Cost after iteration 10400: 0.245626\n",
      "Cost after iteration 10500: 0.245535\n",
      "Cost after iteration 10600: 0.245447\n",
      "Cost after iteration 10700: 0.245361\n",
      "Cost after iteration 10800: 0.245276\n",
      "Cost after iteration 10900: 0.245193\n",
      "Cost after iteration 11000: 0.245112\n",
      "Cost after iteration 11100: 0.245033\n",
      "Cost after iteration 11200: 0.244955\n",
      "Cost after iteration 11300: 0.244878\n",
      "Cost after iteration 11400: 0.244804\n",
      "Cost after iteration 11500: 0.244730\n",
      "Cost after iteration 11600: 0.244658\n",
      "Cost after iteration 11700: 0.244588\n",
      "Cost after iteration 11800: 0.244519\n",
      "Cost after iteration 11900: 0.244451\n",
      "Cost after iteration 12000: 0.244385\n",
      "Cost after iteration 12100: 0.244320\n",
      "Cost after iteration 12200: 0.244256\n",
      "Cost after iteration 12300: 0.244193\n",
      "Cost after iteration 12400: 0.244131\n",
      "Cost after iteration 12500: 0.244071\n",
      "Cost after iteration 12600: 0.244012\n",
      "Cost after iteration 12700: 0.243953\n",
      "Cost after iteration 12800: 0.243896\n",
      "Cost after iteration 12900: 0.243840\n",
      "Cost after iteration 13000: 0.243785\n",
      "Cost after iteration 13100: 0.243730\n",
      "Cost after iteration 13200: 0.243677\n",
      "Cost after iteration 13300: 0.243625\n",
      "Cost after iteration 13400: 0.243573\n",
      "Cost after iteration 13500: 0.243523\n",
      "Cost after iteration 13600: 0.243473\n",
      "Cost after iteration 13700: 0.243424\n",
      "Cost after iteration 13800: 0.243376\n",
      "Cost after iteration 13900: 0.243329\n",
      "Cost after iteration 14000: 0.243283\n",
      "Cost after iteration 14100: 0.243237\n",
      "Cost after iteration 14200: 0.243192\n",
      "Cost after iteration 14300: 0.243148\n",
      "Cost after iteration 14400: 0.243105\n",
      "Cost after iteration 14500: 0.243062\n",
      "Cost after iteration 14600: 0.243020\n",
      "Cost after iteration 14700: 0.242978\n",
      "Cost after iteration 14800: 0.242938\n",
      "Cost after iteration 14900: 0.242897\n",
      "Cost after iteration 15000: 0.242858\n",
      "Cost after iteration 15100: 0.242819\n",
      "Cost after iteration 15200: 0.242781\n",
      "Cost after iteration 15300: 0.242743\n",
      "Cost after iteration 15400: 0.242706\n",
      "Cost after iteration 15500: 0.242669\n",
      "Cost after iteration 15600: 0.242633\n",
      "Cost after iteration 15700: 0.242598\n",
      "Cost after iteration 15800: 0.242563\n",
      "Cost after iteration 15900: 0.242529\n",
      "Cost after iteration 16000: 0.242495\n",
      "Cost after iteration 16100: 0.242461\n",
      "Cost after iteration 16200: 0.242428\n",
      "Cost after iteration 16300: 0.242396\n",
      "Cost after iteration 16400: 0.242364\n",
      "Cost after iteration 16500: 0.242332\n",
      "Cost after iteration 16600: 0.242301\n",
      "Cost after iteration 16700: 0.242270\n",
      "Cost after iteration 16800: 0.242240\n",
      "Cost after iteration 16900: 0.242210\n",
      "Cost after iteration 17000: 0.242181\n",
      "Cost after iteration 17100: 0.242152\n",
      "Cost after iteration 17200: 0.242123\n",
      "Cost after iteration 17300: 0.242095\n",
      "Cost after iteration 17400: 0.242067\n",
      "Cost after iteration 17500: 0.242040\n",
      "Cost after iteration 17600: 0.242013\n",
      "Cost after iteration 17700: 0.241986\n",
      "Cost after iteration 17800: 0.241960\n",
      "Cost after iteration 17900: 0.241934\n",
      "Cost after iteration 18000: 0.241908\n",
      "Cost after iteration 18100: 0.241883\n",
      "Cost after iteration 18200: 0.241858\n",
      "Cost after iteration 18300: 0.241833\n",
      "Cost after iteration 18400: 0.241809\n",
      "Cost after iteration 18500: 0.241785\n",
      "Cost after iteration 18600: 0.241761\n",
      "Cost after iteration 18700: 0.241737\n",
      "Cost after iteration 18800: 0.241714\n",
      "Cost after iteration 18900: 0.241691\n",
      "Cost after iteration 19000: 0.241669\n",
      "Cost after iteration 19100: 0.241646\n",
      "Cost after iteration 19200: 0.241624\n",
      "Cost after iteration 19300: 0.241603\n",
      "Cost after iteration 19400: 0.241581\n",
      "Cost after iteration 19500: 0.241560\n",
      "Cost after iteration 19600: 0.241539\n",
      "Cost after iteration 19700: 0.241518\n",
      "Cost after iteration 19800: 0.241498\n",
      "Cost after iteration 19900: 0.241477\n",
      "Cost after iteration 20000: 0.241457\n",
      "Cost after iteration 20100: 0.241437\n",
      "Cost after iteration 20200: 0.241418\n",
      "Cost after iteration 20300: 0.241399\n",
      "Cost after iteration 20400: 0.241380\n",
      "Cost after iteration 20500: 0.241361\n",
      "Cost after iteration 20600: 0.241342\n",
      "Cost after iteration 20700: 0.241324\n",
      "Cost after iteration 20800: 0.241305\n",
      "Cost after iteration 20900: 0.241287\n",
      "Cost after iteration 21000: 0.241269\n",
      "Cost after iteration 21100: 0.241252\n",
      "Cost after iteration 21200: 0.241234\n",
      "Cost after iteration 21300: 0.241217\n",
      "Cost after iteration 21400: 0.241200\n",
      "Cost after iteration 21500: 0.241183\n",
      "Cost after iteration 21600: 0.241166\n",
      "Cost after iteration 21700: 0.241150\n",
      "Cost after iteration 21800: 0.241134\n",
      "Cost after iteration 21900: 0.241117\n",
      "Cost after iteration 22000: 0.241101\n",
      "Cost after iteration 22100: 0.241086\n",
      "Cost after iteration 22200: 0.241070\n",
      "Cost after iteration 22300: 0.241054\n",
      "Cost after iteration 22400: 0.241039\n",
      "Cost after iteration 22500: 0.241024\n",
      "Cost after iteration 22600: 0.241009\n",
      "Cost after iteration 22700: 0.240994\n",
      "Cost after iteration 22800: 0.240979\n",
      "Cost after iteration 22900: 0.240965\n",
      "Cost after iteration 23000: 0.240950\n",
      "Cost after iteration 23100: 0.240936\n",
      "Cost after iteration 23200: 0.240922\n",
      "Cost after iteration 23300: 0.240908\n",
      "Cost after iteration 23400: 0.240894\n",
      "Cost after iteration 23500: 0.240881\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cost after iteration 23600: 0.240867\n",
      "Cost after iteration 23700: 0.240854\n",
      "Cost after iteration 23800: 0.240840\n",
      "Cost after iteration 23900: 0.240827\n",
      "Cost after iteration 24000: 0.240814\n",
      "Cost after iteration 24100: 0.240801\n",
      "Cost after iteration 24200: 0.240788\n",
      "Cost after iteration 24300: 0.240776\n",
      "Cost after iteration 24400: 0.240763\n",
      "Cost after iteration 24500: 0.240751\n",
      "Cost after iteration 24600: 0.240738\n",
      "Cost after iteration 24700: 0.240726\n",
      "Cost after iteration 24800: 0.240714\n",
      "Cost after iteration 24900: 0.240702\n",
      "Cost after iteration 25000: 0.240690\n",
      "Cost after iteration 25100: 0.240679\n",
      "Cost after iteration 25200: 0.240667\n",
      "Cost after iteration 25300: 0.240655\n",
      "Cost after iteration 25400: 0.240644\n",
      "Cost after iteration 25500: 0.240633\n",
      "Cost after iteration 25600: 0.240621\n",
      "Cost after iteration 25700: 0.240610\n",
      "Cost after iteration 25800: 0.240599\n",
      "Cost after iteration 25900: 0.240588\n",
      "Cost after iteration 26000: 0.240577\n",
      "Cost after iteration 26100: 0.240567\n",
      "Cost after iteration 26200: 0.240556\n",
      "Cost after iteration 26300: 0.240545\n",
      "Cost after iteration 26400: 0.240535\n",
      "Cost after iteration 26500: 0.240525\n",
      "Cost after iteration 26600: 0.240514\n",
      "Cost after iteration 26700: 0.240504\n",
      "Cost after iteration 26800: 0.240494\n",
      "Cost after iteration 26900: 0.240484\n",
      "Cost after iteration 27000: 0.240474\n",
      "Cost after iteration 27100: 0.240464\n",
      "Cost after iteration 27200: 0.240454\n",
      "Cost after iteration 27300: 0.240445\n",
      "Cost after iteration 27400: 0.240435\n",
      "Cost after iteration 27500: 0.240426\n",
      "Cost after iteration 27600: 0.240416\n",
      "Cost after iteration 27700: 0.240407\n",
      "Cost after iteration 27800: 0.240397\n",
      "Cost after iteration 27900: 0.240388\n",
      "Cost after iteration 28000: 0.240379\n",
      "Cost after iteration 28100: 0.240370\n",
      "Cost after iteration 28200: 0.240361\n",
      "Cost after iteration 28300: 0.240352\n",
      "Cost after iteration 28400: 0.240343\n",
      "Cost after iteration 28500: 0.240334\n",
      "Cost after iteration 28600: 0.240325\n",
      "Cost after iteration 28700: 0.240317\n",
      "Cost after iteration 28800: 0.240308\n",
      "Cost after iteration 28900: 0.240300\n",
      "Cost after iteration 29000: 0.240291\n",
      "Cost after iteration 29100: 0.240283\n",
      "Cost after iteration 29200: 0.240274\n",
      "Cost after iteration 29300: 0.240266\n",
      "Cost after iteration 29400: 0.240258\n",
      "Cost after iteration 29500: 0.240250\n",
      "Cost after iteration 29600: 0.240242\n",
      "Cost after iteration 29700: 0.240234\n",
      "Cost after iteration 29800: 0.240226\n",
      "Cost after iteration 29900: 0.240218\n",
      "Cost after iteration 30000: 0.240210\n",
      "Cost after iteration 30100: 0.240202\n",
      "Cost after iteration 30200: 0.240194\n",
      "Cost after iteration 30300: 0.240187\n",
      "Cost after iteration 30400: 0.240179\n",
      "Cost after iteration 30500: 0.240171\n",
      "Cost after iteration 30600: 0.240164\n",
      "Cost after iteration 30700: 0.240156\n",
      "Cost after iteration 30800: 0.240149\n",
      "Cost after iteration 30900: 0.240142\n",
      "Cost after iteration 31000: 0.240134\n",
      "Cost after iteration 31100: 0.240127\n",
      "Cost after iteration 31200: 0.240120\n",
      "Cost after iteration 31300: 0.240113\n",
      "Cost after iteration 31400: 0.240106\n",
      "Cost after iteration 31500: 0.240098\n",
      "Cost after iteration 31600: 0.240091\n",
      "Cost after iteration 31700: 0.240084\n",
      "Cost after iteration 31800: 0.240078\n",
      "Cost after iteration 31900: 0.240071\n",
      "Cost after iteration 32000: 0.240064\n",
      "Cost after iteration 32100: 0.240057\n",
      "Cost after iteration 32200: 0.240050\n",
      "Cost after iteration 32300: 0.240044\n",
      "Cost after iteration 32400: 0.240037\n",
      "Cost after iteration 32500: 0.240030\n",
      "Cost after iteration 32600: 0.240024\n",
      "Cost after iteration 32700: 0.240017\n",
      "Cost after iteration 32800: 0.240011\n",
      "Cost after iteration 32900: 0.240004\n",
      "Cost after iteration 33000: 0.239998\n",
      "Cost after iteration 33100: 0.239991\n",
      "Cost after iteration 33200: 0.239985\n",
      "Cost after iteration 33300: 0.239979\n",
      "Cost after iteration 33400: 0.239973\n",
      "Cost after iteration 33500: 0.239966\n",
      "Cost after iteration 33600: 0.239960\n",
      "Cost after iteration 33700: 0.239954\n",
      "Cost after iteration 33800: 0.239948\n",
      "Cost after iteration 33900: 0.239942\n",
      "Cost after iteration 34000: 0.239936\n",
      "Cost after iteration 34100: 0.239930\n",
      "Cost after iteration 34200: 0.239924\n",
      "Cost after iteration 34300: 0.239918\n",
      "Cost after iteration 34400: 0.239912\n",
      "Cost after iteration 34500: 0.239906\n",
      "Cost after iteration 34600: 0.239900\n",
      "Cost after iteration 34700: 0.239895\n",
      "Cost after iteration 34800: 0.239889\n",
      "Cost after iteration 34900: 0.239883\n",
      "Cost after iteration 35000: 0.239877\n",
      "Cost after iteration 35100: 0.239872\n",
      "Cost after iteration 35200: 0.239866\n",
      "Cost after iteration 35300: 0.239861\n",
      "Cost after iteration 35400: 0.239855\n",
      "Cost after iteration 35500: 0.239849\n",
      "Cost after iteration 35600: 0.239844\n",
      "Cost after iteration 35700: 0.239838\n",
      "Cost after iteration 35800: 0.239833\n",
      "Cost after iteration 35900: 0.239828\n",
      "Cost after iteration 36000: 0.239822\n",
      "Cost after iteration 36100: 0.239817\n",
      "Cost after iteration 36200: 0.239812\n",
      "Cost after iteration 36300: 0.239806\n",
      "Cost after iteration 36400: 0.239801\n",
      "Cost after iteration 36500: 0.239796\n",
      "Cost after iteration 36600: 0.239791\n",
      "Cost after iteration 36700: 0.239785\n",
      "Cost after iteration 36800: 0.239780\n",
      "Cost after iteration 36900: 0.239775\n",
      "Cost after iteration 37000: 0.239770\n",
      "Cost after iteration 37100: 0.239765\n",
      "Cost after iteration 37200: 0.239760\n",
      "Cost after iteration 37300: 0.239755\n",
      "Cost after iteration 37400: 0.239750\n",
      "Cost after iteration 37500: 0.239745\n",
      "Cost after iteration 37600: 0.239740\n",
      "Cost after iteration 37700: 0.239735\n",
      "Cost after iteration 37800: 0.239730\n",
      "Cost after iteration 37900: 0.239725\n",
      "Cost after iteration 38000: 0.239720\n",
      "Cost after iteration 38100: 0.239716\n",
      "Cost after iteration 38200: 0.239711\n",
      "Cost after iteration 38300: 0.239706\n",
      "Cost after iteration 38400: 0.239701\n",
      "Cost after iteration 38500: 0.239697\n",
      "Cost after iteration 38600: 0.239692\n",
      "Cost after iteration 38700: 0.239687\n",
      "Cost after iteration 38800: 0.239683\n",
      "Cost after iteration 38900: 0.239678\n",
      "Cost after iteration 39000: 0.239673\n",
      "Cost after iteration 39100: 0.239669\n",
      "Cost after iteration 39200: 0.239664\n",
      "Cost after iteration 39300: 0.239660\n",
      "Cost after iteration 39400: 0.239655\n",
      "Cost after iteration 39500: 0.239651\n",
      "Cost after iteration 39600: 0.239646\n",
      "Cost after iteration 39700: 0.239642\n",
      "Cost after iteration 39800: 0.239637\n",
      "Cost after iteration 39900: 0.239633\n",
      "Cost after iteration 40000: 0.239628\n",
      "Cost after iteration 40100: 0.239624\n",
      "Cost after iteration 40200: 0.239620\n",
      "Cost after iteration 40300: 0.239615\n",
      "Cost after iteration 40400: 0.239611\n",
      "Cost after iteration 40500: 0.239607\n",
      "Cost after iteration 40600: 0.239602\n",
      "Cost after iteration 40700: 0.239598\n",
      "Cost after iteration 40800: 0.239594\n",
      "Cost after iteration 40900: 0.239589\n",
      "Cost after iteration 41000: 0.239585\n",
      "Cost after iteration 41100: 0.239581\n",
      "Cost after iteration 41200: 0.239577\n",
      "Cost after iteration 41300: 0.239573\n",
      "Cost after iteration 41400: 0.239569\n",
      "Cost after iteration 41500: 0.239564\n",
      "Cost after iteration 41600: 0.239560\n",
      "Cost after iteration 41700: 0.239556\n",
      "Cost after iteration 41800: 0.239552\n",
      "Cost after iteration 41900: 0.239548\n",
      "Cost after iteration 42000: 0.239544\n",
      "Cost after iteration 42100: 0.239540\n",
      "Cost after iteration 42200: 0.239536\n",
      "Cost after iteration 42300: 0.239532\n",
      "Cost after iteration 42400: 0.239528\n",
      "Cost after iteration 42500: 0.239524\n",
      "Cost after iteration 42600: 0.239520\n",
      "Cost after iteration 42700: 0.239516\n",
      "Cost after iteration 42800: 0.239512\n",
      "Cost after iteration 42900: 0.239508\n",
      "Cost after iteration 43000: 0.239504\n",
      "Cost after iteration 43100: 0.239501\n",
      "Cost after iteration 43200: 0.239497\n",
      "Cost after iteration 43300: 0.239493\n",
      "Cost after iteration 43400: 0.239489\n",
      "Cost after iteration 43500: 0.239485\n",
      "Cost after iteration 43600: 0.239481\n",
      "Cost after iteration 43700: 0.239478\n",
      "Cost after iteration 43800: 0.239474\n",
      "Cost after iteration 43900: 0.239470\n",
      "Cost after iteration 44000: 0.239466\n",
      "Cost after iteration 44100: 0.239463\n",
      "Cost after iteration 44200: 0.239459\n",
      "Cost after iteration 44300: 0.239455\n",
      "Cost after iteration 44400: 0.239452\n",
      "Cost after iteration 44500: 0.239448\n",
      "Cost after iteration 44600: 0.239444\n",
      "Cost after iteration 44700: 0.239441\n",
      "Cost after iteration 44800: 0.239437\n",
      "Cost after iteration 44900: 0.239433\n",
      "Cost after iteration 45000: 0.239430\n",
      "Cost after iteration 45100: 0.239426\n",
      "Cost after iteration 45200: 0.239422\n",
      "Cost after iteration 45300: 0.239419\n",
      "Cost after iteration 45400: 0.239415\n",
      "Cost after iteration 45500: 0.239412\n",
      "Cost after iteration 45600: 0.239408\n",
      "Cost after iteration 45700: 0.239405\n",
      "Cost after iteration 45800: 0.239401\n",
      "Cost after iteration 45900: 0.239398\n",
      "Cost after iteration 46000: 0.239394\n",
      "Cost after iteration 46100: 0.239391\n",
      "Cost after iteration 46200: 0.239387\n",
      "Cost after iteration 46300: 0.239384\n",
      "Cost after iteration 46400: 0.239380\n",
      "Cost after iteration 46500: 0.239377\n",
      "Cost after iteration 46600: 0.239374\n",
      "Cost after iteration 46700: 0.239370\n",
      "Cost after iteration 46800: 0.239367\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cost after iteration 46900: 0.239363\n",
      "Cost after iteration 47000: 0.239360\n",
      "Cost after iteration 47100: 0.239357\n",
      "Cost after iteration 47200: 0.239353\n",
      "Cost after iteration 47300: 0.239350\n",
      "Cost after iteration 47400: 0.239347\n",
      "Cost after iteration 47500: 0.239343\n",
      "Cost after iteration 47600: 0.239340\n",
      "Cost after iteration 47700: 0.239337\n",
      "Cost after iteration 47800: 0.239333\n",
      "Cost after iteration 47900: 0.239330\n",
      "Cost after iteration 48000: 0.239327\n",
      "Cost after iteration 48100: 0.239323\n",
      "Cost after iteration 48200: 0.239320\n",
      "Cost after iteration 48300: 0.239317\n",
      "Cost after iteration 48400: 0.239314\n",
      "Cost after iteration 48500: 0.239311\n",
      "Cost after iteration 48600: 0.239307\n",
      "Cost after iteration 48700: 0.239304\n",
      "Cost after iteration 48800: 0.239301\n",
      "Cost after iteration 48900: 0.239298\n",
      "Cost after iteration 49000: 0.239295\n",
      "Cost after iteration 49100: 0.239291\n",
      "Cost after iteration 49200: 0.239288\n",
      "Cost after iteration 49300: 0.239285\n",
      "Cost after iteration 49400: 0.239282\n",
      "Cost after iteration 49500: 0.239279\n",
      "Cost after iteration 49600: 0.239276\n",
      "Cost after iteration 49700: 0.239273\n",
      "Cost after iteration 49800: 0.239270\n",
      "Cost after iteration 49900: 0.239266\n",
      "{'w': array([[ 0.57682533],\n",
      "       [-0.02939087],\n",
      "       [ 0.49713629],\n",
      "       [ 0.35909747],\n",
      "       [ 1.74174757],\n",
      "       [ 1.4519505 ],\n",
      "       [ 1.95983737],\n",
      "       [ 1.89766461],\n",
      "       [ 1.32256263],\n",
      "       [-2.75582194],\n",
      "       [ 2.00004275],\n",
      "       [ 1.8018632 ],\n",
      "       [-1.04709092],\n",
      "       [-0.53233594],\n",
      "       [ 1.175505  ],\n",
      "       [ 0.47261328],\n",
      "       [ 0.32146178],\n",
      "       [ 1.6658689 ],\n",
      "       [ 2.40013829],\n",
      "       [-0.38270539],\n",
      "       [-3.37628802],\n",
      "       [-0.83937688],\n",
      "       [-2.43891441],\n",
      "       [-0.89969624],\n",
      "       [ 0.29264685],\n",
      "       [-2.36936306],\n",
      "       [ 0.94511671],\n",
      "       [ 3.36964616],\n",
      "       [ 0.09254094],\n",
      "       [ 2.6813785 ],\n",
      "       [-2.61745622],\n",
      "       [ 0.64729194],\n",
      "       [ 0.11657655],\n",
      "       [-5.34941841],\n",
      "       [ 0.08928452],\n",
      "       [ 2.89339651],\n",
      "       [-1.13018774],\n",
      "       [-0.16685046],\n",
      "       [ 0.99827118],\n",
      "       [ 0.66598679],\n",
      "       [ 0.64071521],\n",
      "       [ 0.57419204],\n",
      "       [ 1.1498466 ],\n",
      "       [-0.2740554 ],\n",
      "       [ 2.10863548],\n",
      "       [ 0.79796034],\n",
      "       [ 0.81631524],\n",
      "       [ 0.55387135],\n",
      "       [ 0.45813304],\n",
      "       [-0.56293217],\n",
      "       [ 0.31011221],\n",
      "       [-1.50573972],\n",
      "       [-5.00028428],\n",
      "       [-0.1601339 ],\n",
      "       [ 1.58362956],\n",
      "       [-0.20893217],\n",
      "       [ 0.22608313],\n",
      "       [-1.54022376],\n",
      "       [ 0.81384786],\n",
      "       [-1.25110332],\n",
      "       [-2.26911385],\n",
      "       [-0.9756945 ],\n",
      "       [-1.4893724 ],\n",
      "       [-2.73442732]]), 'b': -2.182733197400524, 'training_accuracy': 0.9049740163325909, 'test_accuracy': 0.8044444444444444, 'cost': [0.5558107869210923, 0.47015398040010536, 0.42442935610802374, 0.3954096448848188, 0.3750229195560201, 0.35972637818347325, 0.34771583835857717, 0.337970170374888, 0.32986398989491494, 0.3229903019757201, 0.3170712582666522, 0.31190974182361525, 0.30736144045791236, 0.3033179000927725, 0.29969579480803465, 0.29642988421589017, 0.2934682465990768, 0.2907689655764687, 0.2882977731923775, 0.2860263389515271, 0.28393100524747583, 0.28199183763147634, 0.28019190121475707, 0.27851670216603097, 0.2769537515328955, 0.2754922209183274, 0.27412266798022034, 0.2728368156035638, 0.2716273727573453, 0.27048788803496376, 0.2694126290473818, 0.26839648243407604, 0.2674348704430225, 0.2665236809215595, 0.26565920823497874, 0.2648381031458074, 0.2640573300846177, 0.26331413055230846, 0.2626059916356891, 0.26193061880878965, 0.2612859123434621, 0.2606699467734416, 0.260080952952822, 0.2595173023280086, 0.25897749310557727, 0.2584601380501296, 0.25796395368856034, 0.2574877507319846, 0.2570304255553622, 0.2565909525987606, 0.25616837757411354, 0.2557618113779987, 0.2553704246249537, 0.2549934427276445, 0.25463014146017604, 0.2542798429492994, 0.2539419120454767, 0.25361575303191647, 0.2533008066349621, 0.2529965473037459, 0.25270248073091517, 0.25241814158960874, 0.2521430914647762, 0.2518769169594652, 0.2516192279589033, 0.2513696560371251, 0.2511278529925743, 0.2508934895005833, 0.25066625387192526, 0.2504458509077727, 0.2502320008423986, 0.2500244383658434, 0.24982291171955442, 0.2496271818587022, 0.24943702167549342, 0.24925221527835273, 0.24907255732233355, 0.24889785238655837, 0.2487279143948762, 0.24856256607628105, 0.24840163846194344, 0.24824497041599417, 0.24809240819745101, 0.24794380505090757, 0.24779902082381153, 0.247657921608344, 0.2475203794060805, 0.24738627181376735, 0.2472554817286843, 0.24712789707219013, 0.24700341053016134, 0.24688191930913836, 0.2467633249070872, 0.2466475328977702, 0.24653445272779895, 0.24642399752551233, 0.24631608392088966, 0.24621063187576733, 0.24610756452368285, 0.2460068080187203, 0.24590829139277642, 0.24581194642071055, 0.24571770749287788, 0.24562551149458423, 0.24553529769202984, 0.24544700762434393, 0.24536058500133592, 0.24527597560661724, 0.24519312720577058, 0.24511198945926593, 0.2450325138398414, 0.2449546535540884, 0.2448783634679946, 0.2448036000362176, 0.24473032123487423, 0.24465848649764688, 0.24458805665501812, 0.24451899387645998, 0.24445126161541184, 0.2443848245568957, 0.24431964856762092, 0.2442557006484473, 0.24419294888907464, 0.24413136242484382, 0.24407091139553372, 0.24401156690605103, 0.2439533009889118, 0.24389608656842277, 0.24383989742647377, 0.24378470816985914, 0.24373049419904913, 0.2436772316783388, 0.2436248975073053, 0.243573469293506, 0.24352292532635933, 0.24347324455214636, 0.24342440655008105, 0.24337639150939508, 0.2433291802073897, 0.24328275398840668, 0.24323709474367583, 0.24319218489199632, 0.24314800736121311, 0.24310454557045086, 0.24306178341307033, 0.24301970524031352, 0.24297829584560576, 0.24293754044948498, 0.24289742468512943, 0.2428579345844567, 0.24281905656476854, 0.24278077741591705, 0.24274308428796848, 0.2427059646793439, 0.24266940642541415, 0.24263339768753103, 0.24259792694247367, 0.24256298297229445, 0.24252855485454497, 0.24249463195286802, 0.24246120390793813, 0.24242826062873715, 0.24239579228415006, 0.2423637892948688, 0.2423322423255889, 0.2423011422774897, 0.2422704802809845, 0.24224024768873048, 0.24221043606888673, 0.24218103719861228, 0.24215204305779237, 0.242123445822985, 0.24209523786157863, 0.2420674117261529, 0.24203996014903384, 0.2420128760370363, 0.2419861524663862, 0.2419597826778154, 0.24193376007182332, 0.24190807820409707, 0.24188273078108635, 0.24185771165572534, 0.24183301482329675, 0.2418086344174329, 0.2417845647062478, 0.24176080008859657, 0.24173733509045606, 0.2417141643614233, 0.24169128267132664, 0.2416686849069464, 0.24164636606883944, 0.2416243212682656, 0.24160254572421125, 0.24158103476050655, 0.24155978380303358, 0.24153878837702158, 0.24151804410442593, 0.24149754670138895, 0.24147729197577858, 0.24145727582480273, 0.2414374942326966, 0.24141794326847998, 0.2413986190837831, 0.2413795179107372, 0.24136063605992933, 0.24134196991841758, 0.24132351594780563, 0.2413052706823743, 0.24128723072726818, 0.24126939275673576, 0.24125175351242045, 0.2412343098017019, 0.24121705849608527, 0.24119999652963678, 0.2411831208974645, 0.24116642865424218, 0.2411499169127759, 0.2411335828426106, 0.2411174236686765, 0.2411014366699732, 0.24108561917829105, 0.2410699685769681, 0.24105448229968113, 0.24103915782927096, 0.24102399269659924, 0.24100898447943753, 0.2409941308013856, 0.2409794293308203, 0.24096487777987233, 0.24095047390343058, 0.24093621549817346, 0.24092210040162607, 0.2409081264912424, 0.24089429168351226, 0.24088059393309125, 0.24086703123195438, 0.24085360160857078, 0.24084030312710167, 0.24082713388661725, 0.24081409202033546, 0.2408011756948794, 0.24078838310955453, 0.2407757124956438, 0.24076316211572116, 0.24075073026298233, 0.24073841526059261, 0.24072621546105097, 0.24071412924557034, 0.2407021550234731, 0.24069029123160188, 0.2406785363337451, 0.24066688882007614, 0.24065534720660692, 0.24064391003465477, 0.24063257587032177, 0.24062134330398768, 0.24061021094981455, 0.24059917744526313, 0.2405882414506217, 0.24057740164854535, 0.24056665674360686, 0.24055600546185787, 0.2405454465504006, 0.24053497877696997, 0.24052460092952474, 0.24051431181584934, 0.24050411026316407, 0.24049399511774466, 0.24048396524455098, 0.24047401952686384, 0.2404641568659303, 0.24045437618061727, 0.24044467640707276, 0.24043505649839525, 0.2404255154243098, 0.2404160521708523, 0.2404066657400605, 0.24039735514967162, 0.2403881194328273, 0.24037895763778477, 0.24036986882763442, 0.24036085208002364, 0.24035190648688698, 0.24034303115418196, 0.24033422520163053, 0.24032548776246657, 0.24031681798318863, 0.24030821502331787, 0.24029967805516161, 0.24029120626358147, 0.24028279884576711, 0.24027445501101397, 0.2402661739805064, 0.24025795498710525, 0.24024979727513968, 0.2402417001002035, 0.2402336627289559, 0.24022568443892614, 0.24021776451832258, 0.24020990226584515, 0.2402020969905024, 0.24019434801143166, 0.24018665465772315, 0.2401790162682482, 0.24017143219148981, 0.24016390178537775, 0.2401564244171266, 0.24014899946307677, 0.2401416263085392, 0.2401343043476428, 0.24012703298318527, 0.2401198116264868, 0.24011263969724592, 0.24010551662340007, 0.24009844184098633, 0.24009141479400747, 0.24008443493429876, 0.24007750172139813, 0.2400706146224191, 0.24006377311192548, 0.24005697667180914, 0.2400502247911696, 0.24004351696619664, 0.24003685270005404, 0.240030231502767, 0.24002365289111025, 0.24001711638849954, 0.24001062152488395, 0.24000416783664147, 0.2399977548664757, 0.23999138216331464, 0.23998504928221168, 0.23997875578424807, 0.2399725012364374, 0.23996628521163185, 0.23996010728843004, 0.2399539670510867, 0.23994786408942395, 0.23994179799874432, 0.23993576837974492, 0.2399297748384339, 0.23992381698604773, 0.23991789443897032, 0.2399120068186537, 0.23990615375153984, 0.2399003348689839, 0.23989454980717906, 0.23988879820708273, 0.23988307971434358, 0.2398773939792304, 0.2398717406565621, 0.2398661194056387, 0.23986052989017373, 0.23985497177822768, 0.23984944474214304, 0.23984394845847967, 0.23983848260795215, 0.23983304687536772, 0.2398276409495651, 0.23982226452335514, 0.23981691729346166, 0.23981159896046383, 0.2398063092287392, 0.23980104780640787, 0.23979581440527797, 0.23979060874079108, 0.23978543053196957, 0.23978027950136435, 0.23977515537500357, 0.23977005788234235, 0.23976498675621274, 0.23975994173277562, 0.23975492255147207, 0.23974992895497685, 0.2397449606891515, 0.23974001750299911, 0.23973509914861935, 0.23973020538116427, 0.2397253359587949, 0.23972049064263862, 0.23971566919674742, 0.2397108713880561, 0.2397060969863419, 0.2397013457641846, 0.2396966174969269, 0.23969191196263612, 0.23968722894206565, 0.23968256821861797, 0.23967792957830725, 0.2396733128097235, 0.23966871770399678, 0.2396641440547617, 0.23965959165812337, 0.23965506031262293, 0.23965054981920417, 0.23964605998118088, 0.2396415906042037, 0.23963714149622886, 0.23963271246748624, 0.23962830333044877, 0.23962391389980156, 0.23961954399241225, 0.23961519342730125, 0.2396108620256126, 0.23960654961058572, 0.2396022560075265, 0.23959798104378013, 0.2395937245487038, 0.23958948635363905, 0.2395852662918861, 0.2395810641986772, 0.23957687991115104, 0.23957271326832755, 0.23956856411108265, 0.23956443228212412, 0.23956031762596686, 0.23955621998890952, 0.23955213921901075, 0.23954807516606602, 0.23954402768158511, 0.2395399966187693, 0.2395359818324896, 0.2395319831792647, 0.23952800051723955, 0.2395240337061643, 0.23952008260737348, 0.23951614708376506, 0.23951222699978106, 0.2395083222213866, 0.239504432616051, 0.23950055805272813, 0.23949669840183718, 0.23949285353524433, 0.23948902332624375, 0.23948520764953943, 0.23948140638122736, 0.23947761939877776, 0.23947384658101736, 0.23947008780811246, 0.23946634296155178, 0.2394626119241296, 0.23945889457992947, 0.239455190814308, 0.23945150051387856, 0.2394478235664955, 0.23944415986123882, 0.23944050928839855, 0.23943687173945963, 0.23943324710708708, 0.2394296352851112, 0.23942603616851296, 0.23942244965340997, 0.23941887563704195, 0.23941531401775717, 0.2394117646949985, 0.23940822756928998, 0.23940470254222335, 0.23940118951644515, 0.23939768839564335, 0.23939419908453488, 0.23939072148885293, 0.23938725551533427, 0.23938380107170734, 0.23938035806667968, 0.2393769264099265, 0.23937350601207855, 0.2393700967847105, 0.2393666986403297, 0.23936331149236448, 0.23935993525515353, 0.23935656984393447, 0.23935321517483304, 0.23934987116485257, 0.2393465377318634, 0.2393432147945922, 0.23933990227261193, 0.23933660008633162, 0.23933330815698656, 0.23933002640662787, 0.23932675475811346, 0.23932349313509788, 0.23932024146202305, 0.239316999664109, 0.2393137676673443, 0.23931054539847738, 0.2393073327850072, 0.23930412975517457, 0.23930093623795326, 0.23929775216304136, 0.2392945774608529, 0.2392914120625094, 0.23928825589983138, 0.2392851089053303, 0.23928197101220058, 0.2392788421543114, 0.2392757222661989, 0.2392726112830585, 0.23926950914073702, 0.23926641577572533]}\n"
     ]
    }
   ],
   "source": [
    "pred = model(X_train, y_train, X_test, y_test, num_iterations=50000, learning_rate=1e-1, print_cost=True)\n",
    "print(pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.选做题"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Congratulations on building your first logistic regression model. It is your time to analyze it further."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 4.1 Observe the effect of learning rate on the leraning process.   \n",
    "Hits: plot the learning curve with different learning rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 422,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_cost = {}\n",
    "for lr_i in [4, 0.3, 0.1, 0.05, 0.01, 0.005, 0.001]:\n",
    "    get_pred = model(X_train, y_train, X_test, y_test, 5000, lr_i, False)\n",
    "    dict_cost[str(lr_i)] = get_pred['cost']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 423,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'4': [5.060179519338905,\n",
       "  1.2838817595299268,\n",
       "  0.28726583427010627,\n",
       "  1.3984358832664026,\n",
       "  0.5567114339479535,\n",
       "  0.31672232318488686,\n",
       "  0.3018427165768784,\n",
       "  0.3077034962468226,\n",
       "  0.35862698480213556,\n",
       "  0.6713985039923501,\n",
       "  1.577989371384001,\n",
       "  0.9373323021787056,\n",
       "  0.29211285889861077,\n",
       "  0.26519026080241204,\n",
       "  0.26449416928017294,\n",
       "  0.2657016024623213,\n",
       "  0.2673296079914418,\n",
       "  0.26935163022718805,\n",
       "  0.2719152343738305,\n",
       "  0.27529732921898237,\n",
       "  0.28004280328316267,\n",
       "  0.2901463668997807,\n",
       "  0.6587336778432492,\n",
       "  1.5502682064841065,\n",
       "  0.2767538264495546,\n",
       "  0.2628986525660215,\n",
       "  0.2641548368352721,\n",
       "  0.2661348238730461,\n",
       "  0.26861266917765997,\n",
       "  0.27181424592542847,\n",
       "  0.2761887813147167,\n",
       "  0.28295911989804695,\n",
       "  0.3819337109586174,\n",
       "  1.881515355964402,\n",
       "  0.2909078009211268,\n",
       "  0.2621442102127242,\n",
       "  0.26331704972217584,\n",
       "  0.26537559753970286,\n",
       "  0.2679563600943864,\n",
       "  0.2713016631880619,\n",
       "  0.2758969777210722,\n",
       "  0.2832303580000402,\n",
       "  0.4475250453375605,\n",
       "  1.8475440954874884,\n",
       "  0.2766329865160625,\n",
       "  0.26159026661973445,\n",
       "  0.2630460601538991,\n",
       "  0.2651831498336419,\n",
       "  0.267859086124608,\n",
       "  0.2713391819587576],\n",
       " '0.3': [4.866278066061343,\n",
       "  0.41116978961615014,\n",
       "  0.34910051678742976,\n",
       "  0.3215659603668101,\n",
       "  0.305201021516421,\n",
       "  0.2941106755627533,\n",
       "  0.28601665476152666,\n",
       "  0.2798203313311553,\n",
       "  0.27491535378957643,\n",
       "  0.2709350604401942,\n",
       "  0.2676425542402906,\n",
       "  0.2648768887859366,\n",
       "  0.2625243231733365,\n",
       "  0.26050189227247794,\n",
       "  0.25874749152391385,\n",
       "  0.25721361946727195,\n",
       "  0.2558632778529718,\n",
       "  0.2546671985431202,\n",
       "  0.25360191591632153,\n",
       "  0.2526483949592932,\n",
       "  0.25179103454223223,\n",
       "  0.25101693007655085,\n",
       "  0.25031531928781087,\n",
       "  0.24967715968672458,\n",
       "  0.24909480234226836,\n",
       "  0.24856173712852955,\n",
       "  0.24807239173256054,\n",
       "  0.24762197159224653,\n",
       "  0.24720633133974956,\n",
       "  0.24682187074034026,\n",
       "  0.24646544985181068,\n",
       "  0.24613431939333005,\n",
       "  0.24582606324379858,\n",
       "  0.2455385506835032,\n",
       "  0.24526989651498232,\n",
       "  0.245018427595626,\n",
       "  0.2447826546184608,\n",
       "  0.2445612482123543,\n",
       "  0.24435301861561726,\n",
       "  0.24415689832023368,\n",
       "  0.24397192719698443,\n",
       "  0.24379723970148287,\n",
       "  0.24363205383282566,\n",
       "  0.24347566157413517,\n",
       "  0.2433274205907507,\n",
       "  0.24318674699954115,\n",
       "  0.24305310905355842,\n",
       "  0.24292602161142896,\n",
       "  0.24280504128158834,\n",
       "  0.24268976214856544],\n",
       " '0.1': [4.63496375796394,\n",
       "  0.5286461633893205,\n",
       "  0.4552553414364163,\n",
       "  0.41449686826318294,\n",
       "  0.38794903281959287,\n",
       "  0.369015143226716,\n",
       "  0.35468292547686586,\n",
       "  0.3433694428555531,\n",
       "  0.33415815948820526,\n",
       "  0.32647865723182173,\n",
       "  0.31995553924141124,\n",
       "  0.31433055430396645,\n",
       "  0.30941953617370516,\n",
       "  0.3050871902313677,\n",
       "  0.3012316131536318,\n",
       "  0.2977744023538898,\n",
       "  0.2946541194863609,\n",
       "  0.2918218433799897,\n",
       "  0.28923806739796337,\n",
       "  0.2868704865694707,\n",
       "  0.2846923883306581,\n",
       "  0.2826814617941258,\n",
       "  0.2808189029206271,\n",
       "  0.27908873257873773,\n",
       "  0.27747727019837987,\n",
       "  0.2759727227799122,\n",
       "  0.27456486054830853,\n",
       "  0.27324475846944063,\n",
       "  0.27200458838385017,\n",
       "  0.27083745043903423,\n",
       "  0.269737235321081,\n",
       "  0.2686985108370303,\n",
       "  0.2677164279075514,\n",
       "  0.266786642150662,\n",
       "  0.2659052480788827,\n",
       "  0.2650687235699428,\n",
       "  0.2642738827585536,\n",
       "  0.26351783587231126,\n",
       "  0.2627979548263843,\n",
       "  0.26211184361968143,\n",
       "  0.26145731275476675,\n",
       "  0.26083235704610447,\n",
       "  0.2602351362946989,\n",
       "  0.25966395839822565,\n",
       "  0.2591172645391632,\n",
       "  0.2585936161529676,\n",
       "  0.2580916834268467,\n",
       "  0.25761023511941483,\n",
       "  0.25714812952420096,\n",
       "  0.25670430642698205],\n",
       " '0.05': [4.581573025235467,\n",
       "  0.6122262702247274,\n",
       "  0.5390546279686397,\n",
       "  0.4945264560308077,\n",
       "  0.4636314152889146,\n",
       "  0.4404976414815811,\n",
       "  0.4223262526107712,\n",
       "  0.40756849131946155,\n",
       "  0.39527911296702095,\n",
       "  0.38484201201486323,\n",
       "  0.37583608748761405,\n",
       "  0.3679627336358378,\n",
       "  0.36100376406854556,\n",
       "  0.3547956857270166,\n",
       "  0.34921331422967955,\n",
       "  0.344158983452087,\n",
       "  0.33955523429756745,\n",
       "  0.33533973676212064,\n",
       "  0.3314616851750794,\n",
       "  0.3278791888159104,\n",
       "  0.3245573496380882,\n",
       "  0.3214668235474075,\n",
       "  0.31858272799472537,\n",
       "  0.31588380158392787,\n",
       "  0.3133517497704656,\n",
       "  0.31097072982492713,\n",
       "  0.3087269413120923,\n",
       "  0.3066082974296227,\n",
       "  0.3046041589672895,\n",
       "  0.3027051172368896,\n",
       "  0.3009028156466966,\n",
       "  0.2991898020297491,\n",
       "  0.29755940563951294,\n",
       "  0.2960056340767947,\n",
       "  0.29452308643217107,\n",
       "  0.29310687970621446,\n",
       "  0.2917525861680563,\n",
       "  0.29045617977653887,\n",
       "  0.2892139901503327,\n",
       "  0.28802266285820244,\n",
       "  0.2868791250261008,\n",
       "  0.28578055543744185,\n",
       "  0.2847243584469254,\n",
       "  0.28370814114438436,\n",
       "  0.28272969329922887,\n",
       "  0.28178696969272105,\n",
       "  0.28087807450807556,\n",
       "  0.2800012475000016,\n",
       "  0.27915485170794785,\n",
       "  0.27833736251269503],\n",
       " '0.01': [4.908405189599392,\n",
       "  2.0939274375009322,\n",
       "  0.7568369270005767,\n",
       "  0.6968711296921327,\n",
       "  0.6713380006695788,\n",
       "  0.6488899937286635,\n",
       "  0.6288880513794262,\n",
       "  0.6109612279679674,\n",
       "  0.5948050235638035,\n",
       "  0.5801686674292226,\n",
       "  0.5668450318797529,\n",
       "  0.5546622358385727,\n",
       "  0.5434768299134977,\n",
       "  0.5331683445010618,\n",
       "  0.5236349586026218,\n",
       "  0.5147900658633304,\n",
       "  0.5065595483963068,\n",
       "  0.49887960490920746,\n",
       "  0.4916950119030648,\n",
       "  0.4849577235274888,\n",
       "  0.4786257370749914,\n",
       "  0.4726621677833242,\n",
       "  0.4670344894595364,\n",
       "  0.4617139072632985,\n",
       "  0.45667483648313867,\n",
       "  0.4518944668622449,\n",
       "  0.44735239641067437,\n",
       "  0.44303032200615844,\n",
       "  0.43891177668364517,\n",
       "  0.43498190553008015,\n",
       "  0.43122727367491676,\n",
       "  0.42763570110286736,\n",
       "  0.4241961199919028,\n",
       "  0.4208984510556465,\n",
       "  0.41773349598984727,\n",
       "  0.4146928436216442,\n",
       "  0.411768787763894,\n",
       "  0.4089542551049523,\n",
       "  0.4062427417324666,\n",
       "  0.4036282571099917,\n",
       "  0.4011052745070022,\n",
       "  0.3986686870335363,\n",
       "  0.39631376855613465,\n",
       "  0.39403613887657407,\n",
       "  0.391831732642869,\n",
       "  0.3896967715360962,\n",
       "  0.38762773933921274,\n",
       "  0.3856213595471297,\n",
       "  0.3836745752224631,\n",
       "  0.38178453083991754],\n",
       " '0.005': [3.4297946479251142,\n",
       "  2.037245427090108,\n",
       "  0.9734776689675033,\n",
       "  0.6591238960097933,\n",
       "  0.6175535973499133,\n",
       "  0.6053323909594573,\n",
       "  0.5959454100713408,\n",
       "  0.5872343219611533,\n",
       "  0.5789959685812823,\n",
       "  0.5711831957618213,\n",
       "  0.5637637328475366,\n",
       "  0.5567088891928358,\n",
       "  0.5499924733015502,\n",
       "  0.543590530263648,\n",
       "  0.5374811520054447,\n",
       "  0.5316443017604914,\n",
       "  0.5260616491521309,\n",
       "  0.5207164163529376,\n",
       "  0.5155932357006747,\n",
       "  0.5106780187942316,\n",
       "  0.5059578368024119,\n",
       "  0.501420811512186,\n",
       "  0.49705601650798575,\n",
       "  0.49285338779447246,\n",
       "  0.4888036431383567,\n",
       "  0.48489820939900313,\n",
       "  0.4811291571336731,\n",
       "  0.47748914179421836,\n",
       "  0.4739713508724096,\n",
       "  0.4705694563967507,\n",
       "  0.4672775722316241,\n",
       "  0.46409021567776276,\n",
       "  0.4610022729198996,\n",
       "  0.4580089679120313,\n",
       "  0.45510583433246504,\n",
       "  0.4522886902793772,\n",
       "  0.44955361541289973,\n",
       "  0.44689693028177896,\n",
       "  0.44431517760154576,\n",
       "  0.44180510527706823,\n",
       "  0.4393636509855411,\n",
       "  0.4369879281566163,\n",
       "  0.4346752132047403,\n",
       "  0.43242293388503705,\n",
       "  0.4302286586584983,\n",
       "  0.42809008696499196,\n",
       "  0.42600504031387104,\n",
       "  0.42397145411192694,\n",
       "  0.42198737015722065,\n",
       "  0.4200509297351013],\n",
       " '0.001': [4.557067467099917,\n",
       "  4.271550532161133,\n",
       "  3.9862251781978233,\n",
       "  3.7012251555925597,\n",
       "  3.4167781821829273,\n",
       "  3.1332713723288608,\n",
       "  2.851359891473943,\n",
       "  2.5721421843951457,\n",
       "  2.2974275521198804,\n",
       "  2.030103968064161,\n",
       "  1.7745407838149865,\n",
       "  1.5367896552288904,\n",
       "  1.3241191139157102,\n",
       "  1.1434425806385133,\n",
       "  0.9989413175055333,\n",
       "  0.8903099012835396,\n",
       "  0.8130072285499774,\n",
       "  0.7602316513528703,\n",
       "  0.7250889151631256,\n",
       "  0.7018710714644096,\n",
       "  0.6863941324321122,\n",
       "  0.6758136922716312,\n",
       "  0.6682838957031602,\n",
       "  0.6626405293541436,\n",
       "  0.6581618952769556,\n",
       "  0.6544052305975465,\n",
       "  0.6511005431512444,\n",
       "  0.6480837366660754,\n",
       "  0.6452552122809014,\n",
       "  0.6425545043803766,\n",
       "  0.6399448365015342,\n",
       "  0.637403752914738,\n",
       "  0.6349174489185769,\n",
       "  0.6324773445690632,\n",
       "  0.6300780160682152,\n",
       "  0.6277159475904102,\n",
       "  0.6253887784400274,\n",
       "  0.6230948490734525,\n",
       "  0.6208329273663875,\n",
       "  0.618602043553668,\n",
       "  0.6164013906748382,\n",
       "  0.6142302644993433,\n",
       "  0.6120880272434119,\n",
       "  0.609974085624045,\n",
       "  0.6078878775528563,\n",
       "  0.6058288640369369,\n",
       "  0.6037965242183762,\n",
       "  0.6017903523061835,\n",
       "  0.5998098556496474,\n",
       "  0.597854553500572]}"
      ]
     },
     "execution_count": 423,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dict_cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 456,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x432 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize=(10, 6))\n",
    "for cost_i in dict_cost:\n",
    "    plt.plot(dict_cost[cost_i], label=cost_i)\n",
    "    plt.legend(loc=\"upper right\")\n",
    "    plt.title('learning_curve')\n",
    "    sns.despine()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Summary: 学习率过大会造成训练速度过快，函数震荡。学习率过小则造成学习则造成训练速度慢，消耗过多时间和空间"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 4.2 Observe the effect of iteration_num on the test accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 438,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration_num:1000, test_accuracy:0.5533333333333333\n",
      "\n",
      "iteration_num:5000, test_accuracy:0.7444444444444445\n",
      "\n",
      "iteration_num:10000, test_accuracy:0.7888888888888889\n",
      "\n",
      "iteration_num:30000, test_accuracy:0.8044444444444444\n",
      "\n",
      "iteration_num:50000, test_accuracy:0.8022222222222222\n",
      "\n"
     ]
    }
   ],
   "source": [
    "cost_dict = {}\n",
    "for in_i in [1000, 5000, 10000, 30000, 50000]:\n",
    "    pred_model = model(X_train, y_train, X_test, y_test, in_i, 1e-2,False)\n",
    "    cost_dict[str(in_i)] = pred_model['cost']\n",
    "    print('iteration_num:{}, test_accuracy:{}\\n'\n",
    "          .format(str(in_i), pred_model['test_accuracy']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 440,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'1000': [4.946781035959649,\n",
       "  2.1383989108018655,\n",
       "  0.846857025022259,\n",
       "  0.7775585979318612,\n",
       "  0.7402453356824378,\n",
       "  0.7078432079280825,\n",
       "  0.6794855475932148,\n",
       "  0.6545393566621196,\n",
       "  0.6324691620417364,\n",
       "  0.6128287006191951],\n",
       " '5000': [4.729316298541337,\n",
       "  1.927248489875299,\n",
       "  0.7102749306907491,\n",
       "  0.6618657126417963,\n",
       "  0.6402071426481128,\n",
       "  0.6205975567600727,\n",
       "  0.6026922010452637,\n",
       "  0.5863169779957719,\n",
       "  0.5713152870427856,\n",
       "  0.5575462437366863,\n",
       "  0.5448836216409739,\n",
       "  0.5332146784933378,\n",
       "  0.5224389411684142,\n",
       "  0.5124670056701619,\n",
       "  0.5032193890400805,\n",
       "  0.4946254555322084,\n",
       "  0.4866224288794124,\n",
       "  0.47915449510332664,\n",
       "  0.47217199535004134,\n",
       "  0.4656307050438988,\n",
       "  0.45949119374967845,\n",
       "  0.4537182591393046,\n",
       "  0.44828042808714214,\n",
       "  0.44314951795721375,\n",
       "  0.4383002514439342,\n",
       "  0.43370991877734716,\n",
       "  0.42935808162975736,\n",
       "  0.42522631361288027,\n",
       "  0.4212979728006203,\n",
       "  0.4175580022322841,\n",
       "  0.41399275483309184,\n",
       "  0.41058983962781787,\n",
       "  0.40733798651775144,\n",
       "  0.40422692724185555,\n",
       "  0.4012472904524571,\n",
       "  0.3983905091072654,\n",
       "  0.3956487386165841,\n",
       "  0.39301478439090487,\n",
       "  0.39048203761318767,\n",
       "  0.3880444182153409,\n",
       "  0.3856963241727327,\n",
       "  0.3834325863467203,\n",
       "  0.3812484282055788,\n",
       "  0.37913942984098076,\n",
       "  0.377101495772185,\n",
       "  0.37513082609494963,\n",
       "  0.37322389058830324,\n",
       "  0.3713774054408925,\n",
       "  0.3695883123007318,\n",
       "  0.3678537593886953],\n",
       " '10000': [4.911365479036205,\n",
       "  2.0971766408410684,\n",
       "  0.7321542109380611,\n",
       "  0.6663015482051678,\n",
       "  0.6400735308680501,\n",
       "  0.6173476131391451,\n",
       "  0.5973594725061488,\n",
       "  0.5796608572030885,\n",
       "  0.5638828956045492,\n",
       "  0.5497241965863681,\n",
       "  0.536939549646105,\n",
       "  0.5253294250904235,\n",
       "  0.5147309177686945,\n",
       "  0.5050102787653706,\n",
       "  0.496056924122371,\n",
       "  0.4877787069348193,\n",
       "  0.4800982193350832,\n",
       "  0.4729499100601879,\n",
       "  0.46627783678550616,\n",
       "  0.46003390773123093,\n",
       "  0.45417649875072197,\n",
       "  0.4486693584157881,\n",
       "  0.4434807344957015,\n",
       "  0.43858267136661894,\n",
       "  0.4339504401620802,\n",
       "  0.42956207272118,\n",
       "  0.4253979773220464,\n",
       "  0.4214406193759507,\n",
       "  0.41767425414376125,\n",
       "  0.4140847014560871,\n",
       "  0.4106591546211104,\n",
       "  0.4073860173747315,\n",
       "  0.4042547640026098,\n",
       "  0.40125581874331645,\n",
       "  0.39838045134000377,\n",
       "  0.39562068619921015,\n",
       "  0.39296922307997684,\n",
       "  0.3904193676042769,\n",
       "  0.38796497017320924,\n",
       "  0.38560037210924747,\n",
       "  0.38332035803573583,\n",
       "  0.38112011366039766,\n",
       "  0.37899518825723794,\n",
       "  0.3769414612465465,\n",
       "  0.3749551123601334,\n",
       "  0.37303259495189917,\n",
       "  0.37117061207504265,\n",
       "  0.36936609499879497,\n",
       "  0.3676161838812259,\n",
       "  0.36591821035178007,\n",
       "  0.36426968178885877,\n",
       "  0.3626682671048689,\n",
       "  0.3611117838744421,\n",
       "  0.35959818666159504,\n",
       "  0.3581255564189293,\n",
       "  0.3566920908470009,\n",
       "  0.35529609561502995,\n",
       "  0.35393597635549084,\n",
       "  0.35261023135503555,\n",
       "  0.3513174448728799,\n",
       "  0.35005628102538394,\n",
       "  0.34882547818223275,\n",
       "  0.34762384382549943,\n",
       "  0.34645024982804756,\n",
       "  0.34530362811230125,\n",
       "  0.34418296665445336,\n",
       "  0.3430873058027628,\n",
       "  0.34201573488176606,\n",
       "  0.34096738905705387,\n",
       "  0.33994144643776947,\n",
       "  0.3389371253962273,\n",
       "  0.3379536820860408,\n",
       "  0.3369904081419362,\n",
       "  0.3360466285460202,\n",
       "  0.33512169964670013,\n",
       "  0.33421500731773374,\n",
       "  0.3333259652460379,\n",
       "  0.33245401333791724,\n",
       "  0.3315986162343045,\n",
       "  0.3307592619264426,\n",
       "  0.32993546046419286,\n",
       "  0.3291267427498361,\n",
       "  0.32833265941085205,\n",
       "  0.32755277974571523,\n",
       "  0.3267866907372565,\n",
       "  0.3260339961285929,\n",
       "  0.3252943155570436,\n",
       "  0.32456728374183136,\n",
       "  0.3238525497217065,\n",
       "  0.32314977613894574,\n",
       "  0.3224586385664604,\n",
       "  0.3217788248750092,\n",
       "  0.3211100346377462,\n",
       "  0.3204519785695487,\n",
       "  0.319804377998771,\n",
       "  0.31916696436924413,\n",
       "  0.31853947877051414,\n",
       "  0.31792167149445455,\n",
       "  0.3173133016165345,\n",
       "  0.3167141366001463],\n",
       " '30000': [5.022868687622601,\n",
       "  2.201991680718782,\n",
       "  0.7663944823759478,\n",
       "  0.6927821848646865,\n",
       "  0.6632407598149245,\n",
       "  0.6377090554978259,\n",
       "  0.6153487009881039,\n",
       "  0.5956533773496435,\n",
       "  0.5781973349884445,\n",
       "  0.5626274427852118,\n",
       "  0.5486534991145875,\n",
       "  0.5360378995415652,\n",
       "  0.5245860375765838,\n",
       "  0.5141380175462882,\n",
       "  0.5045617986166211,\n",
       "  0.4957476640850053,\n",
       "  0.48760382351037224,\n",
       "  0.4800529408881016,\n",
       "  0.4730294003845714,\n",
       "  0.4664771508216942,\n",
       "  0.460348000782049,\n",
       "  0.4546002635277737,\n",
       "  0.44919767356510976,\n",
       "  0.44410851470867513,\n",
       "  0.43930491351538853,\n",
       "  0.43476226271103274,\n",
       "  0.4304587474216468,\n",
       "  0.42637495323858515,\n",
       "  0.42249353986376353,\n",
       "  0.41879896766798597,\n",
       "  0.41527726723035996,\n",
       "  0.41191584402168974,\n",
       "  0.4087033120073608,\n",
       "  0.40562935119357374,\n",
       "  0.4026845851128944,\n",
       "  0.3998604750067597,\n",
       "  0.3971492280631139,\n",
       "  0.39454371754381057,\n",
       "  0.3920374130168017,\n",
       "  0.38962431921365814,\n",
       "  0.38729892227984214,\n",
       "  0.3850561423857657,\n",
       "  0.3828912918306204,\n",
       "  0.38080003790563816,\n",
       "  0.37877836989465896,\n",
       "  0.3768225696821495,\n",
       "  0.3749291855157322,\n",
       "  0.37309500853467437,\n",
       "  0.37131705172992313,\n",
       "  0.3695925310469589,\n",
       "  0.367918848381448,\n",
       "  0.36629357625058234,\n",
       "  0.3647144439510633,\n",
       "  0.3631793250387113,\n",
       "  0.3616862259852999,\n",
       "  0.3602332758859655,\n",
       "  0.35881871710586294,\n",
       "  0.3574408967680013,\n",
       "  0.3560982589956957,\n",
       "  0.3547893378330793,\n",
       "  0.3535127507758424,\n",
       "  0.35226719285198216,\n",
       "  0.3510514311990214,\n",
       "  0.3498643000900007,\n",
       "  0.3487046963656974,\n",
       "  0.3475715752350469,\n",
       "  0.34646394640974143,\n",
       "  0.34538087054250793,\n",
       "  0.3443214559416944,\n",
       "  0.3432848555375602,\n",
       "  0.3422702640781302,\n",
       "  0.34127691553465644,\n",
       "  0.34030408069868234,\n",
       "  0.3393510649544402,\n",
       "  0.33841720621186644,\n",
       "  0.33750187298690837,\n",
       "  0.3366044626170351,\n",
       "  0.33572439960098704,\n",
       "  0.3348611340527937,\n",
       "  0.3340141402609929,\n",
       "  0.3331829153447925,\n",
       "  0.3323669779996481,\n",
       "  0.33156586732538484,\n",
       "  0.330779141730591,\n",
       "  0.3300063779075439,\n",
       "  0.32924716987241853,\n",
       "  0.3285011280659692,\n",
       "  0.32776787851027284,\n",
       "  0.32704706201748945,\n",
       "  0.32633833344691954,\n",
       "  0.325641361006944,\n",
       "  0.32495582559870073,\n",
       "  0.32428142019860384,\n",
       "  0.32361784927703724,\n",
       "  0.32296482825076206,\n",
       "  0.3223220829667661,\n",
       "  0.3216893492154581,\n",
       "  0.32106637227126644,\n",
       "  0.32045290645884833,\n",
       "  0.3198487147432484,\n",
       "  0.3192535683424675,\n",
       "  0.31866724636101457,\n",
       "  0.3180895354431184,\n",
       "  0.31752022944436936,\n",
       "  0.3169591291206502,\n",
       "  0.3164060418332933,\n",
       "  0.31586078126947853,\n",
       "  0.31532316717695075,\n",
       "  0.3147930251122019,\n",
       "  0.31427018620131963,\n",
       "  0.31375448691275803,\n",
       "  0.31324576884133626,\n",
       "  0.31274387850281615,\n",
       "  0.31224866713845423,\n",
       "  0.31175999052896075,\n",
       "  0.3112777088173365,\n",
       "  0.31080168634009225,\n",
       "  0.31033179146638534,\n",
       "  0.30986789644464097,\n",
       "  0.3094098772562479,\n",
       "  0.30895761347594847,\n",
       "  0.30851098813856315,\n",
       "  0.3080698876117128,\n",
       "  0.30763420147422366,\n",
       "  0.3072038223999157,\n",
       "  0.3067786460464973,\n",
       "  0.3063585709493004,\n",
       "  0.30594349841961227,\n",
       "  0.30553333244736725,\n",
       "  0.305127979607982,\n",
       "  0.30472734897312515,\n",
       "  0.30433135202522826,\n",
       "  0.3039399025755523,\n",
       "  0.3035529166856374,\n",
       "  0.30317031259197125,\n",
       "  0.3027920106337216,\n",
       "  0.30241793318338633,\n",
       "  0.30204800458022346,\n",
       "  0.30168215106632873,\n",
       "  0.301320300725239,\n",
       "  0.30096238342294335,\n",
       "  0.30060833075119014,\n",
       "  0.30025807597298715,\n",
       "  0.2999115539701929,\n",
       "  0.29956870119310686,\n",
       "  0.2992294556119669,\n",
       "  0.29889375667027174,\n",
       "  0.29856154523984507,\n",
       "  0.2982327635775673,\n",
       "  0.29790735528370016,\n",
       "  0.29758526526173745,\n",
       "  0.29726643967971383,\n",
       "  0.29695082593291144,\n",
       "  0.2966383726079041,\n",
       "  0.29632902944788264,\n",
       "  0.2960227473192081,\n",
       "  0.29571947817914135,\n",
       "  0.29541917504470083,\n",
       "  0.2951217919626014,\n",
       "  0.29482728398023145,\n",
       "  0.2945356071176246,\n",
       "  0.2942467183403869,\n",
       "  0.2939605755335408,\n",
       "  0.2936771374762503,\n",
       "  0.29339636381739054,\n",
       "  0.29311821505193064,\n",
       "  0.29284265249809677,\n",
       "  0.29256963827528565,\n",
       "  0.29229913528269885,\n",
       "  0.29203110717867165,\n",
       "  0.29176551836066783,\n",
       "  0.2915023339459172,\n",
       "  0.29124151975267015,\n",
       "  0.2909830422820474,\n",
       "  0.2907268687004607,\n",
       "  0.29047296682258616,\n",
       "  0.290221305094867,\n",
       "  0.2899718525795283,\n",
       "  0.2897245789390846,\n",
       "  0.28947945442132167,\n",
       "  0.28923644984473645,\n",
       "  0.2889955365844179,\n",
       "  0.28875668655835407,\n",
       "  0.28851987221414904,\n",
       "  0.2882850665161361,\n",
       "  0.2880522429328731,\n",
       "  0.28782137542500663,\n",
       "  0.28759243843349225,\n",
       "  0.28736540686815815,\n",
       "  0.2871402560966013,\n",
       "  0.2869169619334039,\n",
       "  0.28669550062965915,\n",
       "  0.28647584886279726,\n",
       "  0.2862579837266999,\n",
       "  0.2860418827220943,\n",
       "  0.2858275237472179,\n",
       "  0.28561488508874405,\n",
       "  0.2854039454129604,\n",
       "  0.2851946837571919,\n",
       "  0.2849870795214603,\n",
       "  0.28478111246037185,\n",
       "  0.2845767626752273,\n",
       "  0.28437401060634565,\n",
       "  0.2841728370255953,\n",
       "  0.28397322302912625,\n",
       "  0.28377515003029713,\n",
       "  0.2835785997527897,\n",
       "  0.283383554223907,\n",
       "  0.283189995768047,\n",
       "  0.2829979070003489,\n",
       "  0.2828072708205044,\n",
       "  0.2826180704067302,\n",
       "  0.2824302892098968,\n",
       "  0.2822439109478081,\n",
       "  0.28205891959962776,\n",
       "  0.28187529940044814,\n",
       "  0.28169303483599656,\n",
       "  0.2815121106374759,\n",
       "  0.28133251177653507,\n",
       "  0.2811542234603649,\n",
       "  0.280977231126917,\n",
       "  0.2808015204402413,\n",
       "  0.2806270772859377,\n",
       "  0.2804538877667214,\n",
       "  0.2802819381980954,\n",
       "  0.280111215104129,\n",
       "  0.2799417052133392,\n",
       "  0.27977339545467084,\n",
       "  0.279606272953574,\n",
       "  0.27944032502817523,\n",
       "  0.27927553918554016,\n",
       "  0.2791119031180247,\n",
       "  0.2789494046997128,\n",
       "  0.27878803198293767,\n",
       "  0.2786277731948848,\n",
       "  0.2784686167342746,\n",
       "  0.2783105511681213,\n",
       "  0.2781535652285679,\n",
       "  0.2779976478097927,\n",
       "  0.2778427879649888,\n",
       "  0.2776889749034103,\n",
       "  0.2775361979874879,\n",
       "  0.27738444673000806,\n",
       "  0.277233710791357,\n",
       "  0.27708397997682654,\n",
       "  0.27693524423397936,\n",
       "  0.27678749365007405,\n",
       "  0.2766407184495468,\n",
       "  0.2764949089915489,\n",
       "  0.27635005576753807,\n",
       "  0.2762061493989238,\n",
       "  0.2760631806347623,\n",
       "  0.2759211403495033,\n",
       "  0.27578001954078474,\n",
       "  0.27563980932727516,\n",
       "  0.2755005009465632,\n",
       "  0.2753620857530906,\n",
       "  0.27522455521613165,\n",
       "  0.27508790091781343,\n",
       "  0.2749521145511791,\n",
       "  0.274817187918292,\n",
       "  0.2746831129283788,\n",
       "  0.27454988159601224,\n",
       "  0.2744174860393317,\n",
       "  0.2742859184783,\n",
       "  0.2741551712329971,\n",
       "  0.27402523672194856,\n",
       "  0.2738961074604881,\n",
       "  0.273767776059154,\n",
       "  0.27364023522211794,\n",
       "  0.27351347774564594,\n",
       "  0.27338749651658983,\n",
       "  0.27326228451091017,\n",
       "  0.2731378347922274,\n",
       "  0.27301414051040335,\n",
       "  0.2728911949001498,\n",
       "  0.27276899127966536,\n",
       "  0.2726475230492994,\n",
       "  0.2725267836902416,\n",
       "  0.2724067667632378,\n",
       "  0.27228746590733066,\n",
       "  0.2721688748386252,\n",
       "  0.2720509873490777,\n",
       "  0.27193379730530876,\n",
       "  0.2718172986474385,\n",
       "  0.27170148538794475,\n",
       "  0.2715863516105425,\n",
       "  0.27147189146908535,\n",
       "  0.27135809918648734,\n",
       "  0.2712449690536652,\n",
       "  0.27113249542850093,\n",
       "  0.2710206727348234,\n",
       "  0.2709094954614095,\n",
       "  0.2707989581610036,\n",
       "  0.270689055449355,\n",
       "  0.2705797820042742,\n",
       "  0.2704711325647052,\n",
       "  0.2703631019298164,\n",
       "  0.27025568495810676,\n",
       "  0.27014887656652914],\n",
       " '50000': [5.750758410419717,\n",
       "  2.901635908384031,\n",
       "  0.8443855719099306,\n",
       "  0.6919009441248449,\n",
       "  0.6675847037410908,\n",
       "  0.6461930252115036,\n",
       "  0.6267743329631523,\n",
       "  0.6090962904057764,\n",
       "  0.5929582828180124,\n",
       "  0.5781850088133919,\n",
       "  0.5646235351494282,\n",
       "  0.5521405154156568,\n",
       "  0.5406196385054628,\n",
       "  0.5299593446560699,\n",
       "  0.5200708154163883,\n",
       "  0.5108762251382792,\n",
       "  0.5023072317603043,\n",
       "  0.49430368061279345,\n",
       "  0.48681249443107283,\n",
       "  0.479786724168854,\n",
       "  0.47318473756113877,\n",
       "  0.4669695250905421,\n",
       "  0.46110810571666094,\n",
       "  0.45557101725337107,\n",
       "  0.45033187854204043,\n",
       "  0.4453670125458455,\n",
       "  0.44065512118952027,\n",
       "  0.4361770042132865,\n",
       "  0.4319153155290197,\n",
       "  0.4278543515914239,\n",
       "  0.4239798671559974,\n",
       "  0.4202789145147898,\n",
       "  0.41673970290297113,\n",
       "  0.4133514752733888,\n",
       "  0.4101044000588979,\n",
       "  0.40698947589695944,\n",
       "  0.40399844758922326,\n",
       "  0.4011237318199588,\n",
       "  0.39835835136913383,\n",
       "  0.39569587673511675,\n",
       "  0.3931303742337985,\n",
       "  0.3906563597698055,\n",
       "  0.3882687575851341,\n",
       "  0.3859628633840082,\n",
       "  0.3837343113126251,\n",
       "  0.3815790443408154,\n",
       "  0.37949328765127766,\n",
       "  0.37747352469244844,\n",
       "  0.37551647559447104,\n",
       "  0.3736190776851653,\n",
       "  0.3717784678752799,\n",
       "  0.36999196671033696,\n",
       "  0.36825706391070584,\n",
       "  0.36657140524268256,\n",
       "  0.3649327805817621,\n",
       "  0.3633391130453464,\n",
       "  0.361788449086157,\n",
       "  0.36027894944990096,\n",
       "  0.3588088809114978,\n",
       "  0.35737660871362253,\n",
       "  0.35598058963962964,\n",
       "  0.35461936566023633,\n",
       "  0.35329155809979973,\n",
       "  0.3519958622737195,\n",
       "  0.3507310425535419,\n",
       "  0.3494959278208071,\n",
       "  0.3482894072746417,\n",
       "  0.34711042656162355,\n",
       "  0.3459579841995684,\n",
       "  0.34483112826968426,\n",
       "  0.3437289533540198,\n",
       "  0.34265059769735784,\n",
       "  0.34159524057468615,\n",
       "  0.34056209984715924,\n",
       "  0.33955042969105254,\n",
       "  0.33855951848564353,\n",
       "  0.3375886868472355,\n",
       "  0.33663728579769425,\n",
       "  0.3357046950569092,\n",
       "  0.3347903214495245,\n",
       "  0.3338935974171336,\n",
       "  0.3330139796278927,\n",
       "  0.3321509476761985,\n",
       "  0.33130400286570244,\n",
       "  0.3304726670694978,\n",
       "  0.3296564816618299,\n",
       "  0.32885500651614724,\n",
       "  0.32806781906473176,\n",
       "  0.3272945134155353,\n",
       "  0.3265346995221974,\n",
       "  0.32578800240353917,\n",
       "  0.32505406140912163,\n",
       "  0.3243325295277191,\n",
       "  0.32362307273580604,\n",
       "  0.32292536938337174,\n",
       "  0.32223910961458757,\n",
       "  0.32156399482103104,\n",
       "  0.3208997371253468,\n",
       "  0.3202460588933777,\n",
       "  0.3196026922729447,\n",
       "  0.3189693787575854,\n",
       "  0.3183458687736836,\n",
       "  0.3177319212895315,\n",
       "  0.31712730344497286,\n",
       "  0.3165317902003659,\n",
       "  0.3159451640036955,\n",
       "  0.31536721447474236,\n",
       "  0.3147977381052944,\n",
       "  0.3142365379744492,\n",
       "  0.31368342347812733,\n",
       "  0.31313821007196674,\n",
       "  0.31260071902683084,\n",
       "  0.31207077719620685,\n",
       "  0.3115482167948231,\n",
       "  0.31103287518785316,\n",
       "  0.31052459469011706,\n",
       "  0.3100232223747278,\n",
       "  0.3095286098906637,\n",
       "  0.30904061328878285,\n",
       "  0.308559092855822,\n",
       "  0.308083912955954,\n",
       "  0.3076149418795014,\n",
       "  0.3071520516984294,\n",
       "  0.30669511812826306,\n",
       "  0.3062440203960961,\n",
       "  0.3057986411143765,\n",
       "  0.3053588661601748,\n",
       "  0.30492458455965593,\n",
       "  0.30449568837749286,\n",
       "  0.3040720726109759,\n",
       "  0.3036536350885829,\n",
       "  0.3032402763727925,\n",
       "  0.3028318996669321,\n",
       "  0.3024284107258647,\n",
       "  0.3020297177703299,\n",
       "  0.301635731404764,\n",
       "  0.30124636453843434,\n",
       "  0.30086153230973084,\n",
       "  0.30048115201346803,\n",
       "  0.30010514303105573,\n",
       "  0.29973342676340864,\n",
       "  0.29936592656646543,\n",
       "  0.2990025676892025,\n",
       "  0.2986432772140269,\n",
       "  0.2982879839994413,\n",
       "  0.2979366186248823,\n",
       "  0.2975891133376324,\n",
       "  0.2972454020017163,\n",
       "  0.29690542004869425,\n",
       "  0.29656910443026996,\n",
       "  0.29623639357263415,\n",
       "  0.29590722733247055,\n",
       "  0.295581546954552,\n",
       "  0.2952592950308603,\n",
       "  0.29494041546116545,\n",
       "  0.2946248534150028,\n",
       "  0.2943125552949906,\n",
       "  0.29400346870143246,\n",
       "  0.2936975423981513,\n",
       "  0.2933947262795058,\n",
       "  0.29309497133854023,\n",
       "  0.2927982296362229,\n",
       "  0.2925044542717283,\n",
       "  0.2922135993537233,\n",
       "  0.2919256199726166,\n",
       "  0.29164047217373235,\n",
       "  0.2913581129313748,\n",
       "  0.291078500123746,\n",
       "  0.2908015925086862,\n",
       "  0.29052734970020416,\n",
       "  0.29025573214576667,\n",
       "  0.28998670110432007,\n",
       "  0.2897202186250144,\n",
       "  0.2894562475266052,\n",
       "  0.28919475137750633,\n",
       "  0.28893569447647144,\n",
       "  0.28867904183387827,\n",
       "  0.28842475915359694,\n",
       "  0.2881728128154174,\n",
       "  0.28792316985801925,\n",
       "  0.287675797962461,\n",
       "  0.2874306654361725,\n",
       "  0.2871877411974324,\n",
       "  0.28694699476031094,\n",
       "  0.28670839622006583,\n",
       "  0.28647191623897095,\n",
       "  0.28623752603256597,\n",
       "  0.2860051973563104,\n",
       "  0.28577490249262943,\n",
       "  0.28554661423833627,\n",
       "  0.2853203058924204,\n",
       "  0.28509595124418696,\n",
       "  0.2848735245617379,\n",
       "  0.2846530005807802,\n",
       "  0.28443435449375354,\n",
       "  0.2842175619392645,\n",
       "  0.28400259899181757,\n",
       "  0.28378944215183427,\n",
       "  0.283578068335949,\n",
       "  0.28336845486757467,\n",
       "  0.2831605794677274,\n",
       "  0.2829544202461033,\n",
       "  0.2827499556923989,\n",
       "  0.28254716466786683,\n",
       "  0.2823460263970995,\n",
       "  0.2821465204600342,\n",
       "  0.28194862678417215,\n",
       "  0.28175232563700414,\n",
       "  0.2815575976186373,\n",
       "  0.28136442365461667,\n",
       "  0.28117278498893444,\n",
       "  0.2809826631772224,\n",
       "  0.2807940400801213,\n",
       "  0.2806068978568218,\n",
       "  0.2804212189587714,\n",
       "  0.2802369861235438,\n",
       "  0.28005418236886365,\n",
       "  0.2798727909867836,\n",
       "  0.2796927955380086,\n",
       "  0.2795141798463628,\n",
       "  0.27933692799339616,\n",
       "  0.27916102431312356,\n",
       "  0.27898645338689704,\n",
       "  0.27881320003840293,\n",
       "  0.2786412493287831,\n",
       "  0.2784705865518753,\n",
       "  0.27830119722957,\n",
       "  0.27813306710727986,\n",
       "  0.2779661821495186,\n",
       "  0.27780052853558684,\n",
       "  0.27763609265536027,\n",
       "  0.27747286110518005,\n",
       "  0.2773108206838393,\n",
       "  0.27714995838866563,\n",
       "  0.2769902614116952,\n",
       "  0.27683171713593735,\n",
       "  0.27667431313172597,\n",
       "  0.27651803715315587,\n",
       "  0.27636287713460167,\n",
       "  0.27620882118731754,\n",
       "  0.2760558575961142,\n",
       "  0.27590397481611273,\n",
       "  0.27575316146957185,\n",
       "  0.2756034063427873,\n",
       "  0.2754546983830618,\n",
       "  0.27530702669574253,\n",
       "  0.27516038054132513,\n",
       "  0.275014749332623,\n",
       "  0.2748701226319986,\n",
       "  0.274726490148657,\n",
       "  0.2745838417359984,\n",
       "  0.27444216738902916,\n",
       "  0.2743014572418296,\n",
       "  0.2741617015650765,\n",
       "  0.2740228907636202,\n",
       "  0.27388501537411325,\n",
       "  0.27374806606269064,\n",
       "  0.2736120336226999,\n",
       "  0.27347690897247917,\n",
       "  0.2733426831531831,\n",
       "  0.2732093473266549,\n",
       "  0.2730768927733427,\n",
       "  0.2729453108902607,\n",
       "  0.2728145931889923,\n",
       "  0.27268473129373505,\n",
       "  0.2725557169393872,\n",
       "  0.2724275419696722,\n",
       "  0.27230019833530383,\n",
       "  0.2721736780921875,\n",
       "  0.27204797339965964,\n",
       "  0.271923076518762,\n",
       "  0.27179897981055207,\n",
       "  0.27167567573444684,\n",
       "  0.27155315684660086,\n",
       "  0.271431415798316,\n",
       "  0.2713104453344842,\n",
       "  0.2711902382920603,\n",
       "  0.2710707875985659,\n",
       "  0.2709520862706229,\n",
       "  0.27083412741251517,\n",
       "  0.2707169042147802,\n",
       "  0.2706004099528264,\n",
       "  0.2704846379855794,\n",
       "  0.2703695817541528,\n",
       "  0.27025523478054647,\n",
       "  0.2701415906663688,\n",
       "  0.270028643091584,\n",
       "  0.2699163858132837,\n",
       "  0.2698048126644819,\n",
       "  0.26969391755293287,\n",
       "  0.2695836944599708,\n",
       "  0.26947413743937343,\n",
       "  0.26936524061624467,\n",
       "  0.26925699818592014,\n",
       "  0.26914940441289203,\n",
       "  0.2690424536297552,\n",
       "  0.26893614023617146,\n",
       "  0.2688304586978543,\n",
       "  0.26872540354557195,\n",
       "  0.2686209693741686,\n",
       "  0.26851715084160416,\n",
       "  0.2684139426680108,\n",
       "  0.2683113396347675,\n",
       "  0.2682093365835912,\n",
       "  0.268107928415644,\n",
       "  0.26800711009065714,\n",
       "  0.26790687662607005,\n",
       "  0.2678072230961857,\n",
       "  0.2677081446313404,\n",
       "  0.26760963641708846,\n",
       "  0.2675116936934016,\n",
       "  0.26741431175388275,\n",
       "  0.2673174859449929,\n",
       "  0.26722121166529234,\n",
       "  0.2671254843646951,\n",
       "  0.26703029954373597,\n",
       "  0.26693565275285086,\n",
       "  0.2668415395916694,\n",
       "  0.26674795570831955,\n",
       "  0.2666548967987449,\n",
       "  0.2665623586060328,\n",
       "  0.2664703369197552,\n",
       "  0.26637882757531944,\n",
       "  0.26628782645333104,\n",
       "  0.26619732947896707,\n",
       "  0.2661073326213601,\n",
       "  0.2660178318929924,\n",
       "  0.2659288233491008,\n",
       "  0.26584030308709095,\n",
       "  0.26575226724596196,\n",
       "  0.2656647120057403,\n",
       "  0.2655776335869228,\n",
       "  0.26549102824992993,\n",
       "  0.2654048922945669,\n",
       "  0.2653192220594946,\n",
       "  0.26523401392170853,\n",
       "  0.2651492642960268,\n",
       "  0.2650649696345861,\n",
       "  0.2649811264263462,\n",
       "  0.2648977311966022,\n",
       "  0.2648147805065049,\n",
       "  0.26473227095258883,\n",
       "  0.26465019916630794,\n",
       "  0.26456856181357835,\n",
       "  0.2644873555943293,\n",
       "  0.2644065772420602,\n",
       "  0.2643262235234053,\n",
       "  0.2642462912377056,\n",
       "  0.26416677721658677,\n",
       "  0.2640876783235438,\n",
       "  0.2640089914535332,\n",
       "  0.2639307135325701,\n",
       "  0.2638528415173328,\n",
       "  0.26377537239477306,\n",
       "  0.2636983031817324,\n",
       "  0.26362163092456403,\n",
       "  0.2635453526987619,\n",
       "  0.2634694656085936,\n",
       "  0.26339396678674026,\n",
       "  0.2633188533939418,\n",
       "  0.2632441226186468,\n",
       "  0.26316977167666894,\n",
       "  0.26309579781084724,\n",
       "  0.263022198290713,\n",
       "  0.26294897041216003,\n",
       "  0.26287611149712187,\n",
       "  0.2628036188932516,\n",
       "  0.2627314899736085,\n",
       "  0.26265972213634836,\n",
       "  0.2625883128044178,\n",
       "  0.26251725942525495,\n",
       "  0.26244655947049295,\n",
       "  0.262376210435668,\n",
       "  0.26230620983993286,\n",
       "  0.2622365552257734,\n",
       "  0.26216724415872933,\n",
       "  0.26209827422712006,\n",
       "  0.26202964304177345,\n",
       "  0.26196134823575873,\n",
       "  0.26189338746412405,\n",
       "  0.2618257584036367,\n",
       "  0.2617584587525278,\n",
       "  0.2616914862302406,\n",
       "  0.2616248385771821,\n",
       "  0.2615585135544785,\n",
       "  0.2614925089437341,\n",
       "  0.26142682254679334,\n",
       "  0.26136145218550655,\n",
       "  0.2612963957014989,\n",
       "  0.26123165095594264,\n",
       "  0.2611672158293326,\n",
       "  0.2611030882212645,\n",
       "  0.26103926605021693,\n",
       "  0.2609757472533358,\n",
       "  0.26091252978622215,\n",
       "  0.2608496116227231,\n",
       "  0.26078699075472495,\n",
       "  0.2607246651919499,\n",
       "  0.26066263296175524,\n",
       "  0.26060089210893533,\n",
       "  0.26053944069552626,\n",
       "  0.26047827680061336,\n",
       "  0.26041739852014095,\n",
       "  0.2603568039667253,\n",
       "  0.26029649126946947,\n",
       "  0.26023645857378114,\n",
       "  0.2601767040411926,\n",
       "  0.2601172258491832,\n",
       "  0.26005802219100455,\n",
       "  0.25999909127550774,\n",
       "  0.2599404313269726,\n",
       "  0.2598820405849398,\n",
       "  0.25982391730404486,\n",
       "  0.2597660597538547,\n",
       "  0.2597084662187055,\n",
       "  0.259651134997544,\n",
       "  0.25959406440376975,\n",
       "  0.2595372527650798,\n",
       "  0.25948069842331595,\n",
       "  0.25942439973431314,\n",
       "  0.25936835506775024,\n",
       "  0.2593125628070032,\n",
       "  0.25925702134899903,\n",
       "  0.2592017291040729,\n",
       "  0.25914668449582595,\n",
       "  0.2590918859609861,\n",
       "  0.25903733194926953,\n",
       "  0.25898302092324466,\n",
       "  0.25892895135819766,\n",
       "  0.2588751217419997,\n",
       "  0.25882153057497564,\n",
       "  0.25876817636977484,\n",
       "  0.2587150576512433,\n",
       "  0.25866217295629756,\n",
       "  0.25860952083379996,\n",
       "  0.2585570998444359,\n",
       "  0.2585049085605922,\n",
       "  0.25845294556623705,\n",
       "  0.25840120945680173,\n",
       "  0.2583496988390638,\n",
       "  0.25829841233103135,\n",
       "  0.25824734856182885,\n",
       "  0.2581965061715849,\n",
       "  0.2581458838113206,\n",
       "  0.2580954801428396,\n",
       "  0.25804529383862007,\n",
       "  0.25799532358170674,\n",
       "  0.25794556806560565,\n",
       "  0.25789602599417927,\n",
       "  0.25784669608154315,\n",
       "  0.2577975770519639,\n",
       "  0.2577486676397582,\n",
       "  0.25769996658919353,\n",
       "  0.25765147265438954,\n",
       "  0.25760318459922066,\n",
       "  0.2575551011972203,\n",
       "  0.25750722123148606,\n",
       "  0.2574595434945854,\n",
       "  0.2574120667884635,\n",
       "  0.25736478992435174,\n",
       "  0.2573177117226768,\n",
       "  0.2572708310129718,\n",
       "  0.25722414663378745,\n",
       "  0.25717765743260523,\n",
       "  0.2571313622657509,\n",
       "  0.25708525999830917,\n",
       "  0.25703934950403984,\n",
       "  0.25699362966529404,\n",
       "  0.25694809937293234,\n",
       "  0.2569027575262433,\n",
       "  0.256857603032863,\n",
       "  0.25681263480869565,\n",
       "  0.25676785177783534,\n",
       "  0.2567232528724878,\n",
       "  0.2566788370328945,\n",
       "  0.25663460320725606,\n",
       "  0.2565905503516576,\n",
       "  0.25654667742999465,\n",
       "  0.2565029834139001,\n",
       "  0.25645946728267127,\n",
       "  0.2564161280231986,\n",
       "  0.2563729646298948,\n",
       "  0.25632997610462516,\n",
       "  0.2562871614566378,\n",
       "  0.25624451970249573,\n",
       "  0.256202049866009,\n",
       "  0.25615975097816773,\n",
       "  0.2561176220770762,\n",
       "  0.2560756622078874,\n",
       "  0.2560338704227383,\n",
       "  0.2559922457806858,\n",
       "  0.25595078734764376,\n",
       "  0.25590949419632036,\n",
       "  0.2558683654061562,\n",
       "  0.25582740006326327,\n",
       "  0.2557865972603642,\n",
       "  0.25574595609673306,\n",
       "  0.25570547567813534,\n",
       "  0.25566515511677035,\n",
       "  0.25562499353121243]}"
      ]
     },
     "execution_count": 440,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cost_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 462,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(-10, 50)"
      ]
     },
     "execution_count": 462,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x432 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize=(10, 6))\n",
    "for cost_i in cost_dict:\n",
    "    plt.plot(cost_dict[cost_i], label=cost_i)\n",
    "    plt.legend(loc=\"upper right\")\n",
    "plt.title('iteration_num_curve')\n",
    "sns.despine()\n",
    "plt.xlim([-10, 50])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Summary：迭代次数过大并不会得到更优的解，会浪费过多资源；迭代次数小会达不到最优解"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Challenge ! ! !\n",
    "\n",
    "The original data have images labeled 0,1,2,3,4,5,6,7,8,9. In our logistic model, we only detect if the digit in the image is larger or smaller than 5. Now, Let's go for a more challenging problem. Try to use softmax function to build a model to recognize which digit (0,1,2,3,4,5,6,7,8,9) is in the image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 482,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = np.eye(3)\n",
    "a = np.eye(3)[[1,2,0,1]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 483,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 1., 0.],\n",
       "       [0., 0., 1.],\n",
       "       [1., 0., 0.],\n",
       "       [0., 1., 0.]])"
      ]
     },
     "execution_count": 483,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 474,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = np.array([[1], [2], [0], [1]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 475,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4, 1)"
      ]
     },
     "execution_count": 475,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 480,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1],\n",
       "       [2],\n",
       "       [0],\n",
       "       [1]])"
      ]
     },
     "execution_count": 480,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 478,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = np.eye(3)[labels.reshape(-1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 481,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 1., 0.],\n",
       "       [0., 0., 1.],\n",
       "       [1., 0., 0.],\n",
       "       [0., 1., 0.]])"
      ]
     },
     "execution_count": 481,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 563,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 重写模型方法\n",
    "def initialize_parameters_1(dim0, dim1):\n",
    "    dim1 = 1\n",
    "    w = np.random.rand(dim0, dim1)\n",
    "    b = 0\n",
    "    \n",
    "    assert(w.shape == (dim0, dim1))\n",
    "    assert(isinstance(b, float) or isinstance(b, int))\n",
    "    \n",
    "    return w, b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 485,
   "metadata": {},
   "outputs": [],
   "source": [
    "w1, b1 = initialize_parameters_1(64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w1, b1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$ softmax = \\frac{e^{z_j}}{{\\sum _k e^{z_k}}} $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\\frac{d_{cost}}{d_w} = \\frac{d_{cost}}{d_A} \\frac{d_A}{d_z} \\frac{d_z}{d_w} $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\\frac{d_{cost}}{d_b} = \\frac{d_{cost}}{d_A} \\frac{d_A}{d_z} \\frac{d_z}{d_b} $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\\frac {d_A}{d_z} = softmax(z) - softmax(z)^2 $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Activation Function\n",
    "def softmax(Z):\n",
    "    return np.exp(Z) / np.sum(np.exp(Z), axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 630,
   "metadata": {},
   "outputs": [],
   "source": [
    "def propagate_1(w, b, X, Y):\n",
    "    m = X.shape[1]\n",
    "    z = np.dot(w.T, X) + b\n",
    "    A = softmax(z)\n",
    "    pdb.set_trace()\n",
    "    cost = - np.sum(Y * np.log(A))\n",
    "    print(cost)\n",
    "    db = np.sum(Y/A) * (A - A * A)\n",
    "    dw = np.dot(X, db.T)\n",
    "\n",
    "    print(\"dbshape\", db.shape)\n",
    "    print(\"dwshape\", dw.shape)\n",
    "    print(\"wshape\", w.shape)\n",
    "    assert(dw.shape == w.shape)\n",
    "    assert(db.dtype == float)\n",
    "    cost = np.squeeze(cost)\n",
    "    assert(cost.shape == ())\n",
    "    \n",
    "    grads = {'dw':dw, 'db':db}\n",
    "    return grads, cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 632,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(11014.09705923)"
      ]
     },
     "execution_count": 632,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.squeeze(11014.097059229767)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 625,
   "metadata": {},
   "outputs": [],
   "source": [
    "def optimize_1(w, b, X, Y, num_iterations, learning_rate, print_cost=False):\n",
    "    costs = []\n",
    "    \n",
    "    for i in range(num_iterations):\n",
    "        \n",
    "        grads, cost = propagate_1(w, b, X, Y)\n",
    "        \n",
    "        dw = grads['dw']\n",
    "        db = grads['db']\n",
    "        \n",
    "        w = w - learning_rate * dw\n",
    "        b = b - learning_rate * db\n",
    "        if i % 100 == 0:\n",
    "            costs.append(cost)\n",
    "        if print_cost and i % 100 == 0:\n",
    "            print(\"Cost after iteration %i: %f\"%(i, cost))\n",
    "    \n",
    "    params = {\"w\":w, \"b\":b}\n",
    "    \n",
    "    grads = {\"dw\":dw, \"db\":db}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 626,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_1(w, b, X):\n",
    "    '''\n",
    "    Predict whether the label is 0 or 1 using learned logistic regression parameters (w, b)\n",
    "    \n",
    "    Arguments:\n",
    "    w -- weights\n",
    "    b -- bias \n",
    "    X -- data \n",
    "    \n",
    "    Returns:\n",
    "    Y_prediction -- a numpy array (vector) containing all predictions (0/1) for the examples in X\n",
    "    '''\n",
    "    m = X.shape[1]\n",
    "    Y_prediction = np.zeros((1,m))\n",
    "    # w = w.reshape(X.shape[0],1)\n",
    "    \n",
    "    A = softmax(np.dot(w.T, X) + b)\n",
    "    \n",
    "    Y_prediction[0] = A.argmax(axis=0)\n",
    "    \n",
    "    assert(Y_prediction.shape == (1,m))\n",
    "    \n",
    "    return Y_prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 627,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pdb\n",
    "def model_1(X_train, Y_train, X_test, Y_test, num_iterations, learning_rate, print_cost):\n",
    "    \"\"\"\n",
    "    Build the logistic regression model by calling all the functions you have implemented.\n",
    "    Arguments:\n",
    "    X_train - training set\n",
    "    Y_train - training label\n",
    "    X_test - test set\n",
    "    Y_test - test label\n",
    "    num_iteration - hyperparameter representing the number of iterations to optimize the parameters\n",
    "    learning_rate -- hyperparameter representing the learning rate used in the update rule of optimize()\n",
    "    print_cost -- Set to true to print the cost every 100 iterations\n",
    "    \n",
    "    Returns:\n",
    "    d - dictionary should contain following information w,b,training_accuracy, test_accuracy,cost\n",
    "    eg: d = {\"w\":w,\n",
    "             \"b\":b,\n",
    "             \"training_accuracy\": traing_accuracy,\n",
    "             \"test_accuracy\":test_accuracy,\n",
    "             \"cost\":cost}\n",
    "    \"\"\"\n",
    "    # 初始化w，b\n",
    "    dim0 = X_train.shape[0]\n",
    "    dim1 = 1\n",
    "    #dim1 = X_train.shape[1]\n",
    "    w_, b_ = initialize_parameters_1(dim0, dim1)\n",
    "    \n",
    "    # 训练模型\n",
    "    params, grads, costs = optimize_1(w_, b_, X_train, Y_train, num_iterations, learning_rate, print_cost)\n",
    "    \n",
    "    w = params['w']\n",
    "    b = params['b']\n",
    "    \n",
    "    # 预测\n",
    "    Y_train_pred = predict_1(w, b, X_train)\n",
    "    Y_test_pred = predict_1(w, b, X_test)\n",
    "    \n",
    "    # 计算精确度\n",
    "    training_accuracy = np.mean(Y_train_pred == Y_train)\n",
    "    test_accuracy = np.mean(Y_test_pred == Y_test)\n",
    "    \n",
    "    # 返回结果\n",
    "    d = {\"w\":w,\n",
    "         \"b\":b,\n",
    "         \"training_accuracy\": training_accuracy,\n",
    "         \"test_accuracy\":test_accuracy,\n",
    "         \"cost\":costs}\n",
    "    return d\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 628,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 获取数据\n",
    "X_train, X_test, y_train, y_test = train_test_split(digits.data, digits.target, test_size=0.25)\n",
    "\n",
    "# 数据预处理\n",
    "# X标准化处理\n",
    "X_train /= X_train.max()\n",
    "X_test /= X_train.max()\n",
    "# XY参数格式调整\n",
    "X_train = X_train.T\n",
    "y_train = np.eye(10)[y_train].T\n",
    "X_test = X_test.T\n",
    "y_test = np.eye(10)[y_test].T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 492,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 1., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 1., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 1., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 1., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 1., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 1., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 1., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 1., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 1.]])"
      ]
     },
     "execution_count": 492,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.eye(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 615,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.74317351]\n",
      " [0.60055883]\n",
      " [0.65461943]\n",
      " [0.76359327]\n",
      " [0.15956716]\n",
      " [0.88160861]\n",
      " [0.87668029]\n",
      " [0.02421649]\n",
      " [0.32189053]\n",
      " [0.13551655]\n",
      " [0.88053639]\n",
      " [0.75045722]\n",
      " [0.21091565]\n",
      " [0.63473932]\n",
      " [0.20629332]\n",
      " [0.97637536]\n",
      " [0.34446693]\n",
      " [0.9384066 ]\n",
      " [0.63311313]\n",
      " [0.92723755]\n",
      " [0.63054893]\n",
      " [0.69993758]\n",
      " [0.44697037]\n",
      " [0.70855111]\n",
      " [0.6128139 ]\n",
      " [0.80357074]\n",
      " [0.3595893 ]\n",
      " [0.09918097]\n",
      " [0.63670435]\n",
      " [0.8955605 ]\n",
      " [0.12699385]\n",
      " [0.64380256]\n",
      " [0.11368422]\n",
      " [0.34082435]\n",
      " [0.48765533]\n",
      " [0.1018142 ]\n",
      " [0.77704311]\n",
      " [0.74730278]\n",
      " [0.1056142 ]\n",
      " [0.38555788]\n",
      " [0.90261802]\n",
      " [0.22676557]\n",
      " [0.74799785]\n",
      " [0.76162415]\n",
      " [0.95612675]\n",
      " [0.76945904]\n",
      " [0.01590671]\n",
      " [0.71970205]\n",
      " [0.48223615]\n",
      " [0.18731873]\n",
      " [0.42352537]\n",
      " [0.81413156]\n",
      " [0.63568063]\n",
      " [0.18177685]\n",
      " [0.24847952]\n",
      " [0.39971973]\n",
      " [0.77704519]\n",
      " [0.55654532]\n",
      " [0.37523416]\n",
      " [0.21441593]\n",
      " [0.94924423]\n",
      " [0.17710219]\n",
      " [0.37120875]\n",
      " [0.49433881]] 0\n",
      "[[13.8268581  12.2547135  12.31028155 ... 12.54696751  9.62442565\n",
      "  11.06718745]]\n",
      "[[4.77242158e-03 9.90752273e-04 1.04736480e-03 ... 1.32705639e-03\n",
      "  7.13913470e-05 3.02154498e-04]]\n"
     ]
    }
   ],
   "source": [
    "w, b = initialize_parameters_1(64, 1)\n",
    "print(w, b)\n",
    "z = np.dot(w.T, X_train) + b\n",
    "print(z)\n",
    "A = softmax(z)\n",
    "print(A)\n",
    "#Y_r = Y.reshape(-1,1) \n",
    "    \n",
    "#cost = -Y_r + np.log1p(np.sum(np.exp(A))) \n",
    "cost = - np.sum(y_train * np.log(A))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 618,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "11014.097059229767"
      ]
     },
     "execution_count": 618,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 633,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> <ipython-input-630-622f9b0dc230>(6)propagate_1()\n",
      "-> cost = - np.sum(Y * np.log(A))\n",
      "(Pdb) n\n",
      "> <ipython-input-630-622f9b0dc230>(7)propagate_1()\n",
      "-> print(cost)\n",
      "(Pdb) n\n",
      "10896.353168299938\n",
      "> <ipython-input-630-622f9b0dc230>(8)propagate_1()\n",
      "-> db = np.sum(Y/A) * (A - A * A)\n",
      "(Pdb) n\n",
      "> <ipython-input-630-622f9b0dc230>(9)propagate_1()\n",
      "-> dw = np.dot(X, db.T)\n",
      "(Pdb) n\n",
      "> <ipython-input-630-622f9b0dc230>(11)propagate_1()\n",
      "-> print(\"dbshape\", db.shape)\n",
      "(Pdb) n\n",
      "dbshape (1, 1347)\n",
      "> <ipython-input-630-622f9b0dc230>(12)propagate_1()\n",
      "-> print(\"dwshape\", dw.shape)\n",
      "(Pdb) n\n",
      "dwshape (64, 1)\n",
      "> <ipython-input-630-622f9b0dc230>(13)propagate_1()\n",
      "-> print(\"wshape\", w.shape)\n",
      "(Pdb) n\n",
      "wshape (64, 1)\n",
      "> <ipython-input-630-622f9b0dc230>(14)propagate_1()\n",
      "-> assert(dw.shape == w.shape)\n",
      "(Pdb) n\n",
      "> <ipython-input-630-622f9b0dc230>(15)propagate_1()\n",
      "-> assert(db.dtype == float)\n",
      "(Pdb) n\n",
      "> <ipython-input-630-622f9b0dc230>(16)propagate_1()\n",
      "-> cost = np.squeeze(cost)\n",
      "(Pdb) n\n",
      "> <ipython-input-630-622f9b0dc230>(17)propagate_1()\n",
      "-> assert(cost.shape == ())\n",
      "(Pdb) p cost\n",
      "10896.353168299938\n",
      "(Pdb) type(cost)\n",
      "<class 'numpy.float64'>\n",
      "(Pdb) n\n",
      "> <ipython-input-630-622f9b0dc230>(19)propagate_1()\n",
      "-> grads = {'dw':dw, 'db':db}\n",
      "(Pdb) n\n",
      "> <ipython-input-630-622f9b0dc230>(20)propagate_1()\n",
      "-> return grads, cost\n",
      "(Pdb) n\n",
      "--Return--\n",
      "> <ipython-input-630-622f9b0dc230>(20)propagate_1()->({'db': array([[11569...37.37990867]]), 'dw': array([[0.000...1085158e+05]])}, 10896.353168299938)\n",
      "-> return grads, cost\n",
      "(Pdb) n\n",
      "> <ipython-input-625-d11c1394163a>(8)optimize_1()\n",
      "-> dw = grads['dw']\n",
      "(Pdb) n\n",
      "> <ipython-input-625-d11c1394163a>(9)optimize_1()\n",
      "-> db = grads['db']\n",
      "(Pdb) c\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/qinliu/AI/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:3: RuntimeWarning: invalid value encountered in true_divide\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> <ipython-input-630-622f9b0dc230>(6)propagate_1()\n",
      "-> cost = - np.sum(Y * np.log(A))\n",
      "(Pdb) n\n",
      "> <ipython-input-630-622f9b0dc230>(7)propagate_1()\n",
      "-> print(cost)\n",
      "(Pdb) n\n",
      "nan\n",
      "> <ipython-input-630-622f9b0dc230>(8)propagate_1()\n",
      "-> db = np.sum(Y/A) * (A - A * A)\n",
      "(Pdb) pp Y\n",
      "array([[0., 0., 1., ..., 0., 0., 0.],\n",
      "       [0., 0., 0., ..., 0., 0., 0.],\n",
      "       [0., 0., 0., ..., 0., 1., 1.],\n",
      "       ...,\n",
      "       [0., 1., 0., ..., 0., 0., 0.],\n",
      "       [0., 0., 0., ..., 0., 0., 0.],\n",
      "       [0., 0., 0., ..., 0., 0., 0.]])\n",
      "(Pdb) pp n.log(A)\n",
      "*** NameError: name 'n' is not defined\n",
      "(Pdb) pp A\n",
      "array([[nan, nan, nan, ..., nan, nan, nan]])\n",
      "(Pdb) pp z\n",
      "array([[-1246970.37633179, -1255999.32387255, -1211752.28763501, ...,\n",
      "         -959126.18360423, -1082846.12006942, -1134018.76997431]])\n",
      "(Pdb) softmax(z)\n",
      "array([[nan, nan, nan, ..., nan, nan, nan]])\n",
      "(Pdb) n\n",
      "> <ipython-input-630-622f9b0dc230>(9)propagate_1()\n",
      "-> dw = np.dot(X, db.T)\n",
      "(Pdb) quit\n"
     ]
    },
    {
     "ename": "BdbQuit",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mBdbQuit\u001b[0m                                   Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-633-88e7e508dad5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0md\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel_1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1000\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1e-2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-627-f37dc1fd41c5>\u001b[0m in \u001b[0;36mmodel_1\u001b[0;34m(X_train, Y_train, X_test, Y_test, num_iterations, learning_rate, print_cost)\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m     \u001b[0;31m# 训练模型\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 29\u001b[0;31m     \u001b[0mparams\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrads\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcosts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moptimize_1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mw_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mb_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_iterations\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlearning_rate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprint_cost\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     30\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m     \u001b[0mw\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'w'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-625-d11c1394163a>\u001b[0m in \u001b[0;36moptimize_1\u001b[0;34m(w, b, X, Y, num_iterations, learning_rate, print_cost)\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m         \u001b[0mdw\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgrads\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'dw'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m         \u001b[0mdb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgrads\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'db'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m         \u001b[0mw\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mw\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mlearning_rate\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mdw\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-630-622f9b0dc230>\u001b[0m in \u001b[0;36mpropagate_1\u001b[0;34m(w, b, X, Y)\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcost\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0mdb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mY\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mA\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mA\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mA\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mA\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m     \u001b[0mdw\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"dbshape\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-630-622f9b0dc230>\u001b[0m in \u001b[0;36mpropagate_1\u001b[0;34m(w, b, X, Y)\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcost\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0mdb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mY\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mA\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mA\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mA\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mA\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m     \u001b[0mdw\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"dbshape\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/AI/anaconda3/lib/python3.7/bdb.py\u001b[0m in \u001b[0;36mtrace_dispatch\u001b[0;34m(self, frame, event, arg)\u001b[0m\n\u001b[1;32m     86\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0;31m# None\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     87\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mevent\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'line'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 88\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdispatch_line\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mframe\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     89\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mevent\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'call'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     90\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdispatch_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mframe\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0marg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/AI/anaconda3/lib/python3.7/bdb.py\u001b[0m in \u001b[0;36mdispatch_line\u001b[0;34m(self, frame)\u001b[0m\n\u001b[1;32m    111\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstop_here\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mframe\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbreak_here\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mframe\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    112\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muser_line\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mframe\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 113\u001b[0;31m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mquitting\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;32mraise\u001b[0m \u001b[0mBdbQuit\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    114\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrace_dispatch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    115\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mBdbQuit\u001b[0m: "
     ]
    }
   ],
   "source": [
    "d = model_1(X_train, y_train, X_test, y_test, 1000, 1e-2, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 592,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(64, 1)"
      ]
     },
     "execution_count": 592,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X1 = np.zeros((64, 1347))\n",
    "db1 = np.zeros((1, 1347))\n",
    "test = np.dot(X1, db1.T)\n",
    "test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Congratulations ! You have completed assigment 4. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
